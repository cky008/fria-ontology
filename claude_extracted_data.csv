file_name,report_name,organisation_description,contributor_details,assessment_content,technology_description,purposes_description,report_date,aiaaic_link,challenge_11,evaluation_11,impact_level_11,challenge_12,evaluation_12,impact_level_12,challenge_13,evaluation_13,impact_level_13,challenge_14,evaluation_14,impact_level_14,challenge_15,evaluation_15,impact_level_15,challenge_16,evaluation_16,impact_level_16,challenge_21,evaluation_21,impact_level_21,challenge_22,evaluation_22,impact_level_22,challenge_23,evaluation_23,impact_level_23,challenge_31,evaluation_31,impact_level_31,challenge_32,evaluation_32,impact_level_32,challenge_41,evaluation_41,impact_level_41,challenge_42,evaluation_42,impact_level_42,challenge_43,evaluation_43,impact_level_43,challenge_44,evaluation_44,impact_level_44,challenge_45,evaluation_45,impact_level_45,challenge_46,evaluation_46,impact_level_46
fria-instance-claude-1.ttl,"3D masks fool payment, airport facial recognition systems","Kneron, an artificial intelligence company, conducted the research","Researchers from Kneron, including CEO Albert Liu",Researchers found that facial recognition technology can be fooled by using 3D-printed masks depicting different people's faces. The test involved visiting public locations and tricking facial recognition terminals into allowing payment or access. The masks were able to fool payment systems and border checkpoints in China and the Netherlands.,"Facial recognition technology used in payment systems, airport security, and transportation hubs",To test the security and reliability of facial recognition systems in public spaces and payment systems,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/3d-masks-fool-payment-airport-facial-recognition-systems,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,The facial recognition systems tested did not indicate that they were using AI for identification.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating",The facial recognition systems tested did not provide confidence scores or error rates.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The facial recognition systems presented their results as absolute truth, with no apparent way for individuals to contest false positives.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,The facial recognition systems provided no explanation for their decisions.,High,There is no indication of the extent to which the AI system influences the overall decision-making process,The role of facial recognition in the overall security process was not clear.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,No information was provided about how individuals could contest or correct false identifications.,High,The AI system targets members of a specific social group,"The facial recognition systems may have biases based on race, age, or other factors, though this was not specifically tested in the reported experiment.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","There are no apparent mechanisms to flag and correct issues related to bias, discrimination, or poor performance in the facial recognition systems tested.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,The vulnerability to 3D masks suggests a lack of consideration for diverse attack scenarios.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals,The facial recognition systems were used on the general public in airports and payment systems.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals",No information was provided about data access or transparency.,High,There are no mechanisms for the user to exercise control over the processing of personal data,There are no apparent mechanisms for users to exercise control over the processing of their personal data in these facial recognition systems.,Very High,There are no measures to ensure the lawfulness of the processing of personal data,The vulnerability to spoofing attacks raises questions about the legal compliance of these systems.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The vulnerability to spoofing attacks suggests inadequate data protection measures.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There is no apparent mechanism allowing compliance with the exercise of data subject's rights (access, rectification, and erasure of data relating to a specific individual).",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The vulnerability to spoofing attacks suggests inadequate security measures for the processing of personal data.,Very High,There is no procedure to conduct a data protection impact assessment,There is no apparent procedure to conduct a data protection impact assessment for these facial recognition systems.,High
fria-instance-claude-2.ttl,4 Little Trees (4LT) student emotion recognition,"Find Solution AI Limited, a Hong Kong-based company specializing in education using AI and big data analysis","Viola Lam, founder and CEO of Find Solution AI","4 Little Trees is an AI program that analyzes students' emotions as they learn to help teachers make distance learning more engaging and personalized. It uses facial recognition to identify emotions such as happiness, sadness, anger, surprise, and fear by analyzing facial muscular micro-movements in real-time. The system also monitors students' performance, generates reports, and forecasts grades. However, researchers and digital rights activists have raised concerns about the intrusiveness, potential misuse, and bias of emotion recognition technologies.","Emotion recognition, facial analysis, gesture analysis, and computer vision technology","To identify and monitor emotions of students during learning, aiming to improve engagement and personalize education",2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/4-little-trees-4lt,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,4 Little Trees does not explicitly communicate to students that their emotions and performance are being analyzed by an AI system.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","While the company claims 85% accuracy in emotion recognition, it's unclear if this information is communicated to users, potentially leading to overconfidence in the system's assessments.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The system's emotion recognition and performance predictions may be presented as factual, potentially influencing teachers' perceptions of students without clear means for students to contest these assessments.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,The system does not provide clear explanations to students or teachers about how it arrives at its emotional assessments or performance predictions.,High,There is no indication of the extent to which the AI system influences the overall decision-making process,It's not clear how much weight teachers give to the AI system's assessments in their overall evaluation of students.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There's no information about mechanisms for students or parents to challenge or seek redress for potentially harmful or inaccurate assessments made by the system.,High,The AI system targets members of a specific social group,"The system has been primarily trained on and used with Chinese students in Hong Kong, raising concerns about its applicability and potential bias when used with more diverse populations.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","There's no information about mechanisms to identify or correct potential biases or discrimination in the system's assessments, particularly for students from diverse backgrounds.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,The system's training primarily on Chinese faces may not adequately represent the diversity of emotional expressions across different cultures and ethnicities.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The system is deployed to monitor all students in participating schools, raising privacy concerns about the widespread surveillance of children's emotions and behaviors.",Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals",It's unclear whether students and parents have access to the emotional and performance data collected and analyzed by the system.,High,There are no mechanisms for the user to exercise control over the processing of personal data,It's unclear whether students or parents have any control over the collection and processing of their emotional and performance data by the 4 Little Trees system.,High,There are no measures to ensure the lawfulness of the processing of personal data,"There's limited information about the legal framework under which this data is collected and processed, particularly concerning the privacy rights of minors.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,There's no clear information about how access to students' emotional and performance data is restricted or limited to only necessary parties.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","It's not clear if there are mechanisms in place for students or parents to access, correct, or request deletion of their data collected by the 4 Little Trees system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","While the company claims it doesn't record video of students' faces, there's limited information about the security measures in place to protect the collected facial muscle data and performance information.",High,There is no procedure to conduct a data protection impact assessment,"There's no information about whether a data protection impact assessment has been conducted for the 4 Little Trees system, despite its processing of sensitive data from minors.",High
fria-instance-claude-3.ttl,7-Eleven customer survey facial recognition,"7-Eleven, a chain store operating in Australia","Office of the Australian Information Commissioner (OAIC), Angelene Falk (Australian Information Commissioner and Privacy Commissioner)","7-Eleven breached customer privacy in 700 stores between June 2020 and August 2021 by collecting facial imagery without consent. The company used tablets with built-in cameras to capture facial images of customers completing surveys, converting these into faceprints for comparison and demographic profiling. The OAIC ruled that this collection was not reasonably necessary and failed to provide adequate information about data use and storage.","Facial recognition technology, facial image capture, faceprint generation, and demographic profiling",To validate survey responses and understand demographic profiles of customers,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/7-eleven-customer-survey-facial-recognition,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,7-Eleven did not adequately inform customers that their facial images were being captured and processed by an AI system for survey validation and demographic profiling.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating",There was no information provided about the accuracy of the facial recognition system or the possibility of errors in demographic profiling or survey response validation.,Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it",The system's use for validating survey responses could potentially lead to genuine responses being discarded without the customer's knowledge or ability to contest the decision.,Medium,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,7-Eleven did not provide explanations to customers about how their facial images were being used to validate surveys or generate demographic profiles.,High,There is no indication of the extent to which the AI system influences the overall decision-making process,The extent to which the facial recognition system influenced 7-Eleven's decision-making processes regarding customer feedback and demographic analysis was not disclosed.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There were no apparent measures in place for customers to seek redress if they were adversely affected by the facial recognition system's decisions.,High,The AI system targets members of a specific social group,"While not explicitly targeting specific groups, the use of facial recognition for demographic profiling could potentially lead to biased or discriminatory outcomes.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",There was no information provided about mechanisms to identify or correct potential biases or discrimination in the facial recognition system's demographic profiling.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,The facial recognition system's ability to accurately represent the diversity of 7-Eleven's customer base in its demographic profiling was not addressed.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The facial recognition system was deployed to all customers completing surveys, without any mechanism to limit its use to specific individuals or circumstances.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals","Customers were not provided with access to their facial images or faceprints collected by the system, nor were they informed about how to access this data.",Very High,There are no mechanisms for the user to exercise control over the processing of personal data,"Customers were not provided with mechanisms to control the collection, processing, or deletion of their facial data.",Very High,There are no measures to ensure the lawfulness of the processing of personal data,"The OAIC found that 7-Eleven's collection and processing of facial data was not lawful under Australian privacy laws, as it was not reasonably necessary for the company's functions.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,There was no information provided about procedures to limit access to the collected facial data or ensure it was only used for the stated purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","7-Eleven did not provide mechanisms for customers to access, rectify, or request erasure of their facial data collected during the survey process.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","While 7-Eleven claimed to use 'biometric blurring' and encrypted algorithmic representations, the OAIC found that these measures were insufficient to protect customers' sensitive biometric information.",High,There is no procedure to conduct a data protection impact assessment,There was no evidence that 7-Eleven conducted a data protection impact assessment before implementing the facial recognition system for customer surveys.,High
fria-instance-claude-4.ttl,Aadhaar COVID-19 facial recognition marginalisation,"Indian government, National Health Authority","RS Sharma, head of India's National Health Authority; Anushka Jain, associate counsel at the Internet Freedom Foundation","The Indian government plans to use facial recognition integrated with the Aadhaar biometric ID system for COVID-19 vaccination processes. This raises concerns about marginalizing vulnerable populations without mobile phones or internet access, potential misidentification, and privacy issues. The system is being tested in Jharkhand state and may be rolled out nationwide if successful.",Facial recognition technology integrated with Aadhaar biometric ID system,To verify identity for COVID-19 vaccination process and create a 'touchless' vaccination procedure,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aadhaar-covid-19-facial-recognition,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,The facial recognition system for vaccination does not explicitly communicate to individuals that their identity verification is the result of an AI-driven process.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","There is no information about the system providing accuracy rates or confidence levels for its facial recognition matches, which could lead to unquestioned acceptance of potentially incorrect identifications.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","If the facial recognition system misidentifies an individual or fails to recognize them, the burden may fall on the individual to prove their identity through alternative means, potentially depriving them of vaccination.",Very High,,,,,,,,,,The AI system targets members of a specific social group,"The system may disproportionately affect vulnerable populations without access to mobile phones, internet, or Aadhaar IDs, potentially excluding them from the vaccination process.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","There is no information about mechanisms to identify or correct potential biases or errors in the facial recognition system, particularly for diverse populations.",High,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The facial recognition system is being deployed for all individuals seeking vaccination, not limited to specific cases where identity verification is problematic.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,There is no information about individuals being able to opt out of facial recognition or control how their biometric data is processed for vaccination purposes.,Very High,There are no measures to ensure the lawfulness of the processing of personal data,"The legal basis for using facial recognition for COVID-19 vaccination is unclear, raising concerns about the lawfulness of processing sensitive biometric data.",High,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",There is no information about specific security measures to protect the biometric data collected and processed during the vaccination process.,High,There is no procedure to conduct a data protection impact assessment,"There is no mention of a data protection impact assessment being conducted before implementing the facial recognition system for COVID-19 vaccinations, despite the sensitive nature of the biometric data being processed.",High
fria-instance-claude-5.ttl,Aadhaar glitches result in villagers' starvation,"Indian government, Unique Identification Authority of India (UIDAI)","Jean Dreze, economist and activist; Right to Food Campaign","Technical problems with India's Aadhaar biometric ID system have resulted in the deaths of scores of villagers in Jharkhand state and elsewhere, with some committing suicide and others suffering severe malnutrition. Glitches have meant that villagers have been unable to get food rations or subsidised grain, sometimes without explanation. Some groups, including vulnerable minority groups such as the Parhaiya, have been denied their legal entitlement of subsidised grain for failing to correctly link their ration cards with Aadhaar.","Aadhaar biometric ID system, fingerprint authentication",To reduce welfare fraud and ensure efficient distribution of subsidies and rations,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aadhaar-glitch-results-in-villagers-starvation,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,The Aadhaar system does not explicitly communicate to beneficiaries that their access to rations is being determined by an algorithmic decision based on biometric authentication.,Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The Aadhaar system does not provide any indication of the likelihood of errors in biometric authentication, leading to absolute denial of rations without consideration of potential system errors.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","When Aadhaar authentication fails, the burden of proof falls on the beneficiaries to prove their identity and eligibility, often requiring multiple trips to government offices and sometimes even bribes.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"Beneficiaries are often not given clear explanations for why their authentication failed or why their rations were denied, leading to confusion and inability to address the issue.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,The extent to which Aadhaar authentication influences the overall decision to grant or deny rations is not clearly communicated to beneficiaries.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"There appears to be no clear, accessible redress mechanism for beneficiaries who are wrongly denied rations due to Aadhaar authentication failures.",Very High,The AI system targets members of a specific social group,"The system disproportionately affects vulnerable and marginalized groups, such as the Parhaiya tribe, who may have more difficulty with Aadhaar enrollment and authentication.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",There appears to be no systematic mechanism to identify and address issues of bias or poor performance in the Aadhaar authentication system for ration distribution.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The Aadhaar system does not adequately account for diverse populations, such as manual laborers with worn fingerprints or elderly individuals with changing biometrics.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The Aadhaar authentication system is applied universally to all beneficiaries, rather than being limited to cases where fraud is suspected.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,Beneficiaries have little to no control over how their biometric data is processed or used within the Aadhaar system.,High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","The report does not mention specific security measures for protecting biometric data in the Aadhaar system, raising concerns about data security and privacy.",High,There is no procedure to conduct a data protection impact assessment,"There is no mention of a data protection impact assessment being conducted for the Aadhaar system's use in ration distribution, despite its significant impact on vulnerable populations.",High
fria-instance-claude-6.ttl,AccessiBe automated web accessibility,"AccessiBe, an Israel-based web accessibility start-up","Eyebobs (eyewear retailer), blind customers, accessibility advocates","AccessiBe claims to make websites more accessible for people with disabilities using automated AI technology. However, many blind users and accessibility advocates report that the software actually makes websites harder to navigate with screen readers. Eyebobs, a company using AccessiBe, settled a lawsuit with a customer over accessibility issues.","Web accessibility overlay, automated AI technology for website accessibility",To improve website accessibility for people with disabilities and help companies comply with accessibility regulations,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/accessibe-automated-accessibility,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"AccessiBe does not clearly communicate to users that the website's accessibility features are being provided by an AI-powered overlay, which may lead to confusion when issues arise.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","AccessiBe does not provide information about the accuracy or reliability of its automated accessibility fixes, potentially leading users and website owners to believe the system is infallible.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","AccessiBe's marketing claims of full ADA compliance may lead website owners to believe they are protected from lawsuits, potentially shifting the burden of proof to users experiencing accessibility issues.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"AccessiBe does not provide clear explanations for how it determines and implements accessibility fixes, making it difficult for users to understand and troubleshoot issues.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"It's unclear how much of a website's accessibility is determined by AccessiBe's AI versus the site's original code, making it difficult to assess the system's impact.",Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"There appears to be no clear process for users to report and address accessibility issues caused by AccessiBe's overlay, leaving them without recourse when problems occur.",High,The AI system targets members of a specific social group,"While intended to help people with disabilities, AccessiBe's system may disproportionately impact blind users who rely on screen readers, potentially creating new barriers for this group.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","There appears to be no clear mechanism for users to report and have corrected accessibility issues caused by AccessiBe's overlay, despite numerous reports of problems.",High,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"AccessiBe's overlay is applied universally to all users of a website, potentially causing issues for users who don't need or want the additional layer of accessibility features.",Medium,,,,There are no mechanisms for the user to exercise control over the processing of personal data,"It's unclear whether users have control over how AccessiBe processes their data, including information about their use of assistive technologies.",High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","The report doesn't mention specific security measures for protecting user data processed by AccessiBe's overlay, raising potential privacy concerns.",Medium,There is no procedure to conduct a data protection impact assessment,"There's no mention of AccessiBe conducting data protection impact assessments, despite the system processing potentially sensitive information about users' disabilities.",High
fria-instance-claude-7.ttl,Adobe Creative Cloud content analysis,"Adobe, a multinational computer software company","Adobe customers, graphic designers, artists","Adobe Creative Cloud appears to be automatically analyzing customer content stored on Creative Cloud to train its AI algorithms, unless users opt-out. This discovery sparked concerns among users about privacy, intellectual property, and potential job displacement.","Machine learning, pattern recognition, object recognition applied to user-generated content in Creative Cloud","To improve Adobe's products and services, and provide product features",2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-creative-cloud-content-analysis,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"Adobe did not clearly communicate to users that their content was being analyzed by AI algorithms, leading to surprise and concern when the practice was discovered.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating",There is no information about Adobe providing accuracy metrics or error rates for its AI-powered features derived from user content analysis.,Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","While not directly applicable, Adobe's use of user content for AI training without explicit consent shifts the burden to users to prove they didn't agree to such use.",Medium,,,,,,,,,,The AI system targets members of a specific social group,"The system potentially affects all Adobe Creative Cloud users, but may disproportionately impact professional artists and designers who rely on the platform for their work.",Medium,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The content analysis appears to be applied to all users by default, with an opt-out mechanism rather than a targeted or opt-in approach.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,"While users can opt-out of content analysis, there appears to be limited control over how their data is processed if they remain opted-in.",High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","The report does not mention specific security measures for protecting user content during the analysis process, raising concerns about data privacy and security.",High,There is no procedure to conduct a data protection impact assessment,"There is no mention of Adobe conducting a data protection impact assessment for its content analysis practices, despite the potential privacy implications.",High
fria-instance-claude-8.ttl,Adobe Firefly AI art generator training,"Adobe, a multinational computer software company","Adobe Stock contributors, artists, photographers","Adobe has faced criticism for training its Firefly AI art generator on Adobe Stock images without explicitly informing or seeking consent from contributors. While Adobe claims the model is 'safe for commercial use' and offers indemnity from copyright claims, contributors argue this use of their IP is unethical and may threaten their livelihoods.","Machine learning, pattern recognition, object recognition applied to Adobe Stock images for generative AI","To generate images and videos for commercial use, improving Adobe's products and services",2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-firefly-ai-art-generator,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"Adobe did not clearly communicate to Adobe Stock contributors that their content was being used to train the Firefly AI model, leading to surprise and concern when the practice was discovered.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","There is no information about Adobe providing accuracy metrics or error rates for Firefly's generated content, which could lead to overconfidence in its outputs.",Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","Adobe's claim of Firefly being 'safe for commercial use' and offering indemnity shifts the burden of proof to artists claiming copyright infringement, potentially making it difficult for them to defend their rights.",High,,,,,,,,,,The AI system targets members of a specific social group,"The system potentially affects all Adobe Stock contributors, but may disproportionately impact professional artists and photographers who rely on stock image sales for their livelihood.",High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,The Firefly model appears to be trained on all eligible Adobe Stock content without a clear opt-out mechanism for contributors.,High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,"Adobe Stock contributors have limited control over how their content is used in the Firefly AI model, with no clear opt-out option provided.",Very High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","While Adobe claims to use content credentials metadata tags for generated images, there's limited information about specific security measures to protect contributor content during the AI training process.",Medium,There is no procedure to conduct a data protection impact assessment,"There is no mention of Adobe conducting a data protection impact assessment for its use of Adobe Stock content in training Firefly, despite the potential impact on contributors' rights and livelihoods.",High
fria-instance-claude-9.ttl,Adobe Sensei Project Morpheus,"Adobe, a multinational computer software company",Adobe research labs and engineering teams,"Adobe unveiled Project Morpheus, a prototype video editing tool that allows users to alter someone's age, facial expressions, facial hair, and glasses in videos. While it showcases advanced AI capabilities, it raises concerns about the potential for creating deepfake videos that could be used maliciously.","Deepfake technology, video manipulation, generative adversarial networks (GANs), neural networks, deep learning, machine learning","To manipulate video content for creative purposes, but with potential dual-use implications",2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-sensei-project-morpheus,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"Project Morpheus may not clearly indicate when a video has been algorithmically altered, potentially misleading viewers about the authenticity of the content.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","There is no mention of Project Morpheus providing confidence levels or error rates for its video manipulations, which could lead to overconfidence in the altered content's realism.",Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The realistic nature of Project Morpheus's video manipulations could make it difficult for individuals to prove that a video of them has been altered, potentially shifting the burden of proof onto the subject of the video.",Very High,,,,,,,,,,The AI system targets members of a specific social group,"While not targeting a specific group, Project Morpheus could potentially be misused to create misleading or damaging content about individuals or groups, particularly public figures or marginalized communities.",High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"Project Morpheus, as described, does not appear to have mechanisms to limit its use to specific individuals or purposes, potentially allowing for widespread misuse.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,There is no mention of mechanisms for individuals to control or consent to the use of their likeness in videos manipulated by Project Morpheus.,Very High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report does not mention any specific security measures to protect the personal data of individuals whose likenesses are manipulated by Project Morpheus.,High,There is no procedure to conduct a data protection impact assessment,"There is no mention of Adobe conducting a data protection impact assessment for Project Morpheus, despite its potential for significant privacy and ethical implications.",High
fria-instance-claude-10.ttl,"Aespa virtual K-pop anthropomorphism, sexualisation","SM Entertainment, a major Korean entertainment company","SM Entertainment, Aespa members (human and virtual)","SM Entertainment announced Aespa, a K-pop girl group including both human and virtual members with AI brains that can interact independently. This raised concerns about the potential for dehumanization, sexualization, and unethical manipulation of the virtual avatars.","Deepfake technology for image and video, AI for virtual avatar interaction","To create virtual avatars for K-pop idols, enabling new forms of interaction and performance",2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aespa-virtual-k-pop,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"It's unclear whether interactions with Aespa's virtual members will be clearly identified as AI-generated, potentially misleading fans about the nature of their relationship with the avatars.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","There is no mention of the AI system providing accuracy metrics or error rates for its interactions, which could lead to overconfidence in the authenticity of the virtual members' behaviors.",Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The realistic nature of the virtual members could make it difficult for the human members to dispute any actions or statements attributed to their virtual counterparts, potentially compromising their agency and reputation.",High,,,,,,,,,,The AI system targets members of a specific social group,"The system specifically targets young female K-pop idols, raising concerns about the potential for objectification and exploitation of this demographic.",Very High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The virtual members are designed to interact with fans broadly, without clear limitations on who can access or engage with them, potentially exposing vulnerable users to risks.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,There is no mention of mechanisms for the human Aespa members or fans to control how their personal data or likeness is used in the creation and operation of the virtual members.,Very High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report does not mention any specific security measures to protect the personal data of the human Aespa members or fans interacting with the virtual members.,High,There is no procedure to conduct a data protection impact assessment,"There is no mention of SM Entertainment conducting a data protection impact assessment for the Aespa virtual member system, despite its potential for significant privacy and ethical implications.",High
fria-instance-claude-11.ttl,Agricultural Bank of China facial recognition age bias,"Agricultural Bank of China (ABC), a major Chinese bank","Agricultural Bank of China, elderly customers","A video emerged showing people lifting a 94-year-old woman to use an ATM's facial recognition system in Guangshui, Hubei province, China. The incident highlighted age bias in the bank's facial recognition technology, causing public outcry and calls for more inclusive design for elderly users.",Facial recognition technology for identity verification at ATMs,To verify customer identity for activating social security cards and accessing bank services,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/agricultural-bank-of-china-facial-recognition-age-bias,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The facial recognition system at the ATM likely did not clearly communicate to users that their identity verification was being processed algorithmically, which could lead to confusion, especially for elderly users.",Medium,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","There is no information about the system providing confidence levels or error rates for its facial recognition matches, which could lead to unquestioned acceptance of potentially inaccurate results, especially for elderly users.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The facial recognition system's failure to recognize the elderly woman puts the burden on her to prove her identity through extraordinary means (being lifted), rather than the system adapting to her needs.",Very High,,,,,,,,,,The AI system targets members of a specific social group,"The facial recognition system shows clear bias against elderly users, particularly those with limited mobility, effectively discriminating against this demographic in accessing essential financial services.",Very High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The facial recognition system is deployed universally for all ATM users, without consideration for those who might have difficulty using it, such as elderly or disabled individuals.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,"There is no mention of alternative identification methods or opt-out options for users who cannot use the facial recognition system, limiting user control over their personal data and access to services.",High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report does not mention any specific security measures for protecting the biometric data collected by the facial recognition system at ATMs.,Medium,There is no procedure to conduct a data protection impact assessment,"There is no mention of the Agricultural Bank of China conducting a data protection impact assessment for its facial recognition system, which could have identified potential issues for elderly users.",High
fria-instance-claude-12.ttl,AI confuses bus ad for jaywalker,"Ningbo city police, China","Ningbo police, Dong Mingzhu (CEO of Gree Electric Appliances)","An AI-powered facial recognition system in Ningbo, China, mistakenly identified a woman's face on a bus advertisement as a jaywalker. The system publicly shamed Dong Mingzhu, CEO of Gree Electric Appliances, by displaying her image on a large screen, highlighting issues with the accuracy and reliability of such systems.",Facial recognition technology for identifying jaywalkers,To improve street safety by identifying and publicly shaming jaywalkers,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-confuses-bus-ad-for-jaywalker,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The public shaming display did not clearly indicate that the identification of jaywalkers was made by an AI system, potentially misleading the public about the nature and reliability of the accusation.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The system did not provide any indication of confidence levels or potential for error in its identifications, leading to the false accusation being presented as fact.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The public shaming system presents identifications as absolute truth, leaving no immediate recourse for the falsely accused to defend themselves before their image is displayed.",Very High,,,,,,,,,,The AI system targets members of a specific social group,"While not intentionally targeting a specific group, the system's error in this case disproportionately affected a prominent businesswoman, potentially reinforcing biases about women in leadership positions.",Medium,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The system indiscriminately scans and identifies all individuals in its view, including non-human images like advertisements, without any mechanism to limit its scope to actual pedestrians.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,There is no apparent mechanism for individuals to opt-out of the facial recognition system or control how their image data is processed and displayed.,High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report does not mention any specific security measures for protecting the personal data collected and processed by the facial recognition system.,Medium,There is no procedure to conduct a data protection impact assessment,"There is no mention of the Ningbo police conducting a data protection impact assessment for their facial recognition system, which could have identified potential issues like misidentification of images.",High
fria-instance-claude-13.ttl,AI converts Asian-American student into Caucasian,"Playground AI, an AI image generation company","Rona Wang (MIT student), Suhail Doshi (Playground AI founder)",Playground AI's text-to-image generator changed the face of Asian-American MIT student Rona Wang into a Caucasian appearance when asked to create 'a professional LinkedIn profile photo'. The incident raised concerns about racial bias in AI systems and their potential impact on perpetuating stereotypes.,"Text-to-image generation, Generative Adversarial Networks (GANs), neural networks, deep learning, machine learning",To generate professional-looking profile images based on user input,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-converts-asian-american-student-into-caucasian,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"Playground AI did not clearly communicate to users that the generated image was a result of algorithmic decision-making, potentially misleading users about the nature and limitations of the output.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The system did not provide any indication of confidence levels or potential for bias in its generated images, leading users to potentially accept the racially altered output as a valid interpretation of 'professional'.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The system's output implicitly suggests that a Caucasian appearance is more 'professional', placing the burden on users to question and challenge this biased representation.",Very High,,,,,,,,,,The AI system targets members of a specific social group,"The system demonstrated clear bias against Asian-American appearances, effectively discriminating against this demographic by associating 'professional' with Caucasian features.",Very High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The system is deployed for general use without mechanisms to prevent its application in scenarios where racial bias could have significant negative impacts, such as job applications or professional networking.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data,"Users have limited control over how their personal images are processed and transformed by the system, with no apparent option to maintain racial characteristics.",High,,,,,,,,,,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report does not mention any specific security measures for protecting the personal images uploaded by users or the generated outputs.,Medium,There is no procedure to conduct a data protection impact assessment,"There is no mention of Playground AI conducting a data protection impact assessment for their image generation system, which could have identified potential issues like racial bias.",High
fria-instance-claude-14.ttl,AI Dungeon GPT-3 offensive speech filter,"Latitude, developer of AI Dungeon","Latitude, OpenAI, AI Dungeon players","Latitude implemented a content moderation system to prevent AI Dungeon players from generating stories with sexual content involving minors. However, the system blocked more content than intended and raised privacy concerns as private stories were now being reviewed by moderators. A security vulnerability in the API was also discovered, allowing access to unpublished content.","Content moderation system, NLP/text analysis, GPT-3 large language model","To minimize sexual content, particularly involving minors, in the AI-generated stories",2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-dungeon-offensive-speech-filter,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"Latitude did not clearly communicate to users that their content was being filtered by an AI system, leading to confusion and speculation among players.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The content moderation system did not provide any indication of confidence levels or potential for errors in its filtering decisions, leading to situations where innocent content was blocked without explanation.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The system's content filtering decisions were presented as absolute, with no clear mechanism for users to contest or appeal blocked content, effectively shifting the burden of proof to the users.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"Users were not provided with clear explanations for why certain content was flagged or blocked, making it difficult for them to understand and adapt to the system's criteria.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The extent to which the AI system's decisions were reviewed or overridden by human moderators was not clearly communicated, leaving users uncertain about the final decision-making process.",Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"There was no clear process for users to seek redress or appeal content moderation decisions, potentially leading to permanent loss of creative work without recourse.",High,The AI system targets members of a specific social group,"While not intentionally discriminatory, the content moderation system disproportionately affected users creating certain types of content, potentially limiting creative expression for some groups.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","The system lacked clear mechanisms for users to report and address potential biases or discrimination in content moderation, making it difficult to improve the system's fairness.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The content moderation system may not have adequately accounted for diverse use cases or cultural contexts, potentially leading to over-censorship of certain groups or topics.",Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The content moderation system was applied universally to all users' stories, including private ones, without a mechanism to limit its scope to suspected violators or public content only.",Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals","Users had limited access to information about how their data was being stored, processed, or reviewed by the content moderation system, raising concerns about data transparency and user privacy.",High,There are no mechanisms for the user to exercise control over the processing of personal data,"Users had no control over how their private stories were processed or reviewed by the content moderation system, raising significant privacy concerns.",Very High,There are no measures to ensure the lawfulness of the processing of personal data,"The legal basis for processing users' private story data for content moderation purposes was not clearly established or communicated, raising questions about the lawfulness of the data processing.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,There was no clear information about procedures limiting access to users' personal data and story content to only what was necessary for content moderation purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","Users were not provided with clear mechanisms to access, rectify, or request erasure of their personal data and story content processed by the content moderation system.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The discovery of a security vulnerability in the AI Dungeon API that allowed access to unpublished content highlights the lack of robust security measures for protecting users' personal data.,Very High,There is no procedure to conduct a data protection impact assessment,"There is no mention of Latitude conducting a data protection impact assessment before implementing the content moderation system, which could have identified potential privacy and security risks.",High
fria-instance-claude-15.ttl,AI-generated article calls fake tanning 'racist',"The Irish Times, a major Irish newspaper","Unknown AI-assisted author posing as Adriana Acosta-Cortez, Irish Times editorial staff","The Irish Times published an AI-generated article claiming Irish women's use of fake tan was 'cultural appropriation'. The article was later discovered to be a hoax, with the author's identity and photo likely generated by AI. This incident highlighted vulnerabilities in the newspaper's vetting process and the challenges posed by AI in journalism.","Chatbot, NLP/text analysis, neural network, deep learning, machine learning, reinforcement learning for text generation","To generate text for an opinion article, inadvertently spreading misinformation",2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-article-calls-fake-tanning-racist,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The AI-generated article did not disclose its artificial nature, leading readers and the newspaper to believe it was written by a real person.",Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The AI-generated content provided no indication of its potential inaccuracy or bias, presenting its opinions as factual and authoritative.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The article presented its claims about cultural appropriation as absolute truth, potentially placing the burden on readers to disprove these claims rather than the author to substantiate them.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The AI system provided no explanation for its reasoning or criteria in labeling fake tanning as 'cultural appropriation', making it difficult for readers to critically evaluate the claims.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The extent of AI involvement in generating the article was not disclosed, leaving readers and the newspaper unaware of how much of the content was AI-generated versus human-written.",Very High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"There were no clear measures in place for addressing potential harm caused by the publication of AI-generated misinformation, beyond retracting the article and issuing an apology.",Medium,The AI system targets members of a specific social group,"The AI-generated article specifically targeted Irish women who use fake tan, potentially promoting discrimination against this group.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","The Irish Times lacked robust mechanisms to identify and correct biased or discriminatory content before publication, as evidenced by the article passing through the editorial process.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The AI system demonstrated a lack of cultural sensitivity and understanding of the Irish context, misrepresenting a common cosmetic practice as a form of cultural appropriation.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The AI-generated content was published without limitations, potentially affecting a wide audience and interfering with public discourse on cultural issues.",Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals",The Irish Times did not initially provide transparent access to information about the article's origin or the AI involvement in its creation.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data,Readers had no control over how their engagement with the article might be used to further train or refine AI systems generating similar content.,Low,There are no measures to ensure the lawfulness of the processing of personal data,The use of AI to generate a fake author profile raises questions about the lawfulness of creating and using fictional personal data.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,"The incident revealed a lack of procedures to limit access to and use of AI-generated personal data, as evidenced by the creation and publication of a fake author profile.",Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There were no clear mechanisms for individuals to exercise their rights regarding the AI-generated content, such as requesting corrections or deletions of false information.",Medium,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The incident highlighted a lack of security measures to prevent the creation and use of fake personal data in AI-generated content.,High,There is no procedure to conduct a data protection impact assessment,"There was no evidence of a data protection impact assessment being conducted before publishing AI-generated content, which could have identified potential risks and privacy concerns.",High
fria-instance-claude-16.ttl,"AI impersonation scams Canadian couple of USD 21,000",Unidentified scammers using AI voice cloning technology,"Elderly Canadian couple, Benjamin Perkins (son)","An elderly Canadian couple was defrauded of CAD 21,000 after receiving a call from a scammer posing as a lawyer, who used AI-generated voice cloning to impersonate their son. The scammer claimed the son had killed a US diplomat in a car accident and needed money for legal support. The couple only realized it was a scam after their real son called later that evening.",Deepfake audio technology for voice cloning,To defraud victims by impersonating family members in distress,2023-10-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-impersonation-scams-couple-of-usd-21000,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The AI-generated voice clone did not disclose its artificial nature, deliberately misleading the victims into believing they were speaking with their real son.",Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The AI-generated voice provided no indication of its potential inaccuracy, leading the victims to believe unquestioningly in the authenticity of the call.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The AI-generated voice presented a false scenario as absolute truth, leaving the victims with no reasonable way to verify the claims immediately.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The AI system provided no explanation for its ability to mimic the son's voice, making it impossible for the victims to critically evaluate the authenticity of the call.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The extent of AI involvement in generating the voice was not disclosed, leaving the victims unaware that they were interacting with an AI system rather than their son.",Very High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There were no clear measures in place for the victims to seek redress or recover their lost funds after falling victim to the AI-enabled scam.,Very High,The AI system targets members of a specific social group,"The AI-enabled scam specifically targeted elderly individuals, exploiting their potential vulnerability and limited familiarity with AI technology.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",There were no apparent mechanisms in place to flag or prevent the misuse of AI voice cloning technology for discriminatory or fraudulent purposes.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The AI voice cloning system did not appear to have safeguards against misuse for impersonation or fraud, particularly concerning vulnerable populations like the elderly.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The AI voice cloning technology was used without the consent or knowledge of the person being impersonated, violating their privacy and right to control their own voice.",Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals",The victims and the impersonated individual had no access to information about how their personal data (voice samples) were obtained or used in the AI system.,High,There are no mechanisms for the user to exercise control over the processing of personal data,"The impersonated individual had no control over how their voice data was collected, processed, or used in the AI voice cloning system.",Very High,There are no measures to ensure the lawfulness of the processing of personal data,"The use of AI to generate a fake voice based on publicly available data (YouTube videos) raises questions about the lawfulness of such data processing without consent.""

fria:FRIA-reportImpactLevel42 a fria:FRIA-reportImpactLevel ;
    fria:hasImpactLevelContent ""Very High",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,"There were no apparent procedures to limit access to or prevent misuse of personal voice data in AI voice cloning systems, as evidenced by the scammers' ability to create a convincing voice clone.",High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There were no clear mechanisms for individuals to exercise their rights regarding their voice data, such as requesting access to, correction, or deletion of voice samples used in AI systems.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","The incident highlighted a lack of security measures to prevent the unauthorized use of personal voice data in AI voice cloning systems, particularly when sourced from public platforms like YouTube.",Very High,There is no procedure to conduct a data protection impact assessment,"There was no evidence of data protection impact assessments being conducted for AI voice cloning technologies, which could have identified potential risks and privacy concerns related to their misuse.",High
fria-instance-claude-17.ttl,"AI invents 40,000 biochemical warfare agents",Researchers at Collaborations Pharmaceuticals,Fabio Urbina and colleagues,"Researchers tweaked an AI system usually used to predict the toxicity of pipeline drugs to invent 40,000 potentially lethal molecules in just six hours. The experiment demonstrates how easily medical AI technology can be misused to create biochemical weapons.","MegaSyn AI model, normally used for drug discovery, was inverted to seek toxic compounds. Used open-source toxicity datasets and generative models.",To demonstrate potential misuse of AI in drug discovery at the Spiez Convergence conference on biochemical security threats.,2022-03-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-invents-40000-biochemical-warfare-agents,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,The AI system does not communicate that its output is the result of an algorithmic decision. The researchers noted the ease with which the AI could generate potentially lethal molecules without explicitly stating its nature.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The AI system does not provide percentages or indications on the degree of likelihood that the output is correct/incorrect. The researchers noted that some generated compounds were predicted to be more toxic than VX, but acknowledged these were predictions that hadn't been verified.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The AI system produces outcomes that could be presented as absolute truth, potentially reversing the burden of proof. The researchers noted that the AI generated known chemical warfare agents it had never seen before, lending credibility to its other outputs.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"There is limited explanation of the reasons and criteria behind the AI system's output. The researchers noted that while they understood the process, the ease of replication could lead to misuse by those without full understanding.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"There is no clear indication of the extent to which the AI system influences the overall decision-making process. The researchers noted that while synthesis of the molecules would be an additional step, the AI's output could significantly influence decisions on which molecules to pursue.",Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There are no clear measures for redress in case of harm or adverse impact. The researchers highlighted the potential for misuse and the lack of existing safeguards in the AI drug discovery field.,Very High,The AI system targets members of a specific social group,"The AI system could potentially target specific groups if misused. While not explicitly stated in the report, the potential for creating targeted biochemical agents raises significant ethical concerns.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","There are limited mechanisms to flag and correct issues related to bias, discrimination, or poor performance. The researchers noted the lack of guidelines on misuse of AI technology in the field.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,The AI system's consideration of diversity and representativeness is not explicitly addressed. The focus on generating toxic compounds could lead to unintended consequences for various populations.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals,There is no mechanism to limit the deployment of the AI system to specific individuals or purposes. The researchers highlighted the ease of replicating their work with publicly available tools and datasets.,Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals","The data stored, recorded, and produced by the AI system may not be easily accessible to concerned individuals. The researchers debated whether to publish their findings due to potential misuse.",High,There are no mechanisms for the user to exercise control over the processing of personal data,There are limited mechanisms for users to exercise control over the processing of personal data. The use of open-source datasets raises concerns about data protection and control.,Medium,There are no measures to ensure the lawfulness of the processing of personal data,There are limited measures to ensure the lawfulness of the processing of personal data. The researchers noted the lack of guidelines and regulations in the AI drug discovery field regarding potential misuse.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,There are limited procedures to restrict access to personal data. The researchers highlighted the ease of access to toxicity datasets and the potential for misuse.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)",There is no clear mechanism allowing compliance with the exercise of data subject's rights. The researchers did not address this aspect specifically in their experiment.,Medium,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",There are limited specific measures to enhance the security of the processing of personal data. The researchers noted the lack of safeguards and the potential for misuse of the AI technology.,High,There is no procedure to conduct a data protection impact assessment,There is no clear procedure to conduct a data protection impact assessment. The researchers highlighted the need for greater awareness and guidelines in the AI drug discovery community.,High
fria-instance-claude-18.ttl,AI meal planner app suggests chlorine gas recipe,"Pak'nSave, a New Zealand-based supermarket chain",LLM understand: Pak'nSave developers and OpenAI (GPT-3.5 creators),"A chatbot called Savey Meal-bot, powered by GPT-3.5, was designed to generate meal suggestions and recipes from leftover ingredients. However, it produced dangerous recipes, including one for chlorine gas, when users input non-food items. This incident raised concerns about the safety and reliability of AI-generated content, especially when applied to potentially harmful scenarios.","The system uses GPT-3.5, a large language model, for natural language processing and recipe generation. It was trained on recipe data but lacked sufficient safeguards against generating harmful content.",The AI was intended to help customers creatively use leftover ingredients and reduce food waste during a cost of living crisis. It was designed to generate recipes based on user-input ingredients.,2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-meal-planner-app-suggests-chlorine-gas-recipe,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"While the system does have a disclaimer stating that it uses AI, it doesn't explicitly communicate that each individual recipe is an AI-generated output. This lack of clarity could lead users to trust the recipes without proper scrutiny.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The AI system does not provide any indication of the reliability or safety of its generated recipes. It presents all outputs with equal confidence, potentially leading users to trust dangerous recipes.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The AI system presents its recipes as valid suggestions without any caveats or warnings about potential dangers, placing the burden of safety assessment entirely on the user.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The system provides no explanation for why it suggests certain recipes or combinations of ingredients, making it difficult for users to understand or question the logic behind potentially dangerous suggestions.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,The system does not clarify the extent to which its suggestions should influence user decisions. It presents recipes without indicating that users should critically evaluate the safety and appropriateness of the suggestions.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"While the company stated they would fine-tune the bot's controls, there's no clear mechanism for users to report dangerous recipes or seek redress if harm occurs from following the AI's suggestions.",High,The AI system targets members of a specific social group,"While not explicitly targeting specific groups, the system's potential to generate harmful recipes could disproportionately affect vulnerable users who may be more likely to trust AI-generated content without question.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","The system lacks clear mechanisms for users to flag dangerous or inappropriate recipes. While the company stated they would fine-tune controls, there's no apparent user-facing system to report issues.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The system doesn't appear to consider diverse use cases or potential misuse scenarios. It generated dangerous recipes when given non-food items, indicating a lack of robust safety checks for various inputs.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"While the system has an age restriction (18+), there are no apparent mechanisms to verify user age or limit access based on potential risk factors.",Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals",The report doesn't mention any mechanism for users to access or review the data used to generate recipes or their own interaction history with the system.,Low,There are no mechanisms for the user to exercise control over the processing of personal data,The report doesn't mention any user controls over data processing or storage of their ingredient inputs or generated recipes.,Low,There are no measures to ensure the lawfulness of the processing of personal data,"While the system has terms of use, there's no specific mention of measures ensuring lawful processing of personal data in the context of recipe generation.",Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any specific procedures to limit access to user data or to ensure that only necessary data is collected for recipe generation.,Low,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","The report doesn't mention any mechanisms for users to access, rectify, or erase their data related to the recipe generation system.",Low,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention any specific security measures for personal data processing. The focus is more on the safety of the generated content rather than data security.,Low,There is no procedure to conduct a data protection impact assessment,"While the company mentioned they would 'keep fine-tuning' the bot, there's no explicit mention of conducting a data protection impact assessment.",Medium
fria-instance-claude-19.ttl,AI Portrait Ars racial bias,MIT-IBM Watson AI Lab,Mauro Martino and Luca Stornaiuolo,"AI Portrait Ars, an AI-powered portrait generator, drew controversy for whitening the skin of people of color and altering their facial features to conform more closely to European standards. The system was trained on a dataset of 15,000 portraits, predominantly from 15th century Western European Renaissance art, leading to biased outputs that failed to accurately represent diverse racial features.","The system uses a Generative Adversarial Network (GAN) trained on 15,000 portraits, mostly from Western European Renaissance art. It generates portraits in various historical artistic styles based on user-uploaded selfies.",The AI was intended to create artistic portraits from selfies and to highlight the importance of AI fairness by demonstrating how bias in training data affects outputs.,2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-portrait-ars-racial-bias,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"While the system is clearly presented as AI-generated art, it doesn't explicitly communicate to users that the specific alterations to their features are a result of algorithmic bias rather than intentional artistic choices.",Medium,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The system does not provide any indication of the accuracy or potential bias in its outputs, potentially leading users to believe the alterations to their features are somehow 'correct' or 'improved' versions.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The system presents its altered portraits as artistic interpretations, potentially reinforcing harmful beauty standards and racial biases without providing users a clear way to challenge or contextualize the outputs.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"While the creators explain the general process and dataset used, there's no detailed explanation provided to users about why specific alterations were made to their features in each generated portrait.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The system doesn't clearly communicate the extent to which the AI's biases influence the final portrait, potentially leading users to misinterpret the alterations as purely artistic choices rather than reflections of dataset bias.",Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There are no apparent measures for users to seek redress or correction if they feel the system's output has misrepresented or harmed them through racial bias.,High,The AI system targets members of a specific social group,"The system disproportionately alters the features of people of color, whitening skin tones and changing facial features to conform more closely to European standards, effectively discriminating against non-white users.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",There are no apparent mechanisms for users to flag or correct instances of racial bias or discrimination in the system's outputs.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The system's training data is heavily biased towards Western European art, failing to consider or represent the diversity of global populations and artistic traditions.",Very High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The system is publicly available without restrictions, potentially exposing vulnerable individuals to biased representations of themselves.",Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals","While the creators claim that user photos are deleted immediately after processing, there's no clear mechanism for users to verify this or access data about their interactions with the system.",Low,There are no mechanisms for the user to exercise control over the processing of personal data,Users have limited control over how their photos are processed or used by the system beyond the initial upload.,Medium,There are no measures to ensure the lawfulness of the processing of personal data,"While the creators claim to delete user photos immediately, there's limited information about other data protection measures or compliance with data protection laws.",Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,"While the creators state that photos are deleted immediately after processing, there's no clear information about procedures to limit access to personal data during the processing period or for any derived data.",Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There's no apparent mechanism for users to access, rectify, or erase their data beyond the immediate deletion of photos after processing.",Low,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention specific security measures for data processing beyond immediate deletion. There's no information about encryption or anonymization processes during the brief period when photos are being processed.,Medium,There is no procedure to conduct a data protection impact assessment,"While the creators acknowledge the bias in their dataset and its implications, there's no mention of a formal data protection impact assessment being conducted.",High
fria-instance-claude-20.ttl,AI satellite location spoofing,University of Washington researchers,Bo Zhao and colleagues,"Researchers demonstrated the ability to create convincing deepfake satellite images using generative adversarial networks (GANs). They simulated fake satellite imagery by combining features from different cities, highlighting the potential for misinformation and disinformation through manipulated geospatial data.","The system uses Generative Adversarial Networks (GANs) to create deepfake satellite images. The researchers also developed detection software to identify fake images based on characteristics like texture, contrast, and color.",The research aimed to raise awareness about the potential for deepfake geography and to challenge the perceived absolute reliability of satellite images. The study demonstrates both the creation and detection of fake satellite imagery.,2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-satellite-location-spoofing,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The deepfake satellite images created by the AI system do not inherently communicate that they are artificially generated, potentially leading viewers to believe they are authentic satellite imagery.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The generated fake satellite images do not include any indication of their artificial nature or likelihood of accuracy, potentially leading viewers to treat them as authentic without question.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The convincing nature of the deepfake satellite images could make it difficult for targets of disinformation to prove the falsity of the images, potentially shifting the burden of proof onto the accused.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The system does not provide explanations for how or why specific features are combined in the generated fake satellite images, making it difficult for users to critically evaluate the output.",Medium,There is no indication of the extent to which the AI system influences the overall decision-making process,"The system doesn't indicate the extent to which AI has influenced the creation of the fake satellite images, potentially leading viewers to make decisions based on false information without understanding the AI's role.",High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"While the researchers developed detection software, there are no clear measures for redress if fake satellite images are maliciously used to spread disinformation or cause harm.",High,The AI system targets members of a specific social group,"While not directly targeting specific groups, the potential for creating fake satellite imagery could be used to disproportionately affect certain geographic areas or populations.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","While detection software was developed, there are no clear mechanisms for flagging or correcting issues related to bias or discrimination in the generated fake satellite images.",Medium,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The system's ability to combine features from different cities might not adequately consider the diversity of global urban landscapes, potentially leading to misrepresentations.",Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The potential for creating fake satellite imagery raises privacy concerns, as there are no clear limitations on how this technology could be deployed or who could use it.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals",There's no mention of mechanisms for individuals or organizations to access or verify the data used to create fake satellite images that might affect them.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data,"The report doesn't mention any mechanisms for individuals to control how their data (e.g., images of their property) might be used in generating fake satellite imagery.",Medium,There are no measures to ensure the lawfulness of the processing of personal data,"The report doesn't discuss measures to ensure the lawful use of data in creating fake satellite images, which could potentially include personal or sensitive information.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to the data used in creating fake satellite images or to ensure that only necessary data is used.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There's no mention of mechanisms for individuals to access, rectify, or erase their data that might be used in the creation of fake satellite images.",Medium,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't discuss any specific security measures for protecting the data used in generating fake satellite images.,Medium,There is no procedure to conduct a data protection impact assessment,"While the researchers aimed to raise awareness about potential misuse, there's no mention of a formal data protection impact assessment for this technology.",High
fria-instance-claude-21.ttl,AI Stefanie Sun (AI孙燕姿),Various AI developers and content creators,Unknown developers using so-vits-svc fork software,"AI-cloned songs in the voice of Singapore-based Mandopop singer Stefanie Sun went viral on China's video platform Bilibili. The AI-generated songs, created using open-source software so-vits-svc fork, were nearly indistinguishable from Sun's actual voice. This incident raised concerns about copyright infringement and the potential impact on jobs in the music industry.","The system uses so-vits-svc fork, an open-source software that enables training AI models to speak or sing in any voice and language. It employs deep learning and generative adversarial networks (GANs) to clone voices.","The AI was used to generate new songs in Stefanie Sun's voice, creating content that the artist herself had not produced. This demonstrates the potential of AI in content creation but also raises ethical and legal questions.",2023-05-22,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-stefanie-sun,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The AI-generated songs do not explicitly communicate that they are artificial creations, potentially misleading listeners into believing they are authentic works by Stefanie Sun.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The AI-generated songs do not provide any indication of their artificial nature or the likelihood of errors, potentially leading listeners to believe they are authentic without question.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The convincing nature of the AI-generated songs could make it difficult for Stefanie Sun to prove that she didn't create these songs, potentially shifting the burden of proof onto the artist.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The system does not provide explanations for how or why specific songs are generated, making it difficult for users to critically evaluate the output.",Medium,There is no indication of the extent to which the AI system influences the overall decision-making process,"The system doesn't indicate the extent to which AI has influenced the creation of the songs, potentially leading listeners to make decisions based on false assumptions about the artist's work.",High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There are no clear measures for redress if the AI-generated songs cause harm to Stefanie Sun's career or reputation.,High,The AI system targets members of a specific social group,"While not directly discriminatory, the system's focus on a specific artist could lead to unequal treatment or impact on certain artists over others.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",There are no apparent mechanisms for users or affected parties to flag or correct issues related to the AI-generated content.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,The system's focus on a single artist doesn't consider the diversity of the music industry or potential problematic use cases.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The open-source nature of the software means there are no limitations on who can use it to clone voices, potentially leading to privacy violations.",Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals",There's no mention of mechanisms for Stefanie Sun or other affected artists to access or control the data used to clone their voices.,High,There are no mechanisms for the user to exercise control over the processing of personal data,There are no apparent mechanisms for Stefanie Sun to control how her voice data is used or processed by the AI system.,Very High,There are no measures to ensure the lawfulness of the processing of personal data,The report indicates a lack of clear legal guidelines or measures to ensure the lawful use of artists' voice data in AI systems.,Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The open-source nature of the software and lack of regulations mean there are no apparent limitations on access to or use of personal voice data.,Very High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There's no mention of mechanisms for Stefanie Sun or other artists to access, rectify, or erase their voice data used in the AI system.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention any specific security measures for protecting the voice data used in generating AI songs.,High,There is no procedure to conduct a data protection impact assessment,There's no mention of any data protection impact assessment being conducted for this AI voice cloning technology.,High
fria-instance-claude-22.ttl,AI text detector language bias,Stanford University researchers,James Zou and colleagues,"Stanford University researchers found that seven popular AI writing detection tools frequently misclassify non-native English writing as AI-generated. Over half of the essays written by non-native English speakers were flagged as AI-generated, while over 90% of essays by native English-speaking eighth graders were classified as human-generated. This bias raises concerns about discrimination against non-native English speakers in academic and professional settings.",The study tested seven popular GPT detectors using natural language processing and machine learning techniques. These detectors often use metrics like 'text perplexity' to identify AI-generated content.,"The AI detectors were designed to identify AI-generated text, particularly in academic and professional contexts. However, the study reveals significant biases in their performance when analyzing texts from non-native English speakers.",2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-text-detector-language-bias,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The AI text detectors do not clearly communicate that their classifications are based on algorithmic decisions, potentially leading users to treat the results as definitive without understanding the system's limitations or biases.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The AI text detectors often provide binary classifications (human-written or AI-generated) without indicating the confidence level or potential for error, which can lead to unjust accusations of cheating or fraud.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","When AI detectors flag text as AI-generated, it often places the burden of proof on the writer to prove their innocence, which can be particularly challenging for non-native English speakers.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The AI text detectors often do not provide clear explanations for their classifications, making it difficult for users to understand or challenge the results, especially in cases of false positives.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The impact of AI text detectors on decision-making processes in academic and professional settings is not clearly communicated, potentially leading to over-reliance on these tools.",High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"There are limited or no clear measures for redress when AI text detectors incorrectly flag human-written text as AI-generated, potentially causing significant harm to non-native English speakers' academic or professional opportunities.",Very High,The AI system targets members of a specific social group,"The AI text detectors disproportionately misclassify writing by non-native English speakers as AI-generated, effectively discriminating against this group.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","There are limited or no apparent mechanisms for users to flag or correct biases in AI text detectors, particularly regarding their performance on non-native English writing.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The AI text detectors do not adequately consider the diversity of writing styles and language proficiency levels, particularly for non-native English speakers, leading to biased results.",Very High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The widespread use of AI text detectors in academic and professional settings may lead to arbitrary interference with individuals' privacy, particularly affecting non-native English speakers.",Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals",There's no mention of mechanisms for individuals to access or review the data used by AI text detectors to make their classifications.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data,The report doesn't mention any mechanisms for users to control how their text data is processed or used by AI text detectors.,Medium,There are no measures to ensure the lawfulness of the processing of personal data,The report doesn't discuss specific measures to ensure the lawful use of personal data in AI text detection systems.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to personal data used in AI text detection systems or to ensure that only necessary data is used.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There's no mention of mechanisms for individuals to access, rectify, or erase their data used in AI text detection systems.",Medium,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't discuss any specific security measures for protecting the data used in AI text detection systems.,Medium,There is no procedure to conduct a data protection impact assessment,"There's no mention of any data protection impact assessment being conducted for AI text detection systems, particularly regarding their impact on non-native English speakers.",High
fria-instance-claude-23.ttl,Airbnb Smart Pricing algorithm racial bias,Airbnb,"Carnegie Mellon University researchers, led by Professor Param Vir Singh","A Carnegie Mellon University study found that Airbnb's 'Smart Pricing' algorithm, which dynamically adjusts rental prices based on demand, is exacerbating racial inequality. While the algorithm benefits both Black and White hosts who use it, Black hosts were 41% less likely to adopt the tool, leading to an overall increase in the revenue gap between White and Black hosts.","The Smart Pricing algorithm uses machine learning to analyze various factors including property type, location, seasonality, and demand to automatically adjust rental prices.","The algorithm was introduced in 2015 to help hosts optimize their pricing and increase their revenue. However, the study reveals unintended consequences in terms of racial economic disparities.",2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/airbnb-smart-pricing-algorithm-racism,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"While Airbnb offers the Smart Pricing tool to hosts, it may not clearly communicate that the pricing suggestions are the result of an algorithmic decision, which could impact hosts' understanding of how prices are determined.",Medium,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The Smart Pricing algorithm does not appear to provide confidence levels or error margins for its pricing suggestions, which could lead hosts to assume the suggested prices are optimal without considering potential inaccuracies.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The Smart Pricing algorithm's suggestions may be perceived as optimal by hosts, potentially discouraging them from setting their own prices based on personal knowledge or preferences.",Medium,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The report doesn't mention whether Airbnb provides clear explanations to hosts about how the Smart Pricing algorithm determines its suggested prices, potentially limiting hosts' ability to make informed decisions.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,The study doesn't indicate whether Airbnb clearly communicates to hosts how much influence the Smart Pricing algorithm has on their overall pricing strategy and revenue.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"The report doesn't mention any specific measures for hosts to seek redress if they experience negative impacts from using the Smart Pricing algorithm, such as unexpected revenue losses.",High,The AI system targets members of a specific social group,"While the Smart Pricing algorithm doesn't explicitly target specific racial groups, its lower adoption rate among Black hosts has led to increased racial economic disparities, indicating a form of indirect discrimination.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",The report doesn't mention any specific mechanisms for hosts or users to flag potential biases or discrimination issues in the Smart Pricing algorithm.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The Smart Pricing algorithm appears to use pooled data from all hosts, potentially ignoring racial differences and specific challenges faced by Black hosts, leading to suboptimal pricing suggestions for this group.",Very High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The Smart Pricing algorithm is available to all hosts, but the report doesn't mention any mechanisms to limit its use in cases where it might lead to unfair outcomes.",Low,"The data stored, recorded, and produced are not easily accessible to concerned individuals",The report doesn't mention whether hosts have access to the data used by the Smart Pricing algorithm to make its pricing suggestions.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data,The report doesn't mention any mechanisms for hosts to control how their personal data or listing information is used in the Smart Pricing algorithm.,Medium,There are no measures to ensure the lawfulness of the processing of personal data,The report doesn't discuss specific measures to ensure the lawful use of personal data in the Smart Pricing algorithm.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to personal data used in the Smart Pricing algorithm or to ensure that only necessary data is used.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","The report doesn't mention any mechanisms for hosts to access, rectify, or erase their data used in the Smart Pricing algorithm.",Medium,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't discuss any specific security measures for protecting the data used in the Smart Pricing algorithm.,Medium,There is no procedure to conduct a data protection impact assessment,"The report doesn't mention any data protection impact assessment being conducted for the Smart Pricing algorithm, particularly regarding its impact on racial economic disparities.",High
fria-instance-claude-24.ttl,Ajin USA worker crushed to death by robot,"Ajin USA, auto parts manufacturer",Regina Allen Elsea (victim),"Regina Allen Elsea, a 20-year-old worker at Ajin USA, was killed when a robot she was trying to fix restarted unexpectedly and crushed her. The incident occurred when Elsea entered a robotic cell to clear a sensor fault during an assembly line stoppage. Ajin USA was found to have violated federal safety standards, including failing to enforce lockout/tagout procedures that could have prevented the incident.",Industrial robot in an auto parts manufacturing facility. The exact type and capabilities of the robot are not specified in the report.,"The robot was part of the manufacturing process at Ajin USA, an auto parts supplier for Hyundai and Kia. The incident occurred during an attempt to fix a sensor fault and restart a stopped assembly line.",2020-11-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ajin-usa-worker-crushed-to-death-by-robot,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The report does not indicate that the robot communicated its operational status or decision-making process to workers, which may have contributed to the accident.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The report does not mention any system for indicating the likelihood of errors or malfunctions in the robot's operation, which could have alerted workers to potential dangers.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The report does not suggest that the robot's actions were presented as infallible, but the lack of safety procedures indicates a systemic failure to question or safeguard against potential robotic malfunctions.",Medium,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The report does not mention any system for explaining the robot's actions or decision-making process to workers, which could have improved safety awareness.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The report does not provide information on the robot's level of autonomy or its role in the overall manufacturing process, which could have impacted safety procedures.",Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"While the company faced legal consequences, the report does not mention specific measures for worker redress or compensation in case of accidents, beyond general legal recourse.",High,The AI system targets members of a specific social group,"The report does not indicate that the robot targeted specific groups, but the overall safety violations at the plant could disproportionately affect vulnerable workers, including temporary staff.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","The report highlights a lack of effective mechanisms to flag and correct safety issues, including the consistent failure to enforce lockout/tagout procedures.",Very High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The report does not indicate that the robotic system was designed with consideration for diverse worker populations or potentially problematic scenarios, such as maintenance during line stoppages.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The report does not indicate any privacy concerns related to the robotic system, as the primary issue was physical safety rather than data privacy.",Low,"The data stored, recorded, and produced are not easily accessible to concerned individuals",The report does not mention any data storage or access issues related to the robotic system.,Low,There are no mechanisms for the user to exercise control over the processing of personal data,The report does not mention any data processing or control issues related to the robotic system.,Low,There are no measures to ensure the lawfulness of the processing of personal data,The report does not address data processing or lawfulness issues related to the robotic system.,Low,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report does not mention any personal data access or limitation procedures related to the robotic system.,Low,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)",The report does not address any data subject rights or related mechanisms for the robotic system.,Low,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","The report does not mention any data security measures for the robotic system, as the primary focus is on physical safety rather than data protection.",Low,There is no procedure to conduct a data protection impact assessment,"The report does not mention any data protection impact assessment for the robotic system. However, it does highlight the need for better safety assessments and compliance with OSHA standards.",Medium
fria-instance-claude-25.ttl,Alexei Navalny smart voting bot blocking,"Apple, Google, and Telegram",Alexei Navalny and his team,"Apple, Google, and Telegram blocked a smart voting chatbot devised by jailed Russian opposition leader Alexei Navalny on the day Russia started voting in parliamentary elections. The bot was designed to help voters identify candidates capable of defeating pro-Kremlin United Russia party candidates. The tech companies cited compliance with Russian law and election silence rules as reasons for the removal.",The system uses a chatbot and mobile app to provide voting recommendations based on user input and tactical voting strategies.,"The smart voting bot was intended to facilitate tactical voting against pro-Kremlin candidates in Russian parliamentary elections, as part of broader opposition efforts led by Alexei Navalny.",2021-09-17,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alexei-navalny-smart-voting-bot,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The smart voting bot likely communicated its nature as an algorithmic tool for voting recommendations, but its removal by tech companies without clear explanation could be seen as a lack of transparency in decision-making.",Medium,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The report doesn't mention whether the smart voting bot provided confidence levels for its recommendations, which could potentially lead users to view its suggestions as definitive.",Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The smart voting bot's recommendations could be seen as influencing voters' decisions, but it's not clear if it presented its suggestions as absolute truth. The removal of the bot, however, could be seen as depriving the opposition of a chance to counter pro-government narratives.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The report doesn't mention whether the smart voting bot provided explanations for its recommendations, which could potentially limit users' understanding of the suggested voting strategies.",Medium,There is no indication of the extent to which the AI system influences the overall decision-making process,"The smart voting bot's influence on voters' decision-making process is not clearly quantified, but its removal was seen as a significant blow to opposition efforts, suggesting it had a notable impact.",High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"The report doesn't mention any measures for redress regarding the bot's recommendations. The removal of the bot itself was seen as harmful by opposition supporters, with limited recourse for challenging the decision.",High,The AI system targets members of a specific social group,"The smart voting bot was designed to target pro-Kremlin candidates, which could be seen as discrimination against a specific political group. However, it was intended as a counter to perceived unfair advantages of the ruling party.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",The report doesn't mention any mechanisms for users to flag issues with the smart voting bot's recommendations. The bot's removal eliminated any possibility for such feedback.,Medium,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The smart voting bot was designed to promote opposition candidates, which could be seen as addressing a lack of diversity in Russian politics. However, its removal may have disproportionately affected opposition supporters.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The smart voting bot was publicly available, but its removal could be seen as an arbitrary interference with users' privacy and right to access information.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals","The report doesn't mention data accessibility issues related to the smart voting bot. However, its removal effectively made any stored data inaccessible to users.",Medium,There are no mechanisms for the user to exercise control over the processing of personal data,The report doesn't mention data control mechanisms for the smart voting bot. The removal of the bot raises questions about the fate of any user data it may have collected.,Medium,There are no measures to ensure the lawfulness of the processing of personal data,"The report doesn't discuss data processing measures for the smart voting bot. The removal of the bot was justified by tech companies as compliance with Russian law, but this raises questions about data protection standards.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to personal data used by the smart voting bot. The sudden removal of the bot raises concerns about what happened to any collected data.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","The report doesn't mention any mechanisms for users to access, rectify, or erase their data from the smart voting bot. The removal of the bot may have made it impossible for users to exercise these rights.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention any specific security measures for data processing by the smart voting bot. The forced removal of the bot raises questions about the security of any data it may have collected.,Medium,There is no procedure to conduct a data protection impact assessment,"The report doesn't mention any data protection impact assessment for the smart voting bot. The sudden removal of the bot suggests that such assessments, if conducted, did not adequately consider the potential impact on users' rights.",High
fria-instance-claude-26.ttl,"Alfi personalised, real-time advertising","Alfi, a Miami-based digital marketing company","Alfi Inc., Uber, Lyft","Alfi, a digital marketing company, uses AI and computer vision to analyze facial cues and provide targeted advertising in ride-sharing vehicles. The company plans to install tablets with facial recognition technology in 10,000 Uber and Lyft cabs across the US. This has raised privacy concerns and prompted inquiries from US senators.","The system uses computer vision and AI to analyze facial cues, detect age, gender, and ethnicity, and match relevant advertising content. It also tracks viewer reactions to ads.","The technology aims to provide personalized, targeted advertising in real-time to ride-sharing passengers, while also offering additional revenue for drivers.",2021-07-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alfi-personalised-real-time-advertising,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The Alfi system does not appear to explicitly inform passengers that the advertisements they see are the result of AI-driven facial analysis, potentially misleading users about the nature of the content they're viewing.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The report does not indicate that Alfi provides any information about the accuracy or confidence levels of its facial analysis and ad targeting, potentially leading to misinterpretation of the system's capabilities.",Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","While not directly related to legal proceedings, the Alfi system's facial analysis and ad targeting could lead to assumptions about passengers' interests and demographics without providing them an opportunity to correct or challenge these assumptions.",Medium,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The report does not indicate that Alfi provides any explanation to passengers about why they are seeing certain advertisements or how their facial features are being analyzed, limiting transparency and user understanding.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The report does not provide information on how much influence Alfi's AI system has on the overall ad selection process, potentially obscuring the role of human oversight or other factors.",Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,The report does not mention any measures for passengers to seek redress if they feel their privacy has been violated or if they experience any adverse effects from the targeted advertising.,High,The AI system targets members of a specific social group,"Alfi's system explicitly targets ads based on age, gender, and ethnicity, which could lead to discriminatory practices in advertising and reinforce societal biases.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",The report does not mention any mechanisms for users or third parties to flag potential biases or discrimination in Alfi's ad targeting system.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"While Alfi claims to target ads based on demographics, there's no information on how the system ensures fair representation or handles edge cases that might lead to discriminatory outcomes.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"Alfi's system is deployed indiscriminately to all passengers in equipped vehicles, raising concerns about privacy violations for individuals who have not consented to facial analysis.",Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals","While Alfi claims not to store personal data, the report doesn't mention any mechanisms for individuals to access or verify what data has been collected or inferred about them.",High,There are no mechanisms for the user to exercise control over the processing of personal data,The report does not mention any mechanisms for passengers to opt-out of facial analysis or control how their data is processed by Alfi's system.,Very High,There are no measures to ensure the lawfulness of the processing of personal data,"While Alfi claims compliance with GDPR and CCPA, the report raises questions about the lawfulness of processing biometric data without explicit consent in ride-sharing vehicles.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,"While Alfi claims not to store personal data, the report doesn't provide details on how access to real-time facial analysis data is limited or how the company ensures only necessary data is processed.",High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","The report doesn't mention any mechanisms for passengers to access, rectify, or erase their data from Alfi's system, which is particularly concerning given the sensitive nature of facial analysis.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)","While Alfi claims to not store personal data, the report doesn't provide details on security measures for real-time data processing or how aggregated data is protected.",High,There is no procedure to conduct a data protection impact assessment,"The report doesn't mention any data protection impact assessment conducted by Alfi, despite the potentially significant privacy implications of their facial analysis technology in ride-sharing vehicles.",Very High
fria-instance-claude-27.ttl,Algorithm misses gambling addict red flags,"Betfair, owned by Flutter UKI","Luke Ashton (victim), Annie Ashton (wife), Richard Clarke (managing director of customer relations for Flutter UKI)","Luke Ashton, a gambling addict, committed suicide in April 2021 after accumulating large debts. Betfair's algorithm had categorized him as a 'low-risk' customer, missing red flags in his betting patterns. The algorithm failed to trigger human intervention that might have restricted his gambling, despite him gambling over 100 times a day and building up debts of £18,000.",Betfair used a machine learning algorithm that daily analyzed 277 elements of customers' betting activities to detect problem gamblers. The system was designed to flag high-risk customers for intervention by the player protection team.,"The algorithm was intended to identify at-risk gamblers and trigger interventions to prevent harm. However, it failed to correctly assess the risk level of Luke Ashton, leading to a lack of meaningful intervention.",2021-04-22,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/algorithm-misses-gambling-addict-red-flags,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The algorithm's classification of Luke Ashton as 'low-risk' was not communicated to him, nor was he informed that his gambling activity was being algorithmically monitored and assessed.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The algorithm did not provide any indication of confidence levels in its risk assessment, potentially leading to an unwarranted sense of security for both Betfair and the customer.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The algorithm's classification of Luke Ashton as 'low-risk' was treated as definitive, preventing further scrutiny or intervention that might have identified his problematic gambling behavior.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The reasons and criteria used by the algorithm to classify Luke Ashton as 'low-risk' were not explained, making it difficult for both the company and the customer to understand and potentially challenge the assessment.",High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The extent to which the algorithm's risk assessment influenced Betfair's decision not to intervene in Luke Ashton's case was not clear, potentially leading to over-reliance on an imperfect system.",High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"There were no apparent measures in place for redress or correction if the algorithm incorrectly classified a customer's risk level, as happened in Luke Ashton's case.",Very High,The AI system targets members of a specific social group,"While not explicitly targeting a specific social group, the algorithm's failure to correctly identify at-risk gamblers could disproportionately affect vulnerable individuals prone to addiction.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance","The report does not mention any mechanisms to flag or correct issues with the algorithm's performance, despite clear evidence of its failure to identify a high-risk customer.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The algorithm failed to adequately consider the diverse patterns of problematic gambling behavior, missing clear red flags in Luke Ashton's case despite his history of self-exclusion.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The algorithm was applied to all customers, including those like Luke Ashton who had previously self-excluded, without clear limitations or safeguards.",Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals",The report does not indicate that Luke Ashton or his family had any access to the data or risk assessments produced by the algorithm about his gambling behavior.,High,There are no mechanisms for the user to exercise control over the processing of personal data,The report does not mention any mechanisms for users like Luke Ashton to control how their gambling data was processed or used by the risk assessment algorithm.,High,There are no measures to ensure the lawfulness of the processing of personal data,"While Betfair likely had legal grounds to process customer data for risk assessment, the report doesn't mention specific measures to ensure the lawfulness of this processing, particularly in light of the algorithm's failure.",Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to personal gambling data or ensure only necessary data was used in the risk assessment algorithm.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","The report doesn't mention any mechanisms for customers like Luke Ashton to access, rectify, or erase their data used in the risk assessment algorithm.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention any specific security measures for protecting the personal gambling data used in the risk assessment algorithm.,Medium,There is no procedure to conduct a data protection impact assessment,"The report doesn't mention any data protection impact assessment conducted for the risk assessment algorithm, despite its potential to significantly affect individuals' rights and freedoms.",High
fria-instance-claude-28.ttl,Allocation algorithm wrongly places thousands of Italian teachers,Italian government,HP Enterprise and Finmeccanica,"An algorithm used by the Italian government in 2016 to allocate teaching positions resulted in many teachers being relocated to inappropriate locations. The system was found to be fully automated, unmanageable, and full of bugs.","Resource allocation algorithm designed to assess and score teachers based on criteria including work experience, performance and preferred destinations.",The algorithm was intended to match teachers with the most appropriate teaching vacancies across Italy.,2016-08-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allocation-algorithm-wrongly-places-thousands-of-italian-teachers,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,The allocation algorithm did not clearly communicate to teachers that their placements were determined by an automated system.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating",The algorithm did not provide any indication of confidence levels or potential for errors in its allocation decisions.,Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The algorithm's decisions were presented as final, with no clear process for teachers to challenge or appeal inappropriate allocations.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The algorithm was described as 'unmanageable' and 'full of bugs', suggesting a lack of transparency in its decision-making process.",Very High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The system was fully automated, suggesting it had complete control over teacher allocations with no human oversight.",Very High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,"The incident led to multiple legal complaints and lawsuits, suggesting a lack of adequate redress mechanisms within the system itself.",Very High,The AI system targets members of a specific social group,"The system's errors disproportionately affected teachers, potentially impacting their careers and personal lives.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",The system's 'unmanageable' nature suggests a lack of mechanisms to identify and correct issues of bias or poor performance.,Very High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,The algorithm's inappropriate allocations suggest it did not adequately consider diverse teacher circumstances or complex allocation scenarios.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals,"The system was applied to all teachers seeking positions, without apparent limitations or safeguards.",Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals",The system's opaque nature suggests that teachers had limited access to the data used to make allocation decisions.,High,There are no mechanisms for the user to exercise control over the processing of personal data,There's no indication that teachers had any control over how their personal data was used in the allocation process.,High,There are no measures to ensure the lawfulness of the processing of personal data,The system's bugs and unmanageability raise questions about the lawfulness and accuracy of its data processing.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to teachers' personal data or ensure only necessary data was used in the allocation process.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","The report doesn't mention any mechanisms for teachers to access, rectify, or erase their data used in the allocation process.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention any specific security measures for protecting teachers' personal data in the allocation system.,Medium,There is no procedure to conduct a data protection impact assessment,There's no indication that a data protection impact assessment was conducted before implementing this large-scale automated decision-making system.,Very High
fria-instance-claude-29.ttl,Allstate car insurance 'suckers list' overcharging,Allstate Insurance Company,"The Markup, Consumer Reports","An investigation found that Allstate used a 'customer retention model' algorithm to identify and overcharge customers already paying high premiums, while keeping rates stable for 'thriftier' customers. The algorithm also limited discounts for customers owed large refunds, disproportionately affecting seniors.","Price adjustment algorithm called a 'customer retention model', containing dozens of variables to adjust insurance premiums.","The algorithm was intended to adjust insurance premiums in line with Allstate's new risk model, ostensibly to limit policy cancellations from sudden price changes.",2021-02-25,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allstate-car-insurance-suckers-list-overcharging,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"Allstate did not inform customers that their premiums were being adjusted by an algorithm, potentially misleading them about the basis for price changes.",Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating",The algorithm did not provide any indication of confidence levels or potential for errors in its pricing decisions.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The algorithm's decisions were presented as final, with no clear process for customers to challenge or appeal unfair pricing.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,Allstate did not provide clear explanations to customers about how their premiums were calculated or why they might have increased.,Very High,There is no indication of the extent to which the AI system influences the overall decision-making process,The extent to which the algorithm influenced final pricing decisions was not made clear to customers or regulators.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There were no apparent measures for customers to seek redress if they were unfairly overcharged by the algorithm.,Very High,The AI system targets members of a specific social group,"The algorithm disproportionately affected middle-aged and senior customers, potentially discriminating based on age.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",There were no apparent mechanisms to identify or correct biases in the algorithm's pricing decisions.,Very High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The algorithm did not appear to consider the diverse circumstances of customers, particularly those who might be less likely to shop around for better rates.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals,The algorithm was applied to all customers without apparent limitations or safeguards.,Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals",Customers did not have access to the data or criteria used by the algorithm to make pricing decisions about their policies.,High,There are no mechanisms for the user to exercise control over the processing of personal data,Customers had no apparent control over how their personal data was used in the pricing algorithm.,High,There are no measures to ensure the lawfulness of the processing of personal data,"The lawfulness of the data processing was called into question, as evidenced by Maryland's rejection of the plan as discriminatory.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to customer data or ensure only necessary data was used in the pricing algorithm.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","The report doesn't mention any mechanisms for customers to access, rectify, or erase their data used in the pricing algorithm.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention any specific security measures for protecting customer data used in the pricing algorithm.,Medium,There is no procedure to conduct a data protection impact assessment,There's no indication that Allstate conducted a data protection impact assessment before implementing this pricing algorithm.,High
fria-instance-claude-30.ttl,"Alonzo Sawyer facial recognition wrongful arrest, jailing","Maryland Transit Administration Police, Baltimore Police Department",Intelligence analyst at Maryland Transit Administration Police,"54-year-old Alonzo Sawyer was wrongfully arrested and jailed for nine days due to a faulty facial recognition match from CCTV footage. The analysis failed to account for significant differences between Sawyer and the actual suspect, including age, height, facial features, and gait.",Facial recognition technology applied to CCTV footage,The facial recognition technology was used to identify suspects in a bus assault and theft case.,2022-05-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alonzo-sawyer-facial-recognition-mistaken-arrest,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision,"The facial recognition system did not clearly communicate that its match was based on an algorithmic decision, leading to an unwarranted arrest.",Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating","The system did not provide any indication of the confidence level or potential for error in its facial recognition match, leading to an unjustified arrest.",Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it","The facial recognition match was treated as definitive evidence, shifting the burden of proof onto Sawyer to prove his innocence despite clear discrepancies.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand,"The system provided no explanation for its match, making it difficult for Sawyer or his wife to understand or challenge the decision.",Very High,There is no indication of the extent to which the AI system influences the overall decision-making process,"The extent to which the facial recognition match influenced the decision to arrest Sawyer was not clear, suggesting over-reliance on the technology.",High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact,There were no apparent measures for Sawyer to seek immediate redress or compensation for his wrongful arrest and nine-day imprisonment.,Very High,The AI system targets members of a specific social group,"The incident raises concerns about potential racial bias in facial recognition systems, as Sawyer is Black and such systems have been shown to be less accurate for people of color.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance",There were no apparent mechanisms to flag or correct the system's poor performance or potential bias in this case.,Very High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases,"The facial recognition system failed to account for diversity in appearance and physical characteristics, leading to a false match despite significant differences.",Very High,There is no mechanism to limit the deployment of the AI system to suspected individuals,The facial recognition system was applied broadly to CCTV footage without apparent limitations or safeguards.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals","Sawyer and his wife had limited access to the data and reasoning behind the facial recognition match, hindering their ability to challenge it effectively.",High,There are no mechanisms for the user to exercise control over the processing of personal data,There were no apparent mechanisms for Sawyer to control how his personal data or image was processed by the facial recognition system.,High,There are no measures to ensure the lawfulness of the processing of personal data,The report doesn't mention any specific measures to ensure the lawful processing of personal data in the facial recognition system.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes,The report doesn't mention any procedures to limit access to personal data or ensure only necessary data was used in the facial recognition process.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual)","There's no mention of mechanisms for Sawyer to access, rectify, or erase his data used in the facial recognition system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation)",The report doesn't mention any specific security measures for protecting personal data used in the facial recognition system.,Medium,There is no procedure to conduct a data protection impact assessment,There's no indication that a data protection impact assessment was conducted before implementing this facial recognition system for law enforcement use.,Very High
fria-instance-claude-31.ttl,Amazon algorithms promote vaccine misinformation,Content recommendation system,"Amazon, Waterstones, Foyles","Amazon's recommendation algorithms promote anti-vaccination and health misinformation books, apparel, and other products. The system pushes users interested in these products towards more related products, creating a 'filter bubble' effect.",Content recommendation system using algorithms based on user behavior and product attributes.,Recommend content and products to users based on their search queries and browsing history.,2023-07-11,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-algorithms-promote-vaccine-misinformation,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The AI system does not communicate that recommendations are the result of algorithmic decisions. It presents anti-vaccination books and products without clear indication of their controversial nature or potential misinformation.,High,,,,,,,,,,,,,,,,,,,,,,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The AI system does not consider the diversity and representativeness of the population. It promotes anti-vaccination content that can disproportionately affect vulnerable groups or those seeking accurate health information.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"The system's recommendations of anti-vaccination and health misinformation books may interfere with the right to receive accurate information, potentially impacting public health decisions.",High,,,,There are no mechanisms for the user to exercise control over the processing of personal data.,"The AI system uses personal data to create personalized recommendations, potentially creating 'filter bubbles' that reinforce misinformation. There's no clear mechanism for users to control or understand how their data is being used to generate these recommendations.",Medium,There are no measures to ensure the lawfulness of the processing of personal data.,There are no clear measures to ensure the lawfulness of the processing of personal data for recommendation purposes. The system uses user data to create personalized recommendations without explicit consent for this use.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks clear procedures to limit access to personal data used for recommendations. It's unclear how Amazon restricts access to user data within its organization or with third-party sellers.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","The system doesn't provide a clear mechanism for users to access, rectify, or erase their data related to the recommendation algorithm. Users have limited control over how their data influences book recommendations.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","The report doesn't mention any specific measures to enhance the security of personal data processing in the recommendation system. It's unclear if Amazon employs encryption, anonymization, or aggregation techniques for user data used in the algorithm.",Medium,There is no procedure to conduct a data protection impact assessment.,"There's no mention of a data protection impact assessment for the recommendation system. Given the potential risks of promoting misinformation, such an assessment would be crucial.",High
fria-instance-claude-32.ttl,Amazon Astro home robot,"Robotics, Computer vision, Facial recognition","Amazon, VICE, CCS Insight, University of Washington","Amazon launched Astro, a home robot powered by Alexa smart home technology and facial recognition. It can patrol homes, notify owners of unusual activities, and check on pets. However, leaked documents suggest performance issues, privacy concerns, and potential safety risks.","Autonomous robot with cameras, facial recognition, Alexa integration, and mobility capabilities. Uses AI and machine learning for navigation and person recognition.","Strengthen home security, provide remote home monitoring, and assist with everyday tasks",2021-09-28,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-astro-home-robot,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Astro does not clearly communicate that its decisions and actions are based on algorithmic processing. It may identify and follow 'strangers' without explicitly informing users of the algorithmic nature of these decisions.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","Astro does not provide clear indications of the likelihood of errors in its identification or decision-making processes, potentially leading to unquestioned trust in its judgments.",Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",Astro's surveillance capabilities and potential to identify 'strangers' may lead to presumptions of guilt without proper due process.,High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"The decision-making process of Astro, particularly in identifying 'strangers' or unusual activities, is not transparent to users.",High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which Astro's decisions influence overall home security and user actions is not clearly communicated.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There is no clear mechanism for redress if Astro makes errors or causes harm through its actions or decisions.,High,The AI system targets members of a specific social group.,"Astro's facial recognition system may have biases or perform poorly, potentially leading to discrimination in identifying 'strangers' or authorized household members.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no clear mechanisms to identify and correct biases or discrimination in Astro's decision-making processes.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The training data and algorithms used for Astro's facial recognition and decision-making may not adequately represent diverse populations.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"Astro's constant surveillance may inhibit freedom of expression within the home, as individuals may feel constantly monitored.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The data collected and stored by Astro may not be easily accessible to the individuals being monitored.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,"Astro collects extensive personal data, including facial recognition data and home layout information. There are concerns about data security and the extent of user control over this data.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness and consent mechanisms for Astro's data collection and processing are not clearly defined.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to personal data collected by Astro to only necessary parties and purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There's no clear mechanism for users to access, rectify, or erase their personal data collected by Astro.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The security measures for protecting personal data collected and processed by Astro are not clearly defined or communicated.,High,There is no procedure to conduct a data protection impact assessment.,"There's no clear indication that Amazon has conducted a comprehensive data protection impact assessment for Astro, despite its significant privacy implications.",High
fria-instance-claude-33.ttl,Amazon UK automated pricing glitch,Pricing automation,"Amazon, Repricer Express, third-party sellers","A technical glitch in Repricer Express software caused thousands of products on Amazon UK Marketplace to be priced at 1p, resulting in significant losses for small businesses. Amazon cancelled most orders but some were processed, leaving sellers facing potential bankruptcy.",Automated pricing software (Repricer Express) used to adjust prices of third-party products on Amazon Marketplace,Change product pricing to ensure competitiveness,2014-12-12,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-automated-pricing-glitch,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The automated pricing system did not communicate that price changes were the result of algorithmic decisions, leading to unexpected and damaging outcomes for sellers.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The system did not provide any indication of the likelihood of errors in pricing, leading sellers to assume the automated prices were correct.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The automated pricing system presented its decisions as absolute, leaving sellers with no recourse to challenge or reverse the pricing errors.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"There was no clear explanation of the criteria or reasons behind the drastic price changes, leaving sellers unable to understand or prevent the issue.",High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the automated pricing system influenced overall pricing decisions was not clearly communicated to sellers.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,"There was no clear mechanism for redress or compensation for affected sellers, with Amazon and Repricer Express initially avoiding responsibility.",Very High,The AI system targets members of a specific social group.,"The pricing glitch disproportionately affected small businesses and independent sellers, potentially driving them out of business while larger companies could absorb the losses.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There were no apparent mechanisms in place to flag and correct issues related to bias or poor performance in the pricing algorithm.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The pricing system did not appear to consider the diversity of sellers and their different capacities to absorb pricing errors.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The automated pricing system was applied universally without apparent mechanisms to limit its deployment to specific sellers or products.,Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Sellers had limited access to information about the cause of the glitch and potential remedies, with both Amazon and Repricer Express providing minimal communication.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Sellers had limited control over their pricing data and the automated system's actions, leading to severe financial consequences.",High,There are no measures to ensure the lawfulness of the processing of personal data.,There were no clear measures to ensure the lawfulness of the processing of pricing data by the automated system.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There were no apparent procedures to limit access to sellers' pricing data and to ensure it was only used for necessary purposes.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There was no clear mechanism for sellers to access, rectify, or erase their pricing data used by the automated system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There were no specific measures mentioned to enhance the security of the processing of pricing data in the automated system.,Medium,There is no procedure to conduct a data protection impact assessment.,There was no indication that a data protection impact assessment had been conducted for the automated pricing system.,High
fria-instance-claude-34.ttl,Amazon AWS Panorama automated workplace surveillance,"CCTV, Computer vision","Amazon Web Services, BBC, Fender","Amazon launched AWS Panorama, a service that enables automated analysis of workplace security camera footage. It can be used to evaluate manufacturing quality, identify bottlenecks, and monitor workplace safety and security. However, it raises concerns about workplace privacy and surveillance, especially regarding monitoring employee behavior and compliance with rules like wearing face masks or social distancing.","Computer vision technology applied to security camera footage, with machine learning functions occurring on-device",Assess product quality; Monitor workplace safety & security,2020-12-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-aws-panorama-workplace-surveillance,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,AWS Panorama does not explicitly communicate to employees that their actions are being monitored and analyzed by an AI system.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide information on the accuracy or potential error rates of its monitoring and analysis.,Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The system's surveillance and analysis could be used to make decisions about employee performance or compliance without clear mechanisms for employees to challenge these decisions.,High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"The system does not provide clear explanations for its decisions or flagged issues, making it difficult for employees to understand and potentially contest the outcomes.",High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which AWS Panorama influences overall decision-making processes in workplace management is not clearly communicated.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for redress if employees are negatively impacted by the system's decisions or analysis.,High,The AI system targets members of a specific social group.,"The system's monitoring could disproportionately affect certain groups of employees, potentially leading to discriminatory outcomes.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct potential biases in the system's analysis and decision-making processes.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system may not adequately consider the diversity of workplace environments and employee characteristics in its analysis.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The constant surveillance may inhibit employees' freedom of expression and natural behavior in the workplace.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Employees may not have easy access to the data collected about them or the ability to contest its accuracy.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,Employees have limited control over the collection and processing of their personal data through the surveillance system.,Very High,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness of processing personal data through continuous surveillance is not clearly established or communicated.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to the surveillance data to only necessary parties and purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for employees to access, rectify, or erase their personal data collected through the surveillance system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","While Amazon claims data processing happens on-device, there are no clear measures to enhance the security of personal data processing.",Medium,There is no procedure to conduct a data protection impact assessment.,There is no indication that a comprehensive data protection impact assessment has been conducted for the AWS Panorama system.,High
fria-instance-claude-35.ttl,Amazon botches delivery drone commercial launch,Drone,"Amazon, Federal Aviation Administration (FAA), Prime Air","The first commercial delivery by Amazon's Prime Air drone delivery service in December 2022 failed due to software issues and inability to deliver to a slightly moved QR-code. This incident, along with previous technical failures and regulatory challenges, raises questions about Amazon's technical capabilities and progress in the drone delivery industry.","MK27-2 drone with flight package software, ground-based QR-code delivery system",Deliver products,2022-12-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-botches-delivery-drone-commercial-launch,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The drone delivery system failed to communicate that its inability to deliver was due to algorithmic decisions or technical failures.,Medium,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system did not provide any indication of the likelihood of delivery failure or potential errors in its operation.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The drone's inability to adapt to a slightly moved QR-code suggests a lack of flexibility in its decision-making process, potentially leading to absolute outcomes without room for adjustment.",Medium,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There was no clear explanation provided for why the flight package software failed to boot up or why the drone couldn't adapt to the moved QR-code.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the drone's decision-making process influences the overall success of the delivery was not clearly communicated.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There were no clear measures in place for redress or compensation in case of delivery failures or damages caused by the drone system.,High,The AI system targets members of a specific social group.,"The drone delivery system's limitations, such as inability to fly over roads or people, might disproportionately affect certain communities or areas.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There were no apparent mechanisms to identify and correct biases or discrimination in the drone's delivery patterns or customer selection.,Medium,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The drone delivery system may not adequately consider the diversity of delivery locations and customer needs in its operational design.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The limitations on drone flights over roads and people may restrict the free movement and activities of individuals in delivery areas.,Low,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","There was limited transparency about the drone's operations and decision-making processes, potentially restricting public access to information about the technology's impact on their community.",Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,The drone delivery system may collect and process personal data about delivery locations and customer behavior without clear mechanisms for user control.,High,There are no measures to ensure the lawfulness of the processing of personal data.,"The lawfulness of data collection and processing by the drone delivery system, especially regarding privacy in residential areas, is not clearly established.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to data collected by the drone delivery system to only necessary parties and purposes.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for individuals to access, rectify, or erase their personal data collected through the drone delivery system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There are no specific measures mentioned to enhance the security of personal data processing in the drone delivery system.,Medium,There is no procedure to conduct a data protection impact assessment.,There is no indication that a comprehensive data protection impact assessment has been conducted for the drone delivery system.,High
fria-instance-claude-36.ttl,Amazon chemical food preservative suicides,Recommendation algorithm,"Amazon, US lawmakers, New York Times",Amazon is facing pressure from lawmakers to stop sales of a food preservative containing a chemical compound that is being used in suicides. The company's recommendation algorithm has been suggesting other items commonly purchased for suicide alongside the preservative. At least 10 people have killed themselves using the compound bought on Amazon in the past two years.,E-commerce platform with product recommendation algorithms,Recommend products,2022-01-29,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-chemical-food-preservative-suicides,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"Amazon's recommendation system does not communicate that its suggestions are based on algorithmic decisions, potentially leading to harmful outcomes.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide any indication of the potential risks or harmful outcomes associated with its product recommendations.,Very High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The recommendation system presents its suggestions as absolute truths, without acknowledging the potential for misuse or harmful outcomes.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"There is no clear explanation of the criteria or reasons behind the product recommendations, particularly those that could be used for harmful purposes.",High,There is no indication of the extent to which the AI system influences the overall decision-making process.,"The extent to which the recommendation algorithm influences purchasing decisions, especially for potentially harmful products, is not clearly communicated.",High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for redress or compensation for harm caused by the system's recommendations of potentially dangerous products.,Very High,The AI system targets members of a specific social group.,The recommendation system may disproportionately affect vulnerable individuals by suggesting potentially harmful products.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the recommendation system that could lead to harmful suggestions.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diverse needs and vulnerabilities of different user groups in its product recommendations.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system's recommendations may inadvertently promote harmful information or products without proper safeguards.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Users may not have easy access to information about the potential risks associated with recommended products.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,The recommendation system may use personal data to make potentially harmful suggestions without clear user consent or control.,High,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness of using personal data to recommend potentially harmful products is not clearly established.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to personal data used in the recommendation system to only necessary parties and purposes.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for users to access, rectify, or erase their personal data used in the recommendation system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There are no specific measures mentioned to enhance the security of personal data processing in the recommendation system, especially concerning potentially harmful product suggestions.",High,There is no procedure to conduct a data protection impact assessment.,"There is no indication that a comprehensive data protection impact assessment has been conducted for the recommendation system, particularly regarding its potential to suggest harmful products.",Very High
fria-instance-claude-37.ttl,Amazon Driveri delivery driver safety monitoring,"CCTV, Computer vision","Amazon, Netradyne, delivery drivers","Amazon is installing AI-enabled video cameras in delivery vans to monitor driver behavior and improve safety. The system has faced criticism for privacy concerns, unfair punishments, and creating additional stress for drivers. Some drivers report the system flags false violations and provides limited ability to appeal.",AI-powered cameras with computer vision capabilities to detect driver behaviors and road conditions,Improve safety,2021-02-03,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-delivery-driver-safety-cameras,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The AI system does not clearly communicate to drivers that its decisions are algorithmic, potentially leading to misunderstandings about flagged violations.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The system does not provide clear indications of the likelihood of errors in its violation detection, leading to a presumption of guilt for drivers.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The system's decisions are presented as absolute truths, with limited ability for drivers to contest or appeal flagged violations.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"There is no clear explanation provided to drivers about the criteria or reasons behind flagged violations, making it difficult for them to understand and improve their performance.",High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the AI system influences overall driver performance evaluations and compensation is not clearly communicated to drivers.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for redress or compensation for drivers who may be unfairly penalized by the system's decisions.,Very High,The AI system targets members of a specific social group.,"The system may disproportionately affect certain groups of drivers, such as those in high-traffic areas or with challenging delivery routes.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the system's violation detection and scoring algorithms.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diversity of driving conditions and environments that different drivers may encounter.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The constant surveillance may inhibit drivers' freedom of expression and natural behavior while performing their job.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Drivers may not have easy access to the data collected about them or the ability to contest its accuracy.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Drivers have limited control over the collection and processing of their personal data, including biometric information, through the camera system.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness of collecting and processing drivers' personal and biometric data through constant video surveillance is not clearly established or communicated.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to the surveillance data to only necessary parties and purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for drivers to access, rectify, or erase their personal data collected through the surveillance system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","While Amazon claims data processing happens on-device, there are no clear measures to enhance the security of personal data processing and storage.",High,There is no procedure to conduct a data protection impact assessment.,"There is no indication that a comprehensive data protection impact assessment has been conducted for the surveillance system, particularly regarding its potential privacy implications.",Very High
fria-instance-claude-38.ttl,"Amazon delivery drone malfunctions, sparks 25-acre fire",Drone,"Amazon, Federal Aviation Administration (FAA)","An Amazon Prime Air delivery drone on a test flight crashed into a field in eastern Oregon in June 2021, setting on fire 25 acres of wheat. The drone's motor shut off during flight, with two safety features failing. The incident is part of a series of crashes and safety concerns surrounding Amazon's drone program.",MK27 drone with flight control software and safety features,Deliver products,2021-06-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-delivery-drone-crashes-sparks-22-acre-fire,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The drone system did not clearly communicate the reasons for its malfunction or the failure of its safety features.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system did not provide any indication of the likelihood of failure or potential errors in its operation.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The drone's malfunction and crash presented as an absolute outcome, with no opportunity for intervention or correction.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There was no clear explanation provided for why the drone's motor shut off or why the safety features failed.,Very High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the drone's AI system influenced the overall failure was not clearly communicated.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There were no clear measures for redress or compensation for the environmental damage caused by the drone crash and subsequent fire.,Very High,The AI system targets members of a specific social group.,The drone crash disproportionately affected the local environment and potentially the local farming community.,Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There were no apparent mechanisms to identify and correct biases in the drone's operation that could lead to disproportionate risks in certain areas.,Medium,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The drone system did not appear to consider the diversity of environmental conditions and potential impacts in its operational design.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The incident raised concerns about the transparency of Amazon's drone program and its potential impacts on communities.,Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Information about the drone crash and its environmental impact was not readily accessible to the public or affected communities.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,The drone's operation and crash potentially infringed on the privacy and property rights of individuals in the affected area.,Medium,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness of conducting drone tests in populated areas without clear safety measures is not established.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to data collected during drone operations and crash investigations to only necessary parties.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for individuals affected by drone operations or crashes to access, rectify, or erase their personal data that may have been collected.",Medium,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There are no specific measures mentioned to enhance the security of data processing and storage related to drone operations and crash investigations.,Medium,There is no procedure to conduct a data protection impact assessment.,"There is no indication that a comprehensive data protection impact assessment has been conducted for the drone delivery program, particularly regarding its potential privacy and safety implications.",High
fria-instance-claude-39.ttl,Amazon Echo Dot Kids remembers kids' conversations,"Speech recognition, Natural language understanding (NLU)","Amazon, Campaign for a Commercial-Free Childhood (CCFC), Center for Digital Democracy (CDD), Georgetown University's Institute for Public Representation",Privacy groups filed a complaint with the FTC alleging that Amazon's Echo Dot Kids violates COPPA by collecting and retaining children's personal information without adequate parental consent. The device allegedly keeps voice recordings and transcripts even after parents attempt to delete them.,"Voice-activated smart speaker with AI assistant, designed for children","Provide information, services",2019-05-09,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-echo-dot-kids-remembers-kids-conversations,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The Echo Dot Kids device does not clearly communicate to parents or children that it is recording and retaining conversations.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide clear information about the potential for errors or misuse of children's data.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The device retains children's personal information even after parents attempt to delete it, presenting the retention as an unalterable fact.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no clear explanation provided to parents about why children's data is retained or how it is used.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the Echo Dot Kids influences children's behavior and data collection is not clearly communicated to parents.,Medium,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for redress or compensation if children's data is misused or improperly retained.,High,The AI system targets members of a specific social group.,The Echo Dot Kids may disproportionately affect children's privacy rights compared to adult users of similar devices.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the data collection or processing of children's information.,Medium,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diverse needs and vulnerabilities of different child users.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The constant recording and retention of children's conversations may inhibit their freedom of expression within their own homes.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Children and parents do not have easy access to the data collected about them, limiting their right to information.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,The Echo Dot Kids collects and retains children's personal data without adequate parental consent or control.,Very High,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness of collecting and retaining children's data without proper consent is questionable under COPPA regulations.,Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to children's data to only necessary parties and purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).",Parents are unable to fully delete their children's personal data from the Echo Dot Kids system.,Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There are no specific measures mentioned to enhance the security of children's personal data processing and storage.,High,There is no procedure to conduct a data protection impact assessment.,"There is no indication that a comprehensive data protection impact assessment has been conducted for the Echo Dot Kids, particularly regarding its potential privacy implications for children.",Very High
fria-instance-claude-40.ttl,Amazon employees use Ring to spy on customers,"CCTV, Computer vision","Amazon, Ring, Federal Trade Commission (FTC)","Employees at Amazon Ring and a Ukrainian contractor were able to access and download customer videos without restriction, viewing thousands of videos from female users in intimate spaces. This privacy violation resulted in a $5.8 million fine from the FTC.",Ring doorbell cameras and associated video storage systems,Strengthen security,2023-05-31,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-employees-use-ring-to-spy-on-customers,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Ring did not clearly communicate to customers that employees had unrestricted access to their private videos.,Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system did not provide any indication of the potential for misuse or unauthorized access to customer data.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","Ring's system allowed employees to access and view customer videos without any justification or customer consent, presenting this access as a normal part of operations.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There was no clear explanation provided to customers about who could access their videos and for what purposes.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which employee access to videos could impact customer privacy was not clearly communicated.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There were no clear measures for redress or compensation for customers whose privacy was violated by employee access to their videos.,Very High,The AI system targets members of a specific social group.,"The incident disproportionately affected female users, with employees specifically targeting videos from women's intimate spaces.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There were no apparent mechanisms to identify and correct biases in employee access or prevent targeting of specific user groups.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,"The system did not consider the diverse privacy needs of different user groups, particularly in sensitive areas like bedrooms and bathrooms.",High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"The unrestricted employee access to customer videos could inhibit freedom of expression within private spaces, as users may feel constantly monitored.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Customers did not have easy access to information about how their data was being accessed and used by Ring employees.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Ring allowed unrestricted employee access to customer videos without user consent or knowledge, severely violating privacy rights.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,"The lawfulness of Ring's data handling practices, including unrestricted employee access to customer videos, is questionable under privacy laws.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,"There were no clear procedures to limit access to customer videos to only necessary parties and purposes, allowing broad and unnecessary employee access.",Very High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","Customers had no mechanism to access, rectify, or erase their personal data that was being viewed by Ring employees.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",Ring failed to implement basic security measures to protect customer data from unauthorized employee access and potential external threats.,Very High,There is no procedure to conduct a data protection impact assessment.,There is no indication that Ring conducted a comprehensive data protection impact assessment before allowing such broad employee access to customer videos.,Very High
fria-instance-claude-41.ttl,Amazon Flex algorithm fires delivery drivers,"Automated management system, Image recognition","Amazon, Bloomberg","Amazon's Flex delivery service uses an algorithm to manage and fire independent contract drivers with little human intervention. Some dismissals are considered unfair, with drivers given only 10 days to appeal terminations. Drivers are unable to access the algorithm and its decision-making processes.",Algorithmic management system using performance metrics and image recognition for driver selfies,Increase efficiency,2021-06-26,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-flex-algorithm-delivery-driver-firings,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The algorithmic management system does not clearly communicate to drivers that their performance evaluations and terminations are the result of automated decisions.,Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide any indication of the potential for errors in its decision-making process or the likelihood of unfair terminations.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The algorithmic system makes termination decisions without providing drivers the opportunity to explain circumstances or challenge the decision effectively.,Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no clear explanation provided to drivers about the criteria or reasons behind their performance evaluations or terminations.,Very High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the algorithmic system influences overall employment decisions is not clearly communicated to drivers.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for redress or compensation for drivers who may be unfairly terminated by the algorithmic system.,Very High,The AI system targets members of a specific social group.,"The algorithmic system may disproportionately affect certain groups of drivers, such as those with physical changes or in challenging delivery areas.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the algorithmic decision-making process for driver evaluations and terminations.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diverse circumstances and challenges faced by different drivers in its performance evaluations.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The algorithmic management system may inhibit drivers' freedom to express concerns or challenge decisions due to fear of negative consequences.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Drivers do not have easy access to information about how their performance is evaluated or how termination decisions are made.,Very High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Drivers have limited control over their personal data used in the algorithmic management system, including selfies and performance metrics.",High,There are no measures to ensure the lawfulness of the processing of personal data.,"The lawfulness of collecting and processing drivers' personal data, including biometric information from selfies, for algorithmic management decisions is not clearly established.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to drivers' personal data and performance metrics to only necessary parties and purposes.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","Drivers have no clear mechanism to access, rectify, or erase their personal data used in the algorithmic management system.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There are no specific measures mentioned to enhance the security of personal data processing in the algorithmic management system.,Medium,There is no procedure to conduct a data protection impact assessment.,"There is no indication that a comprehensive data protection impact assessment has been conducted for the algorithmic management system, particularly regarding its impact on drivers' privacy and employment rights.",High
fria-instance-claude-42.ttl,Amazon Flex delivery drivers forced to take unsafe routes,Routing algorithm,"Amazon, Vice News","Amazon delivery drivers are being forced to take dangerous routes and run across four-lane highways at night with multiple packages and boxes. The company's routing algorithms sometimes group deliveries on both sides of a street into a single stop, forcing drivers to risk their personal safety to meet delivery demands.",Routing algorithm for package delivery optimization,Manage package delivery,2022-06-22,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-flex-delivery-driver-routing-safety,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The routing algorithm does not clearly communicate to drivers that its decisions may lead to unsafe situations.,Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide any indication of the potential risks or safety hazards associated with its routing decisions.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The routing algorithm presents its decisions as absolute, leaving drivers with little room to challenge or modify unsafe routes.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"There is no clear explanation provided to drivers about the criteria or reasons behind the routing decisions, especially those that may lead to unsafe situations.",High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the routing algorithm influences overall delivery decisions and driver safety is not clearly communicated.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for redress or compensation for drivers who may be injured or face consequences due to unsafe routing decisions.,Very High,The AI system targets members of a specific social group.,"The routing algorithm may disproportionately affect certain groups of drivers, such as those working in high-traffic areas or during night shifts.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the routing algorithm that could lead to unfair or discriminatory treatment of certain drivers or areas.,Medium,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The routing system does not appear to consider the diverse needs and safety concerns of different drivers and delivery areas.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The routing system may inhibit drivers' freedom to express concerns or refuse unsafe routes due to fear of negative consequences.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Drivers do not have easy access to information about how routing decisions are made or how to challenge unsafe routes.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Drivers have limited control over their personal data used in the routing algorithm, including their location and performance metrics.",Medium,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness of collecting and processing drivers' personal data for routing purposes is not clearly established.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no clear procedures to limit access to drivers' personal data and routing information to only necessary parties and purposes.,Medium,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","Drivers have no clear mechanism to access, rectify, or erase their personal data used in the routing algorithm.",Medium,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There are no specific measures mentioned to enhance the security of personal data processing in the routing algorithm.,Low,There is no procedure to conduct a data protection impact assessment.,"There is no indication that a comprehensive data protection impact assessment has been conducted for the routing algorithm, particularly regarding its impact on driver safety and privacy.",High
fria-instance-claude-43.ttl,Amazon Go fails to inform NYC customers about facial recognition,"Facial recognition, Computer vision, Deep learning","Amazon, Rodriguez Perez, Richard McCall, Surveillance Technology Oversight Project","Amazon is alleged to have failed to inform customers about its use of facial and body biometrics scanning at its cashierless Go retail stores in New York for over a year, violating New York City's 2021 Biometric Identifier Information Law. Amazon denies using facial recognition but acknowledges using computer vision and deep learning algorithms for customer tracking.","Computer vision, deep learning algorithms, and sensor fusion to track customers and their purchases",Verify identity,2023-03-16,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-go-fails-to-inform-nyc-customers-about-facial-recognition,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Amazon Go stores did not clearly communicate to customers that their biometric data was being collected and processed by algorithmic systems.,Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide any indication of the potential for errors or misuse of biometric data collected in the stores.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The biometric tracking system presents its decisions as absolute, with no apparent way for customers to challenge or verify the accuracy of their purchase tracking.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no clear explanation provided to customers about the criteria or reasons behind the biometric tracking and purchase decisions made by the system.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the biometric tracking system influences overall purchase tracking and customer identification is not clearly communicated to customers.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for redress or compensation for customers who may be wrongly charged or have their biometric data misused.,Very High,The AI system targets members of a specific social group.,"The biometric tracking system may disproportionately affect certain groups of customers, such as those with disabilities or from minority groups.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the biometric tracking and purchase identification systems.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diverse needs and characteristics of different customer groups in its biometric tracking and purchase identification processes.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The constant biometric tracking in Amazon Go stores may inhibit customers' freedom of movement and behavior due to the feeling of being constantly monitored.,Medium,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Customers do not have easy access to information about how their biometric data is collected, processed, and stored by the system.",Very High,There are no mechanisms for the user to exercise control over the processing of personal data.,Customers have limited control over their biometric data collected and processed by the Amazon Go system.,Very High,There are no measures to ensure the lawfulness of the processing of personal data.,The lawfulness of personal data processing in Amazon Go stores is questionable due to the lack of clear customer consent and information.,Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There is no clear indication of how access to personal data is limited or how the extent of data collection is determined for the system's purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for customers to access, rectify, or erase their personal data collected by the Amazon Go system.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There is no information provided about specific security measures, such as encryption or anonymization, used to protect customers' personal data in the Amazon Go system.",High,There is no procedure to conduct a data protection impact assessment.,There is no evidence of a data protection impact assessment being conducted for the Amazon Go biometric tracking system.,High
fria-instance-claude-44.ttl,Amazon India own brand search engine rigging,Search engine algorithm,"Reuters, Amazon, Indian sellers","Reuters obtained internal Amazon documents showing the company in India systematically copied other companies' products and manipulated search results to promote its own brand products, violating its public statements and raising unfair competition concerns.","Search engine algorithms, data analytics, 'search seeding', 'search sparkles'",Rank content/search results,2021-10-13,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-india-search-rigging,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"Amazon's search algorithm does not communicate that search results are manipulated to favor its own products, depriving sellers of a fair chance to compete.",Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The search algorithm does not provide any indication of potential bias or error in its rankings, leading users to assume the results are unbiased and accurate.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The search algorithm's results are presented as absolute truth, making it difficult for sellers to challenge or counter unfavorable rankings.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no explanation provided to users or sellers about the criteria used by the search algorithm to determine product rankings.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the search algorithm influences product visibility and sales is not disclosed to sellers or customers.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for sellers to seek redress if they believe they have been unfairly impacted by the search algorithm's rankings.,Very High,The AI system targets members of a specific social group.,"The search algorithm appears to favor Amazon's own products, potentially discriminating against third-party sellers and their products.",Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms in place to identify and correct biases in the search algorithm's rankings.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,"The search algorithm does not appear to consider the diversity of sellers or products, potentially leading to a lack of representation for certain groups or types of products.",Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"The search algorithm's manipulation of results may limit the visibility of certain products or sellers, potentially infringing on their freedom of expression in the marketplace.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data and criteria used by the search algorithm are not easily accessible or comprehensible to sellers or customers, limiting their access to important information.",Very High,There are no mechanisms for the user to exercise control over the processing of personal data.,There appear to be no mechanisms for users or sellers to control how their data is used by the search algorithm.,High,There are no measures to ensure the lawfulness of the processing of personal data.,There are no apparent measures to ensure the lawful processing of personal data used by the search algorithm.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There is no clear indication of procedures to limit access to personal data or to ensure data minimization in the search algorithm's operations.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There appears to be no mechanism for users or sellers to access, rectify, or erase their personal data used by the search algorithm.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There is no information provided about specific security measures to protect personal data used by the search algorithm.,High,There is no procedure to conduct a data protection impact assessment.,There is no evidence of a data protection impact assessment being conducted for the search algorithm and its use of personal data.,High
fria-instance-claude-45.ttl,Amazon Mentor delivery driver scoring,Performance scoring algorithm,"Amazon, CNBC, eDriving, Amazon delivery drivers","Amazon uses the Mentor app to monitor and score delivery drivers' performance, generating a daily 'FICO' score. Drivers report inaccuracies, unfair disciplinary actions, and privacy concerns. The app tracks location even after work hours and may lead to loss of bonuses and perks.","Smartphone app using GPS, accelerometer, and other sensors to track driving behavior",Assess delivery driver performance,2021-02-12,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-mentor-dsp-delivery-driver-scoring,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The Mentor app does not clearly communicate to drivers that their actions are being monitored and scored by an algorithmic system, potentially affecting their job performance evaluations.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The Mentor app does not provide clear indications of potential errors or inaccuracies in its scoring system, leading drivers to assume the scores are always correct.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The Mentor app's scoring system presents its results as absolute truth, making it difficult for drivers to challenge or appeal unfair assessments.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"The Mentor app does not provide clear explanations of the criteria used to determine scores, making it difficult for drivers to understand how to improve their performance.",High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the Mentor app's scores influence overall job performance evaluations and potential disciplinary actions is not clearly communicated to drivers.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for drivers to seek redress or appeal unfair scores or disciplinary actions resulting from the Mentor app's assessments.,Very High,The AI system targets members of a specific social group.,"The Mentor app may disproportionately affect certain groups of drivers, such as those in areas with more challenging driving conditions or those with older devices.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases or inaccuracies in the Mentor app's scoring system.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The Mentor app does not appear to consider the diverse driving conditions and challenges faced by drivers in different regions or routes.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"The constant monitoring by the Mentor app may inhibit drivers' freedom of movement and expression, as they feel constantly under surveillance.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data collected and stored by the Mentor app is not easily accessible to drivers, limiting their ability to review and understand their own performance data.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Drivers have limited control over their personal data collected by the Mentor app, especially concerning location tracking outside of work hours.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,"There are no clear measures to ensure the lawful processing of personal data collected by the Mentor app, especially concerning data collected outside of work hours.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no apparent procedures to limit access to personal data collected by the Mentor app or to ensure data minimization.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for drivers to access, rectify, or erase their personal data collected by the Mentor app.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There is no information provided about specific security measures to protect the personal data collected by the Mentor app, including location data collected outside of work hours.",High,There is no procedure to conduct a data protection impact assessment.,There is no evidence of a data protection impact assessment being conducted for the Mentor app and its extensive data collection practices.,High
fria-instance-claude-46.ttl,Amazon One palmprint biometric opacity,Palm print scanning,"Amazon, TechCrunch, Albert Fox Cahn","Amazon offers customers $10 for their palmprints when signing up for Amazon One, a contactless ID and transaction service. Despite promises of data security, some palm data is used anonymously to improve the system, raising concerns about potential commercial and other uses.","Palm scanning hardware captures minute characteristics of palm, including surface-area details and subcutaneous features, to create a palm signature stored in the cloud.",Verify identity; Authorise transactions,2021-08-02,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-one-palmprint-biometrics,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Amazon One does not clearly communicate to users that their palm print data may be used for purposes beyond identity verification and transaction authorization.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide clear information about the accuracy or potential errors in palm print recognition.,Medium,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The palm print system presents its results as absolute, potentially making it difficult for users to challenge any errors or misidentifications.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no clear explanation of how the palm print data is used to improve the system or for other potential commercial uses.,Very High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which palm print data influences other Amazon services or decision-making processes is not clearly communicated.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for users to seek redress or compensation if their palm print data is misused or compromised.,Very High,The AI system targets members of a specific social group.,"The palm print system may disproportionately affect certain groups, such as those with hand disabilities or injuries.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the palm print recognition system.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diversity of palm prints across different demographics or potential challenges for certain populations.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The use of palm print biometrics may inhibit individuals' freedom of movement and anonymity in physical spaces.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The palm print data collected and stored by Amazon is not easily accessible to users for review or verification.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users have limited control over their palm print data once it is collected by Amazon, with data potentially being used for system improvements and other undisclosed purposes.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,"There are no clear measures to ensure the lawful processing of palm print data, especially concerning its use for system improvements and potential commercial applications.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no apparent procedures to limit access to palm print data or to ensure data minimization in its use for system improvements.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","While Amazon allows users to delete their palm print data, the process and implications of doing so are not clearly explained.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","While Amazon claims to use encryption and secure zones, specific measures to protect palm print data are not fully disclosed.",High,There is no procedure to conduct a data protection impact assessment.,There is no evidence of a comprehensive data protection impact assessment being conducted for the Amazon One palm print system.,Very High
fria-instance-claude-47.ttl,Amazon Ring Always Home Cam,"Drone, Computer vision","Amazon, Ring, Digital rights advocates","Amazon launched Ring Always Home Cam, an autonomous drone with a security camera that can fly around homes on pre-programmed routes, streaming video. Despite claims of privacy-first design, critics worry about erosion of privacy, security risks, and potential for misuse.","Autonomous drone with camera, obstacle avoidance technology, pre-programmed flight paths, video streaming",Strengthen home security,2021-09-28,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-always-home-cam,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The Always Home Cam does not clearly communicate to users that it is an autonomous drone making algorithmic decisions about when and where to fly within the home.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide clear information about potential errors in its autonomous navigation or video capture capabilities.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The Always Home Cam's autonomous nature and ability to capture video without direct user control may present its findings as absolute truth, making it difficult for individuals to challenge or dispute captured footage.",Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"There is no clear explanation of the criteria used by the Always Home Cam to determine when and where to fly, or how it processes and interprets the video it captures.",Very High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the Always Home Cam's autonomous decisions influence overall home security and privacy is not clearly communicated to users.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for users to seek redress or compensation if the Always Home Cam captures inappropriate footage or causes damage within the home.,Very High,The AI system targets members of a specific social group.,"The Always Home Cam may disproportionately affect certain groups, such as those with disabilities or sensitivities to drone noise and movement.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the Always Home Cam's autonomous decision-making or video capture processes.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,"The system does not appear to consider the diversity of home layouts, cultural practices, or privacy expectations across different user groups.",Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The presence of an autonomous flying camera in the home may inhibit individuals' freedom of expression and behavior within their own living spaces.,Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The data collected and stored by the Always Home Cam may not be easily accessible to users for review or verification.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users have limited control over the personal data and video footage captured by the Always Home Cam, especially given its autonomous nature.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,"There are no clear measures to ensure the lawful processing of personal data and video footage collected by the Always Home Cam, especially concerning its potential use for system improvements or other undisclosed purposes.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no apparent procedures to limit access to personal data and video footage captured by the Always Home Cam or to ensure data minimization.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","While Amazon claims the camera is physically blocked when docked, there is no clear mechanism for users to access, rectify, or erase their personal data and video footage collected by the Always Home Cam.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","While Amazon claims to use encryption and secure zones, specific measures to protect the personal data and video footage collected by the Always Home Cam are not fully disclosed.",Very High,There is no procedure to conduct a data protection impact assessment.,There is no evidence of a comprehensive data protection impact assessment being conducted for the Always Home Cam and its potential privacy implications.,Very High
fria-instance-claude-48.ttl,Police request Amazon Ring BLM protest footage,CCTV,"Los Angeles Police Department (LAPD), Amazon Ring, Electronic Frontier Foundation (EFF)","The LAPD requested footage from Amazon Ring users during the 2020 Black Lives Matter protests. This raised concerns about privacy, surveillance of protestors, and the relationship between law enforcement and private surveillance technology companies.","Smart home security cameras, video footage sharing platform","Strengthen security, safety",2021-02-16,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-blm-protest-surveillance,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The LAPD's request for Ring footage of protests without specifying a particular crime raises concerns about blanket surveillance of First Amendment-protected activities.,Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide clear information about how the requested footage will be used or the potential consequences for individuals captured in the videos.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The request for protest footage without specific criminal allegations may lead to presumptions of guilt for individuals captured in the videos.,Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no clear explanation of the criteria used by law enforcement to request or analyze the Ring footage in relation to protests.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which Ring footage influences law enforcement decisions regarding protests is not clearly communicated.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for individuals captured in Ring footage to seek redress or challenge the use of this surveillance in relation to protests.,Very High,The AI system targets members of a specific social group.,The request for Ring footage of Black Lives Matter protests raises concerns about disproportionate surveillance of specific racial and political groups.,Very High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","There are no apparent mechanisms to identify and correct biases in the use of Ring footage for law enforcement purposes, particularly in the context of protests.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diverse perspectives and experiences of different communities in relation to surveillance and law enforcement.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The request for Ring footage of protests may have a chilling effect on freedom of expression and the right to peaceful assembly.,Very High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data collected and stored by Ring cameras is not easily accessible to individuals captured in the footage, limiting their ability to access information about themselves.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,Individuals captured in Ring footage of protests have limited control over their personal data and how it is used by law enforcement.,Very High,There are no measures to ensure the lawfulness of the processing of personal data.,"There are no clear measures to ensure the lawful processing of personal data collected through Ring cameras, especially when used for law enforcement purposes during protests.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no apparent procedures to limit access to personal data collected by Ring cameras or to ensure data minimization when used for law enforcement purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for individuals captured in Ring footage during protests to access, rectify, or erase their personal data.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",There is no information provided about specific security measures to protect the personal data collected by Ring cameras when shared with law enforcement.,High,There is no procedure to conduct a data protection impact assessment.,"There is no evidence of a comprehensive data protection impact assessment being conducted for the use of Ring footage in law enforcement activities, particularly in the context of protests.",Very High
fria-instance-claude-49.ttl,Amazon shares Ring data with police,"CCTV, Computer vision","Amazon, Ring, US Police, Senator Ed Markey","Amazon Ring shared private recordings with US police eleven times in 2022 without user consent, citing emergency circumstances. This raises concerns about police reliance on private surveillance and the extent of user knowledge about data usage.","Video doorbells, surveillance cameras, data sharing platform",Strengthen security,2022-07-13,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-police-data-sharing,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Amazon Ring's practice of sharing data with police without user consent or warrant bypasses due process and may infringe on users' rights to privacy and fair treatment.,Very High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system does not provide clear information about the frequency or circumstances under which user data may be shared with law enforcement without consent.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",Ring's 'good-faith determination' of emergency circumstances lacks transparency and may lead to unjustified sharing of user data with law enforcement.,Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no clear explanation of the criteria used by Ring to determine 'emergency circumstances' that justify sharing user data without consent.,Very High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which Ring's data sharing practices influence law enforcement activities is not clearly communicated to users.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no clear measures for users to seek redress or challenge the sharing of their data with law enforcement without their consent.,Very High,The AI system targets members of a specific social group.,"The practice of sharing Ring data with police without user consent may disproportionately affect certain communities, particularly those already subject to over-policing.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There are no apparent mechanisms to identify and correct biases in the use of Ring data by law enforcement.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system does not appear to consider the diverse privacy needs and expectations of different communities in its data sharing practices.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The knowledge that Ring may share data with police without consent could have a chilling effect on people's behavior and freedom of expression in their own homes and neighborhoods.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data collected and shared by Ring is not easily accessible to users for review or verification, limiting their right to information about their own data.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Ring users have limited control over their personal data once it is collected, especially in cases where it is shared with law enforcement without their consent.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,"There are no clear measures to ensure the lawful processing of personal data collected by Ring, especially when shared with law enforcement without user consent.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no apparent procedures to limit access to personal data collected by Ring or to ensure data minimization when shared with law enforcement.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There is no clear mechanism for Ring users to access, rectify, or erase their personal data, especially in cases where it has been shared with law enforcement without their consent.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","While Ring claims to use encryption for data transmission, there is limited information about specific security measures to protect personal data when shared with law enforcement.",High,There is no procedure to conduct a data protection impact assessment.,"There is no evidence of a comprehensive data protection impact assessment being conducted for Ring's data sharing practices with law enforcement, particularly in emergency circumstances.",Very High
fria-instance-claude-50.ttl,Amazon Ring video doorbell 'invades' neighbour privacy,"CCTV, Computer vision","Jon Woodard, Dr Mary Fairhurst, Judge Melissa Clarke","A UK judge ruled that Amazon Ring doorbell cameras installed by Jon Woodard 'unjustifiably invaded' the privacy of his neighbour Dr Mary Fairhurst. The devices captured images and audio of Dr Fairhurst's property, violating data protection laws.","Video doorbell, CCTV cameras, audio recording",Strengthen security,2021-10-13,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-video-doorbell-neighbour-privacy-invasion,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The Ring doorbell and cameras collected data without clear consent or transparency, potentially violating the presumption of innocence for those captured in recordings.",High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system did not provide clear information about its data collection capabilities or potential errors in capturing and processing information.,High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The continuous recording of video and audio data without clear boundaries or consent may infringe on the presumption of innocence for those captured.,Very High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There was no clear explanation provided to neighbors about the extent of data collection or the criteria used for capturing and storing information.,Very High,There is no indication of the extent to which the AI system influences the overall decision-making process.,The extent to which the Ring doorbell and cameras influenced the overall security and privacy landscape of the neighborhood was not clearly communicated.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There were no clear measures for affected individuals to seek redress or challenge the collection and use of their personal data by the Ring devices.,Very High,The AI system targets members of a specific social group.,"The indiscriminate collection of data by Ring devices may disproportionately affect certain groups, such as those who frequently visit or live near the property.",Medium,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",There were no apparent mechanisms to identify and correct biases in the data collection or processing of the Ring devices.,High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system did not appear to consider the diverse privacy needs and expectations of different individuals in the neighborhood.,Medium,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The constant surveillance by Ring devices may have a chilling effect on freedom of expression and movement in the neighborhood.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The data collected and stored by Ring devices was not easily accessible to individuals captured in the recordings.,High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Individuals captured by Ring devices had no control over their personal data, including images and audio recordings.",Very High,There are no measures to ensure the lawfulness of the processing of personal data.,"There were no clear measures to ensure the lawful processing of personal data collected by the Ring devices, particularly concerning the excessive audio recording range.",Very High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There were no apparent procedures to limit the collection of personal data to only what was necessary for the stated purpose of home security.,High,"There is no mechanism allowing to comply with the exercise of data subject's rights (access, rectification and erasure of data relating to a specific individual).","There was no clear mechanism for individuals captured by the Ring devices to access, rectify, or erase their personal data.",Very High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","While Ring claimed to have privacy features, there was insufficient information about specific security measures to protect personal data collected by the devices.",High,There is no procedure to conduct a data protection impact assessment.,There was no evidence of a comprehensive data protection impact assessment being conducted for the use of Ring devices in residential areas.,Very High
