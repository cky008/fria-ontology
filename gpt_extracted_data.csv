file_name,report_name,organisation_description,contributor_details,assessment_content,technology_description,purposes_description,report_date,aiaaic_link,challenge_11,evaluation_11,impact_level_11,challenge_12,evaluation_12,impact_level_12,challenge_13,evaluation_13,impact_level_13,challenge_14,evaluation_14,impact_level_14,challenge_15,evaluation_15,impact_level_15,challenge_16,evaluation_16,impact_level_16,challenge_21,evaluation_21,impact_level_21,challenge_22,evaluation_22,impact_level_22,challenge_23,evaluation_23,impact_level_23,challenge_31,evaluation_31,impact_level_31,challenge_32,evaluation_32,impact_level_32,challenge_41,evaluation_41,impact_level_41,challenge_42,evaluation_42,impact_level_42,challenge_43,evaluation_43,impact_level_43,challenge_44,evaluation_44,impact_level_44,challenge_45,evaluation_45,impact_level_45,challenge_46,evaluation_46,impact_level_46
fria-instance-gpt-1.ttl,"3D masks fool payment, airport facial recognition systems",,AI development company Kneron,"Researchers in China have found that facial recognition technology can be fooled by using a 3D-printed mask depicting a different person's face, raising questions about the security of the technology.","The AI system uses facial recognition technology at public transport portals, point of sales terminals, airport security checkpoints, and on mobile devices.",Test facial recognition; Security; Accuracy/reliability,2021-05-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/3d-masks-fool-payment-airport-facial-recognition-systems,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The system's lack of transparency regarding algorithmic decisions means users are unaware that their identities are being verified by AI, which can lead to security vulnerabilities.",High,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"Facial recognition systems may exhibit biases, leading to differential treatment of individuals based on race or ethnicity.",Medium,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The use of facial recognition at various public checkpoints without consent leads to arbitrary interference with privacy.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Users do not have access to their biometric data, limiting their ability to control their personal information.",Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users are unable to control how their facial data is processed, raising significant privacy and data protection concerns.",High,There are no measures to ensure the lawfulness of the processing of personal data.,The collection and processing of biometric data were conducted without sufficient legal justification or compliance with privacy laws.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The lack of procedures to limit access to biometric data increases the risk of unauthorized access and misuse.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Customers were not provided with means to exercise their data subject rights, such as accessing or correcting their personal data, undermining their control over their own information.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","The lack of security measures, such as encryption and anonymization, to protect biometric data increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-2.ttl,4 Little Trees (4LT) student emotion recognition,,Find Solution AI,"4 Little Trees, an AI programme that analyses students' emotions as they learn in order to help teachers make distance learning more engaging and personalised, has come under fire from researchers and digital rights activists fearful that emotion recognition technologies tend to be intrusive, can be misused, and may be biased against people with darker skins.",The AI system uses facial recognition and gesture analysis to monitor and identify student emotions by analysing facial muscular micro-movements in real-time.,Identify & monitor emotions; Accuracy/reliability; Privacy; Surveillance; Bias/discrimination - race; ethnicity,2021-05-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/4-little-trees-4lt,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The system's lack of transparency regarding algorithmic decisions means students and parents may not be aware that their emotions are being monitored and analyzed by an AI.,High,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"The emotion recognition technology may have biases against people with darker skins, leading to unfair treatment and discrimination.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","The system lacks mechanisms to detect and correct biases, potentially perpetuating discrimination against certain demographic groups.",Medium,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The widespread use of the AI system without proper consent and notification constitutes arbitrary interference with student privacy.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Students and parents do not have access to the data collected by the system, limiting their ability to control their personal information.",Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,"Students and parents are not provided with options to control or limit the use of their biometric data, compromising their ability to protect their personal information.",High,There are no measures to ensure the lawfulness of the processing of personal data.,The collection and processing of student facial data were conducted without sufficient legal justification or compliance with privacy laws.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,"The absence of procedures to limit access to biometric data means that personal information was potentially accessible to unauthorized individuals, increasing the risk of data breaches.",High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Customers were not provided with means to exercise their data subject rights, such as accessing or correcting their personal data, undermining their control over their own information.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","The lack of security measures, such as encryption and anonymization, to protect biometric data increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-3.ttl,7-Eleven customer survey facial recognition,,7-Eleven,"Chain store 7-Eleven breached customer privacy in 700 stores between June 2020 and August 2021 by collecting facial imagery without consent, according to Australia's privacy commissioner the OAIC. The commissioner ruled that 7-Eleven abused customer privacy by collecting and storing their facial images in order to validate a survey it had been conducting into customer needs and to understand their demographic profile.",The AI system used facial recognition technology to validate survey responses by capturing customers' facial images at two points during the survey-taking process.,Validate survey responses; Privacy; Necessity/proportionality,2021-08-31,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/7-eleven-customer-survey-facial-recognition,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,7-Eleven did not provide adequate notice to customers that their facial images were being used to validate survey responses. This lack of transparency means that customers were unaware of the AI system's role in the process.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The lack of transparency in how facial images are processed and used makes it impossible for customers to understand the potential error rates and biases in the system.,Medium,,,,,,,,,,,,,The AI system targets members of a specific social group.,"The system may have inherent biases that disproportionately affect certain demographic groups, particularly minorities, leading to unfair treatment.",High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The absence of mechanisms to address bias and discrimination means that the system could perpetuate existing inequalities without accountability.,Medium,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The widespread deployment of facial recognition for surveys without consent or sufficient notification constitutes arbitrary interference with customer privacy.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Customers were not informed about how their data would be used or stored, nor were they given access to their data, limiting transparency and control over personal information.",Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,"Customers were not provided with options to control or limit the use of their biometric data, compromising their ability to protect their personal information.",High,There are no measures to ensure the lawfulness of the processing of personal data.,"The collection and processing of facial images were conducted without sufficient legal justification or compliance with privacy laws, leading to unlawful data processing practices.",Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,"The absence of procedures to limit access to biometric data means that personal information was potentially accessible to unauthorized individuals, increasing the risk of data breaches.",High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Customers were not provided with means to exercise their data subject rights, such as accessing or correcting their personal data, undermining their control over their own information.",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","The lack of security measures, such as encryption and anonymization, to protect biometric data increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-4.ttl,Aadhaar COVID-19 facial recognition marginalisation,Plans by the Indian government to use facial recognition integrated within the Aadhaar biometric ID system have raised fears that millions of vulnerable people without mobile phones or internet access will lose out on receiving their COVID-19 vaccinations.,"Reuters, Internet Freedom Foundation","A facial recognition system based on Aadhaar ID is being tested as a 'touchless' vaccination process. Concerns include misidentification, exclusion of vulnerable populations, and privacy violations.",Facial recognition technology integrated with Aadhaar biometric ID system.,Verify identity; Accuracy/reliability; Bias/discrimination; Economic; Security; Privacy,2021-04-15,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aadhaar-covid-19-facial-recognition,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The lack of transparency regarding the system's algorithmic decisions leaves vulnerable people unaware of why they may be denied vaccination, contributing to marginalisation and exclusion.",High,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"Vulnerable groups, including the homeless and transgender individuals, are disproportionately affected by the reliance on Aadhaar-linked facial recognition, leading to potential denial of vaccines.",High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The widespread deployment of facial recognition without consent or clear limits raises significant privacy concerns.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Individuals do not have access to their facial recognition data, making it difficult to contest errors or misidentifications.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users lack control over their biometric data, increasing the risk of unauthorized use and exclusion.",High,There are no measures to ensure the lawfulness of the processing of personal data.,The processing of biometric data without adequate legal safeguards results in privacy violations and unauthorized access.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,Unlimited access to personal data increases the risk of unauthorized use and further exclusion from services.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Individuals are not provided with means to correct or access their data, leading to ongoing denial of essential services.",High,"There are no security measures to ensure the confidentiality, integrity and availability of personal data (e.g. encryption, anonymization, etc.).","The lack of security measures, such as encryption and anonymization, to protect biometric data increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-5.ttl,Aadhaar glitches result in villagers' starvation,"India's Aadhaar biometric ID system faced technical problems leading to villagers being unable to access food rations or subsidized grain, resulting in starvation and deaths.","Campaigners, The Guardian","Technical problems with India's Aadhaar biometric ID system have resulted in the deaths of scores of villagers in Jharkhand state and elsewhere, with some committing suicide and others suffering severe malnutrition.",Fingerprint biometrics used to reduce welfare fraud.,Reduce welfare fraud; Accuracy/reliability; Robustness,2021-06-30,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aadhaar-glitch-results-in-villagers-starvation,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The lack of transparency regarding the system's algorithmic decisions leaves villagers unaware of why they are denied rations, contributing to severe malnutrition and death.",High,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"Vulnerable groups such as the Parhaiya are disproportionately affected by the glitches, leading to denial of their legal entitlements and exacerbating their poverty.",High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system's mandatory linkage and biometric authentication without sufficient infrastructure result in arbitrary denial of food rations.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Villagers are unable to access or correct their personal data, which exacerbates their inability to obtain essential services.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users lack control over their biometric data, leading to misidentification and denial of services.",High,There are no measures to ensure the lawfulness of the processing of personal data.,The processing of biometric data without adequate legal safeguards results in exclusion and denial of entitlements.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,Unlimited access to personal data increases the risk of unauthorized use and further exclusion from services.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Villagers are not provided with means to correct or access their data, leading to ongoing denial of essential services.",High,"There are no specific measures in place to enhance the security of the processing of personal data (e.g. encryption, anonymization, etc.).","The lack of security measures, such as encryption and anonymization, to protect biometric data increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-6.ttl,AccessiBe automated web accessibility,"The use of AccessiBe's automated web accessibility solution by Eyebobs led to a lawsuit due to failure to provide users of its website with equal access, especially for screen readers.","Accessibility advocates, software developers, U.S. Magistrate Judge Richard Lanzillo, NBC News","AccessiBe's solution claims to make websites easier for people with disabilities, but has been criticized for failing to deliver on those promises and potentially causing more harm.",Web accessibility overlay using automated AI technology.,Improve website accessibility; Accuracy/reliability,2021-11-11,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/accessibe-automated-accessibility,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The lack of transparency regarding the system's algorithmic decisions leads to confusion and dissatisfaction among users, especially those relying on assistive technologies.",High,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"The system disproportionately affects users with disabilities, particularly those who rely on screen readers, by failing to provide the promised accessibility improvements.",High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"The deployment of the system without user consent raises privacy concerns, particularly regarding the detection of assistive technology use and persistent settings across sites.",High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Users do not have access to their accessibility settings data, making it difficult to manage or correct issues that arise from the overlay's automated adjustments.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users lack control over how their accessibility data is processed, increasing the risk of unauthorized use and privacy violations.",High,There are no measures to ensure the lawfulness of the processing of personal data.,"The automated accessibility solution processes data without adequate legal safeguards, leading to potential GDPR and CCPA noncompliance.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,"The overlay solution lacks procedures to restrict access to personal data, resulting in excessive and potentially unauthorized data access.",High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Individuals are not provided with means to access, rectify, or erase their data, leading to ongoing issues with data management and user dissatisfaction.",High,"There are no security measures to ensure the confidentiality, integrity, and availability of personal data (e.g., encryption, anonymization, etc.).","The lack of security measures, such as encryption and anonymization, to protect biometric data increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-7.ttl,Adobe Creative Cloud content analysis,Adobe's automated content analysis of Creative Cloud files to train AI algorithms raised privacy and employment concerns among users.,"Adobe, Adobe Creative Cloud users, Bloomberg","The use of content analysis in Adobe Creative Cloud has sparked concerns about privacy, data security, and potential job displacement among creatives.","Machine learning, pattern recognition, object recognition.","Improve products, services; Privacy; Confidentiality; Employment",2023-01-13,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-creative-cloud-content-analysis,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Adobe's lack of transparency about the algorithmic decisions made during content analysis has caused significant user distrust.,High,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"The content analysis disproportionately affects creative professionals, raising concerns about job security and the potential replacement of human roles with AI.",High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,Adobe's automatic opt-in for content analysis without explicit user consent raises significant privacy concerns.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Users do not have easy access to the data Adobe collects and processes, limiting their ability to manage their own content and privacy settings.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users lack control over how their data is processed, raising concerns about the security and unauthorized use of their creative content.",High,There are no measures to ensure the lawfulness of the processing of personal data.,"Adobe's content analysis practices may not comply with legal data protection standards, such as GDPR, raising potential legal risks.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The lack of adequate procedures to restrict access to user data increases the risk of unauthorized access and misuse.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Adobe does not provide clear mechanisms for users to exercise their rights to access, rectify, or erase their data.",High,"There are no security measures to ensure the confidentiality, integrity, and availability of personal data (e.g., encryption, anonymization, etc.).","The lack of security measures, such as encryption and anonymization, to protect user content increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-8.ttl,Adobe Firefly AI art generator training,"Adobe's use of Adobe Stock content to train the Firefly AI art generator without explicit consent from contributors raises significant ethical, copyright, and employment concerns.","Adobe, Adobe Stock contributors, VentureBeat","Adobe's use of stock images to train the Firefly AI model has led to concerns about intellectual property rights, transparency, and the impact on artists' livelihoods.","Machine learning, pattern recognition, object recognition.","Generate video, images; Copyright; Employment; Ethics",2023-09-13,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-firefly-ai-art-generator,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Adobe's lack of transparency about the algorithmic decisions made during the Firefly AI training has caused significant user distrust.,High,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"The Firefly AI's use of stock images affects creative professionals, raising concerns about job security and potential displacement by AI-generated content.",High,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,Adobe's automatic use of stock images without explicit user consent raises significant privacy concerns.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Users do not have easy access to the data Adobe collects and processes, limiting their ability to manage their own content and privacy settings.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,"Users lack control over how their data is processed, raising concerns about the security and unauthorized use of their creative content.",High,There are no measures to ensure the lawfulness of the processing of personal data.,"Adobe's content analysis practices may not comply with legal data protection standards, such as GDPR, raising potential legal risks.",High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The lack of adequate procedures to restrict access to user data increases the risk of unauthorized access and misuse.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","Adobe does not provide clear mechanisms for users to exercise their rights to access, rectify, or erase their data.",High,"There are no security measures to ensure the confidentiality, integrity, and availability of personal data (e.g., encryption, anonymization, etc.).","The lack of security measures, such as encryption and anonymization, to protect user content increases the risk of unauthorized access and misuse.",High,There is no procedure to conduct a data protection impact assessment.,The failure to conduct a data protection impact assessment (DPIA) means that potential risks to data privacy and security were not adequately identified or mitigated.,High
fria-instance-gpt-9.ttl,Adobe Sensei Project Morpheus,,Adobe,"Adobe has unveiled Project Morpheus, a prototype video version of the Neural Filters machine learning-based image-editing tools that the company released as part of Photoshop 22.0. Like Neural Filters, Morpheus enables users to alter someone's age and facial expressions, whilst adding the ability to change facial hair and glasses. The software raises concerns about the ease with which lifelike deepfake videos could be created to malign, undermine, or otherwise damage the interests or reputation of an individual, group, or society - a possibility Adobe confirmed to The Verge that it is aware of.","Project Morpheus uses deepfake technology, generative adversarial networks (GANs), neural networks, deep learning, and machine learning to manipulate video. It can change facial attributes such as age, expressions, facial hair, and glasses.",Manipulate video; Dual/multi-use,2021-10-28,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-sensei-project-morpheus,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,This is the evaluation content for FRIA-reportEvaluation11.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",This is the evaluation content for FRIA-reportEvaluation12.,Medium,,,,,,,,,,,,,The AI system targets members of a specific social group.,This is the evaluation content for FRIA-reportEvaluation21.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",This is the evaluation content for FRIA-reportEvaluation22.,Medium,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,This is the evaluation content for FRIA-reportEvaluation31.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",This is the evaluation content for FRIA-reportEvaluation32.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,This is the evaluation content for FRIA-reportEvaluation41.,High,There are no measures to ensure the lawfulness of the processing of personal data.,This is the evaluation content for FRIA-reportEvaluation42.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,This is the evaluation content for FRIA-reportEvaluation43.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",This is the evaluation content for FRIA-reportEvaluation44.,High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",This is the evaluation content for FRIA-reportEvaluation45.,High,There is no procedure to conduct a data protection impact assessment.,This is the evaluation content for FRIA-reportEvaluation46.,High
fria-instance-gpt-10.ttl,"Aespa virtual K-pop anthropomorphism, sexualisation",,SM Entertainment,"Korean entertainment company SM Entertainment has announced that Aespa, its latest K-pop girl group, will include human and virtual members that can 'interact and communicate as independent beings as they have AI brains. The announcement was met with a mixture of intrigue and excitement, tempered by concerns about the 'ownership' of young girls by fans and the potential for dehumanisation and sexualisation. Others raised the possibility of Aespa avatars being manipulated for pornography and other unethical uses.","Aespa uses deepfake technology, neural networks, and AI to create virtual avatars that interact with human members. These avatars can communicate and perform independently in a virtual world.",Create virtual avatars; Anthropomorphism; Dual/multi-use; Safety,2020-11-17,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aespa-virtual-k-pop,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,This is the evaluation content for FRIA-reportEvaluation11.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",This is the evaluation content for FRIA-reportEvaluation12.,Medium,,,,,,,,,,,,,The AI system targets members of a specific social group.,This is the evaluation content for FRIA-reportEvaluation21.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",This is the evaluation content for FRIA-reportEvaluation22.,Medium,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,This is the evaluation content for FRIA-reportEvaluation31.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",This is the evaluation content for FRIA-reportEvaluation32.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,This is the evaluation content for FRIA-reportEvaluation41.,High,There are no measures to ensure the lawfulness of the processing of personal data.,This is the evaluation content for FRIA-reportEvaluation42.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,This is the evaluation content for FRIA-reportEvaluation43.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",This is the evaluation content for FRIA-reportEvaluation44.,High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",This is the evaluation content for FRIA-reportEvaluation45.,High,There is no procedure to conduct a data protection impact assessment.,This is the evaluation content for FRIA-reportEvaluation46.,High
fria-instance-gpt-11.ttl,Agricultural Bank of China facial recognition age bias,,Agricultural Bank of China,"A video has emerged of people waiting in line for an ATM in Guangshui, Hubei province in China, having to lift a 94 year-old woman in order to have her identification verified using the bank's facial recognition system in order to activate her social security card. The video caused an outcry on social media, with people calling the bank 'inhuman' and urging it and other organisations to consider the needs of elderly people when designing products and services. The bank apologised and said it would improve its customer service.",The bank uses facial recognition technology to verify the identity of customers for activating social security cards.,Verify identity; Bias/discrimination - age,2021-10-13,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/agricultural-bank-of-china-facial-recognition-age-bias,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,This is the evaluation content for FRIA-reportEvaluation11.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",This is the evaluation content for FRIA-reportEvaluation12.,Medium,,,,,,,,,,,,,The AI system targets members of a specific social group.,This is the evaluation content for FRIA-reportEvaluation21.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",This is the evaluation content for FRIA-reportEvaluation22.,Medium,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,This is the evaluation content for FRIA-reportEvaluation31.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",This is the evaluation content for FRIA-reportEvaluation32.,Medium,There are no mechanisms for the user to exercise control over the processing of personal data.,This is the evaluation content for FRIA-reportEvaluation41.,High,There are no measures to ensure the lawfulness of the processing of personal data.,This is the evaluation content for FRIA-reportEvaluation42.,Medium,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,This is the evaluation content for FRIA-reportEvaluation43.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",This is the evaluation content for FRIA-reportEvaluation44.,High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",This is the evaluation content for FRIA-reportEvaluation45.,High,There is no procedure to conduct a data protection impact assessment.,This is the evaluation content for FRIA-reportEvaluation46.,High
fria-instance-gpt-12.ttl,AI confuses bus ad for jaywalker,,Ningbo Police,"An AI system in the Chinese city of Ningbo has mistakenly accused a woman pictured on the side of a bus of jaywalking. The system had reacted to an advert on the side of a bus that showed the face of Dong Mingzhu, CEO of Gree Electric Appliances, China's biggest air-conditioner maker. Ms Dong's face had subsequently been shown on a large display on a roadside in an effort to shame her. Meantime, the real jaywalker walked free. Ningbo police said they had deleted the photo of Ms Dong and would update the AI system so that it could differentiate between ads and people.",The AI system uses facial recognition technology to identify jaywalkers by comparing captured images with a database of faces.,Improve street safety; Accuracy/reliability,2021-11-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-confuses-bus-ad-for-jaywalker,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to public embarrassment and false accusation of an individual.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in wrongful conclusions.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential miscarriages of justice.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects elderly individuals.,High impact due to age discrimination.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct age bias.,High impact due to unchecked discriminatory outcomes.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Elderly individuals could not easily access or verify their data, causing inconvenience and distress.",Medium impact due to difficulties faced by elderly users in accessing data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-13.ttl,AI converts Asian-American student into Caucasian,,Playground AI,"Image generator Playground AI changed the face of Asian-American Massachusetts Institute of Technology (MIT) student Rona Wang into a Caucasian, fueling accusations that the system is racist. Wang, 24, said she used text-to-image generator Playground AI to convert an image of her in an MIT sweatshirt into 'a professional LinkedIn profile photo.' The system returned an image of her with a fairer complexion, dark blonde hair, and blue eyes. Wang told Boston.com, 'Wow, does this thing think I should become white to become more professional?' Playground AI founder Suhail Doshi responded to the incident by saying 'models aren't instructable like that' and will pick 'any generic thing based on the prompt.' He later confessed to being 'quite displeased with this and hope to solve it.'","The AI system uses text-to-image generation, Generative Adversarial Network (GAN), neural network, deep learning, and machine learning technologies to generate images based on text prompts.","Generate image; Bias/discrimination - race, ethnicity",2023-01-10,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-converts-asian-american-student-into-caucasian,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to public embarrassment and false accusation of an individual.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in wrongful conclusions.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential miscarriages of justice.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects people of certain races and ethnicities.,High impact due to racial and ethnic discrimination.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct racial and ethnic bias.,High impact due to unchecked discriminatory outcomes.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Users could not easily access or verify their data, causing inconvenience and distress.",Medium impact due to difficulties faced by users in accessing data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-14.ttl,AI Dungeon GPT-3 offensive speech filter,Content moderation system | NLP/text analysis,"AI Dungeon, Latitude, OpenAI, Suchin Gururangan, Nick Walton","AI Dungeon developer Latitude faced criticism for its content moderation system to prevent stories depicting sexual encounters with minors. The upgrade to OpenAI's GPT-3 resulted in some players generating inappropriate stories. The new solution blocked a wider range of content than intended, raising concerns over privacy as private content was reviewed by moderators.","NLP/text analysis for content moderation, using OpenAI's GPT-3 model to filter offensive content.",Minimise sexual content,2022-08-30,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-dungeon-offensive-speech-filter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fria-instance-gpt-15.ttl,AI-generated article calls fake tanning 'racist',,Irish Times,"An article published by the Irish Times that labelled Irish people's use of fake tan as 'cultural appropriation' has discovered to have been a hoax generated wholly or in part by artificial intelligence (AI). Titled 'Irish women's obsession with fake tan is problematic', the article was published in the name of Adriana Acosta-Cortez, described as a 29-year-old Ecuadorian healthcare administrator living in north Dublin and whose photograph apparently accompanied the article. However, people quickly started raising questions about the article and the author's photograph, persuading the Irish Times to retract it. Irish Times editor Ruadhn MacCormaic apologised for what he described as a 'breach of trust' and promised to make the publication's pre-publication processes more transparent.",The AI system uses natural language processing and text generation technologies to create articles.,Generate text; Mis/disinformation; Ethics,2023-05-12,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-article-calls-fake-tanning-racist,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to public embarrassment and false information spread by the AI-generated article.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in wrongful conclusions.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential miscarriages of justice.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their cultural practices.,High impact due to cultural insensitivity and discrimination.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct cultural bias.,High impact due to unchecked discriminatory outcomes.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-16.ttl,"AI impersonation scams Canadian couple of USD 21,000",,"Canadian Authorities, Washington Post","An elderly Canadian couple were defrauded of CAD 21,000 after they were contacted by an alleged lawyer who said their son had killed a US diplomat in a car accident and required money for legal support. According to the Washington Post, the 'lawyer' had allegedly put Benjamin Perkins, the couple's son, on the line to underline the gravity and urgency of the situation. Perkins' synthetic voice was sufficiently close to his real voice that his parents believed the call and sent the money to the scammer using Bitcoin. The couple only realized they had been scammed after Perkin called later that evening. Perkin told the Post that he didn't know how the scammer discovered his voice, though he had posted videos about snowmobiling on YouTube.",The AI system uses deepfake audio technology to impersonate voices.,Defraud; Security,LLM understand: Unable to determine the exact date from the provided text,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-impersonation-scams-couple-of-usd-21000,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to financial loss and emotional distress caused by the scam.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in wrongful conclusions.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential miscarriages of justice.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their vulnerabilities.,High impact due to exploitation of elderly individuals.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct bias in targeting vulnerable populations.,High impact due to unchecked exploitation.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-17.ttl,"AI invents 40,000 biochemical warfare agents",,"Researchers from USA, UK, Switzerland","Researchers tweaked an AI system usually used to predict the toxicity of pipeline drugs to invent 40,000 potentially lethal molecules in just six hours in order to demonstrate to participants at a conference in Switzerland how these kinds of systems can be misused and abused. The experiment shows how easy it is to turn a 'good' or 'helpful' medical technology into one with potentially negative or terrifying consequences. Commentators also noted the naivety of the researchers - and many of their colleagues - for failing to consider the broader implications of their work.",The AI system uses machine learning models to predict molecule toxicity.,Predict molecule toxicity; Safety; Security; Dual/multi-use,2021-11-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-invents-40000-biochemical-warfare-agents,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential misuse and creation of lethal molecules.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in wrongful conclusions and misuse.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential miscarriages of justice.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their vulnerabilities.,High impact due to exploitation of vulnerable populations.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct bias in targeting vulnerable populations.,High impact due to unchecked exploitation.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-18.ttl,AI meal planner app suggests chlorine gas recipe,,"Pak 'n Save, New Zealand political commentator Liam Hehir","A chatbot that generates meal suggestions and directions generated a recipe that recommended people concoct chlorine gas, which it called 'aromatic water mix'. The incident called into question the safety of the GPT-3.5 powered bot, despite its operator saying it had built in safeguards to stop these kinds of outputs. Kiwi political commentator Liam Hehir had asked Pak 'n Save's Savey meal-bot what he could make if only he had water, bleach and ammonia. A spokesperson for New Zealand-based supermarket chain Pak 'n Save told The Guardian that the company was disappointed to see 'a small minority have tried to use the tool inappropriately and not for its intended purpose.' Pak 'n Save promised to 'keep fine-tuning' its bot.",The AI system uses GPT-3.5 to generate recipes based on user-inputted ingredients.,Generate recipes; Accuracy/reliability; Safety,2023-07-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-meal-planner-app-suggests-chlorine-gas-recipe,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential health hazards from following dangerous recipes.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in following dangerous or harmful recipes.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential misuse and harm.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their vulnerabilities.,High impact due to exploitation of vulnerable populations.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct bias in targeting vulnerable populations.,High impact due to unchecked exploitation.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-19.ttl,AI Portrait Ars racial bias,Generative adversarial network (GAN)| Neural network| Machine learning,"Mashable journalist Morgan Sung discovered that the generator, which turned selfies into realistic Impressionist and Baroque portraits, 'whitened my skin to an unearthly pale tone, turned my flat nose into one with a prominent bridge and pointed end, and replaced my very hooded eyes with heavily lidded ones","AI-powered portrait generator AI Portrait Ars drew controversy soon after its launch when it was found to be whitening colored peoples' skins. The GAN is composed of two neural networks, one learns to recognize portraits of the actors (Discriminator), and the other learns to generate them (Generator).","The system uses Generative Adversarial Networks (GANs) to create portraits. The model was based on a collection of 15,000 portraits, predominantly from the 15th century Western European Renaissance period, which would help explain the skin whitening.","The research aimed to share the experience of being portrayed by an AI algorithm, to discover how AI sees you, and to reflect on the importance of AI fairness.",2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-portrait-ars-racial-bias,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",High,,,,,,,,,,,,,The AI system targets members of a specific social group.,The AI system targets members of a specific social group.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data stored, recorded, and produced are not easily accessible to concerned individuals.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,There are no mechanisms for the user to exercise control over the processing of personal data.,High,There are no measures to ensure the lawfulness of the processing of personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification, and erasure of data relating to a specific individual).","There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification, and erasure of data relating to a specific individual).",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymization, and aggregation).","There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymization, and aggregation).",High,There is no procedure to conduct a data protection impact assessment.,There is no procedure to conduct a data protection impact assessment.,High
fria-instance-gpt-20.ttl,AI satellite location spoofing,,University of Washington,"A University of Washington research study has shown the ease with which deepfake satellite images can be created and used to create misinformation and disinformation. The aim, author Bo Zhao told The Verge, 'is to demystify the function of absolute reliability of satellite images and to raise public awareness of the potential influence of deep fake geography.' As part of their study, Zhao and his colleagues created software to generate deepfake satellite images, using generative adversarial networks (GANs), and then created detection software that was able to spot the fakes based on characteristics like texture, contrast, and color. Per PetaPixel, the authors simulated their own deepfakes using Tacoma, Washington as a base map and placed onto it features extracted from Seattle, Washington, and Beijing, China. The high rises from Beijing cast shadows in the fake satellite image while the low-rise buildings and greenery were superimposed from the urban landscape found in Seattle. The authors warn false satellite images could be used to create hoaxes about natural disasters, support disinformation, or mislead geo-political foes.",The AI system uses generative adversarial networks (GANs) to create and detect deepfake satellite images.,Scare/confuse/destabilize; Mis/disinformation; Dual/multi-use,2023-06-15,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-satellite-location-spoofing,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential for creating misleading satellite images.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in following inaccurate or misleading satellite images.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential misuse and harm from misleading images.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their vulnerabilities.,High impact due to exploitation of vulnerable populations.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct bias in targeting vulnerable populations.,High impact due to unchecked exploitation.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-21.ttl,AI Stefanie Sun (AI孙燕姿),,Bilibili,"AI-cloned songs in the name and voice of Singapore-based Mandopop singer Stefanie Sun have gone viral on China's most popular video platform Bilibili, raising questions about copyright and jobs in the music industry. The videos were generated by so-vits-svc fork, an open source software that enables anyone to train their own AI model to speak in any voice and language. Fans report it difficult to distinguish between songs sung by Sun and her virtual version. Commentators focused on Sun's loss of copyright, mentioning that she had not sought to have her songs taken down. By contrast, Sun, who had not released an album since 2017, responded primarily by lamenting AI's impact on jobs.",The AI system uses generative adversarial networks (GANs) and deep learning techniques to generate songs that mimic the voice and style of Stefanie Sun.,Generate music; Copyright; Employment; jobs,2023-05-22,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-stefanie-sun,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential misuse and misunderstanding of AI-generated content.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in following inaccurate or misleading AI-generated songs.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential misuse and harm from misleading AI-generated songs.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their vulnerabilities.,High impact due to exploitation of vulnerable populations.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct bias in targeting vulnerable populations.,High impact due to unchecked exploitation.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-22.ttl,AI text detector language bias,,Stanford University,"Seven AI writing detection tools 'frequently' misclassify non-native English writing as generated by AI systems, according to a study by Stanford University researchers. The findings raise questions about the accuracy and reliability of AI writing detection tools in general, as well as about their potential to discriminate against non-native English speaking students, academics, and job applicants. The researchers ran English essays written by non-native English speakers through seven popular GPT detectors to see how well the AI detection systems performed. Over half were classified as AI-generated. By contrast, over 90% of essays written by native English-speaking eighth graders in the US were classified as human-generated by the same systems. Over half of essays written by people were wrongly flagged as AI-made, with implications for students and job applicants.","The AI system uses NLP/text analysis, neural networks, deep learning, and machine learning techniques to detect AI-generated writing.",Detect AI writing; Bias/discrimination - language,2023-07-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-text-detector-language-bias,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential misuse and misunderstanding of AI-generated content.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in following inaccurate or misleading AI-generated classifications.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential misuse and harm from misleading AI-generated classifications.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their vulnerabilities.,High impact due to exploitation of vulnerable populations.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct bias in targeting vulnerable populations.,High impact due to unchecked exploitation.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The system fails to consider the needs of diverse populations.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-23.ttl,Airbnb Smart Pricing algorithm racial bias,,Carnegie Mellon University,"The Financial Times reports that a Carnegie Mellon University study has found that Airbnb's 'Smart Pricing' algorithm is exacerbating racial inequality. Launched in 2015, the algorithm dynamically adjusts the cost of a nights stay based on demand and allows hosts to set a minimum price. However, Carnegie Mellon professor Param Vir Singh and his team of researchers discovered that White hosts appeared to prefer using the algorithm more than Black ones, thereby unintentionally widening existing real-world racial discrepancies. Research studies have also discovered that Airbnb secretly collects and feeds users personal data into an algorithm that assesses whether they are trustworthy enough to make a booking. Machine learning algorithms can leverage vast amounts of consumer data, allowing automation of business decisions such as pricing, product offerings, and promotions. Airbnb, an online marketplace for vacation rentals and other lodging, created an algorithm-based smart-pricing tool that is free to all Airbnb hosts and allows hosts to set their properties' daily price automatically. A new study investigated the impact of Airbnb's algorithm on racial disparities among Airbnb hosts. Adopting the tool narrowed the revenue gap between White and Black hosts considerably, but because far fewer Black hosts used the algorithm, the revenue gap between White and Black hosts actually increased after the tool's introduction.",The AI system uses a pricing algorithm to determine the price based on demand and other factors.,Determine price; Bias/discrimination; race; Opacity: Governance,2023-07-02,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/airbnb-smart-pricing-algorithm-racism,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential misuse and misunderstanding of AI-generated content.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in following inaccurate or misleading AI-generated classifications.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential misuse and harm from misleading AI-generated classifications.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact as it leaves users vulnerable to adverse outcomes.,The AI system targets members of a specific social group.,The system disproportionately affects individuals based on their vulnerabilities.,High impact due to exploitation of vulnerable populations.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The system lacks mechanisms to detect and correct bias in targeting vulnerable populations.,High impact due to unchecked exploitation.,"There is no reference to the inclusion of potentially marginalized groups in datasets, and a lack of guidance to avoid bias in algorithm development and testing.",The system fails to account for marginalized groups in its dataset.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-24.ttl,Ajin USA worker crushed to death by robot,,,"Regina Allen Elsea was killed when a robot she had been trying to fix at auto parts manufacturer Ajin USA restarted and crushed her. Elsea had entered a robotic station ('cell') containing several robots to clear a sensor fault on a piece of machinery that had stopped working during an assembly line stoppage. When inside the cell, one of the robots energized and she was struck by a robotic arm which pinned her against a piece of machinery. In November 2020, Ajin USA was ordered to pay USD 1.5 million after admitting violating federal safety standards before Elsea was crushed to death. It also had to complete three years of probation, during which it must comply with a safety compliance plan overseen by a third-party auditor. Ajin pleaded guilty to knowingly failing to enforce federal safety standards, including the mandatory use of so-called lockout/tagout procedures to prevent the type of incident that killed Elsea. Two weeks before Elsea's death, the US Labor Department fined Ajin USA and two staffing agencies USD 2.5 million for 27 safety violations. JOON LLC, d/b/a AJIN USA (Ajin), an auto-parts manufacturing company, was sentenced in federal court today in Montgomery, Alabama, after pleading guilty to a charge related to the death of a machinery operator Regina Elsea, who was 20 years old, worked at Ajin's Cusseta, Alabama, facility. On June 18, 2016, she entered an enclosure called a cell containing several robots and other pieces of machinery. While she was inside the cell, troubleshooting a sensor fault, one of the machines started up and Elsea was struck by a robotic arm. She died of her injuries. The Occupational Safety and Health Act (OSH Act) requires employers to develop and utilize procedures to de-energize machinery during maintenance and servicing activities to prevent the kind of unplanned startup that killed Elsea. These procedures are often referred to as lockout/tagout. Ajin knew these procedures were required and had developed them, but Ajin also knew that over a period of at least two years supervisors did not effectively enforce them. In the 15 minutes prior to Elsea's fatal injury in the presence of their supervisors workers entered cells to troubleshoot machinery without following lockout/tagout no less than five times, and the supervisors did not take any action to stop or reprimand them. In two other instances, the supervisors themselves entered a cell without following lockout/tagout. At the time of Elsea's fatal injury, several individuals were inside the cell, none of whom had followed lockout/tagout procedures to de-energize the machinery within the cell. Ajin pleaded guilty to a willful violation of the OSH Act standard requiring the use of lockout/tagout procedures. U.S. Magistrate Judge Stephen Michael Doyle sentenced Ajin to pay a $500,000 fine the statutory maximum $1,000,000 in restitution to Elsea's estate, and a three-year term of probation, during which Ajin must comply with a safety compliance plan, overseen by a third-party auditor. Among other things, the safety compliance plan requires a full review of Ajin's lockout/tagout procedures, weekly inspections to ensure compliance, and creation of a mechanism for employees to report any safety concerns about the facility anonymously.",The technology involved was a robotic arm used in an automotive parts manufacturing assembly line.,Safety,2023-07-02,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ajin-usa-worker-crushed-to-death-by-robot,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential misuse and misunderstanding of AI-generated content.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in following inaccurate or misleading AI-generated classifications.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential misuse and harm from misleading AI-generated classifications.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact due to lack of recourse for affected individuals.,The dataset that was used to train the AI system did not include potentially marginalized groups (e.g. persons with disabilities).,The training dataset excluded marginalized groups.,Medium impact due to lack of inclusivity.,The development and testing phases of the AI system were not subject to guidance and checks to avoid bias.,Bias checks were not conducted during development and testing.,Medium impact due to potential biases in AI decisions.,"There is no reference to the inclusion of potentially marginalized groups in datasets, and a lack of guidance to avoid bias in algorithm development and testing.",The system fails to account for marginalized groups in its dataset.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-25.ttl,Alexei Navalny Smart Voting Bot Blocking Report,"This report was conducted to evaluate the blocking of Alexei Navalny's Smart Voting bot by Apple, Google, and Telegram.",Research Team at Example.org,The AI system assessed in this report is the Smart Voting chatbot devised by Alexei Navalny.,"The technology and data aspects involve the use of chatbots and AI for facilitating tactical voting, leveraging data on election candidates.",The purpose of the Smart Voting bot was to provide Russian voters with information on candidates most likely to defeat the pro-Kremlin United Russia party candidates.,2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alexei-navalny-smart-voting-bot,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.","The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,There is no indication of the extent to which the AI system influences the overall decision-making process.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,High,The AI system targets members of a specific social group.,The AI system targets members of a specific social group.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data stored, recorded, and produced are not easily accessible to concerned individuals.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,There are no mechanisms for the user to exercise control over the processing of personal data.,High,There are no measures to ensure the lawfulness of the processing of personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",High,There is no procedure to conduct a data protection impact assessment.,There is no procedure to conduct a data protection impact assessment.,High
fria-instance-gpt-26.ttl,"Alfi personalised, real-time advertising",,,"Alfi is a Miami-based digital marketing company that uses AI and computer vision to understand facial cues and perceptual details and matches relevant advertising or content based on the viewers profile. The firm says its software is designed to show ads to people based on their age, gender and ethnicity without specifically identifying any individual. In July 2021, Alfi drew criticism for saying it would run digital tablets equipped with facial recognition in 10,000 Uber and Lyft cabs in the US. The system analyses passengers' reactions to advertising and other content, sending information back to advertisers. Uber and Lyft told Bloomberg that Alfi contracts with their drivers directly, and that drivers must comply with local laws and regulations. The move prompted a written request (pdf) for information from US senators Amy Klobuchar and Richard Blumenthal, who claimed the programme raises 'serious concerns about privacy for your passengers. According to Bloomberg, Alfi also plans to sell user data, including retina tracking, keyword recognition, voice intonation, and demographics. Alfis exclusive network of AI-powered screens at the back of Uber & Lyft rideshares brings unprecedented targeting precision to digital out-of-home. We use computer vision technology to match audiences with relevant advertising in real-time and in a privacy-compliant process. Alfi brings next level transparency and accountability to the digital out-of-home advertising marketplace. ALFI is an intelligent software powered by AI and computer vision. With privacy engrained in the companys core values, we create relevant and interactive digital out-of-home (DOOH) advertising experiences, delivering the content to the end-user ethically and respectfully and offering DOOH advertisers a never-before-seen precision in DOOH targeting. ALFIs software turns a digital display into an intelligent screen that can detect audiences and serve the right messages to make their media more valuable and effective. ALFIs proprietary platform recognizes what not who is viewing a digital out-of-home ad and serves content based on the viewers gender and age in an ethical and respectful manner. ALFI believes this type of anonymized, yet targeted advertising is the future of the industry, especially as companies move away from cookies and deep collection of a consumers personal data and browsing history. ALFI solves many of the challenges advertisers face today and provides brands the ability to serve relevant ads to consumers in physical environments, driving sales and relevance without capturing any personal information. ALFI understands small facial cues and perceptual details to detect a face and predict the age and gender of the person looking at the screen. Hence, ALFI uses facial detection not facial recognition. ALFIs smart technology detects behavior without collecting or storing personal data or facial images. It sets new standards by providing precise targeting information to advertisers by collecting information in non-intrusive ways that are compliant with GDPR (General Data Protection Regulation), CCPA (California Consumer Privacy Act), and HIPAA (Health Insurance Portability and Accountability Act). Since both GDPR and CCPA focus on the collection and storage of personal data, the fact that ALFI does not collect any personal identifiers or information makes it compliant to these privacy laws. The ALFI platform is built using GDPR standards in the UK. For example, the Hammersmith Broadway Mall leverages ALFIs software. With ALFI, they are able to control the advertisements consumers are served in real time without capturing personal information. Similarly, in the Belfast Airport, we have set up kiosks that detect metrics like age and gender to deliver relevant content to the demographic viewing the screen, all built to GDPR standards. And, in keeping with those standards, our tablets in taxis do not store images of riders. Our mission is to enable the real-time advertising economy with the next generation of disruptive technology. Alfi is excited to be partnering with Uber and Lyft drivers to help provide them with an additional revenue opportunity. Juan Carlos, Miami-area Uber driver, states, \""This is a great opportunity for all of us rideshare drivers. Alfi not only gives us extra revenue, but it is a great tool for our customers. We can share places to eat, places to shop, and things to do around Miami. I absolutely love my Alfi. I have been driving around with Alfi installed for the past year and I am so excited to see the whole system come to life with advertisers and content providers. This is like magic and Alfi really works,\"" commented Juan Carlos. This is a huge advancement for the advertising industry - brand owners have a completely new way to market their business. With ALFI's technology, the DOOH landscape now has accountability, transparency, and genuine reporting. It's a game changer; we are already seeing it with the demand from major advertisers and agencies,\"" said Ron Spears, CRO, ALFI. Alfi's enterprise grade, multimedia computer vision and machine learning platform, generates powerful advertising recommendations and insights. Multiple technologies work together in Alfi with viewer privacy and reporting objectives as its two goals. Alfi solves the problem of providing real time, accurate and rich reporting on customer demographics, usage, interactivity, and engagement, while never storing any personal, identifiable information. Alfi was designed to be fully compliant with all privacy regulations including the General Data Protection Regulation, in Europe, California Consumer Privacy Act, and the Health Insurance Portability and Accountability Act. Ron Spears noted, \""The demand for Alfi has been overwhelming from both the UBER and Lyft drivers and the advertisers. This is going to be a massive ramp up and will happen in lightning speed. Alfi, Inc. provides solutions that bring transparency and accountability to the digital out of home advertising marketplace. Since 2018, Alfi, Inc. has been developing its artificial intelligence advertising platform to deliver targeted advertising in an ethical and privacy-conscious manner. This press release contains forward-looking statements. These statements are made under the \""safe harbor\"" provisions of the U.S. Private Securities Litigation Reform Act of 1995. Statements that are not historical facts, including statements about the Company's beliefs and expectations, are forward-looking statements. Forward-looking statements involve inherent risks and uncertainties, and a number of factors could cause actual results to differ materially from those contained in any forward-looking statement. In some cases, forward-looking statements can be identified by words or phrases such as \""may\"", \""will,\"" \""expect,\"" \""anticipate,\"" \""target,\"" \""aim,\"" \""estimate,\"" \""intend,\"" \""plan,\"" \""believe,\"" \""potential,\"" \""continue,\"" \""is/are likely to\"" or other similar expressions. Further information regarding these and other risks, uncertainties or factors is included in the Company's filings with the SEC. All information provided in this press release is as of the date of this press release, and the Company does not undertake any duty to update such information, except as required under applicable law. We invested into Alfi before we even thought that we had an opportunity of working with them. When I saw the technology, I was blown away that this was already here and running. My whole family invested in the IPO. This technology platform is disruptive and revolutionary. We want to be a part of it,\"" comments Miguel Radillo, VP of All-Niter. Florentino Diaz, Project Manager of Alfi states, \""This is only the beginning. We are going to light up every major city in the USA. There is no other choice when it comes to accurate brand targeting and real data reporting. Nobody does this while remaining privacy compliant.\"" Diaz comments further, noting, \""We have a massive operation unfolding in Miami today with hundreds of Ubers and Lyfts installing Alfi tablets. It's an epic event and marks the start of our nationwide rollout. What's most rewarding, is we can help rideshare drivers increase their income and help create employment opportunities with community outreach programs for students. The DOOH world is transitioning rapidly to impression-based accountability with major advertisers and national brands demanding more transparency and better accuracy in reporting metrics. Alfi is far ahead of this curve, having already built, tested, and now deploying these advanced platforms in the OOH world globally. It is incredibly exciting to see these paramount shifts towards ALFI's intelligent platform. Utilizing Alfi's technology, rideshare drivers can earn extra income while they drive their customers around. Alfi optimizes its network of digital tablets to deliver powerful reach, dynamic visuals, and tailored message capability by age, gender, geography, demographics, brand behavior and interests, all in real-time. Alfi's computer vision can change and run ads remotely to deliver the right content, to the right person, at the right time in a responsible and privacy compliant manner. Alfi provides data rich reporting functionality that informs the advertisers that someone viewed their ad, the number of views, and each viewer's reaction to the ad. Increasingly, advertisers are demanding improved performance and capabilities from the ad technology they utilize. Alfi delivers for advertisers with analytics, accountability, transparency, proof of engagement and actual impressions. Alfi, Inc. provides solutions that bring transparency and accountability to the digital out of home advertising marketplace. Since 2018, Alfi, Inc. has been developing its artificial intelligence advertising platform to deliver targeted advertising in an ethical and privacy-conscious manner. A family owned and operated business established in 2005 providing full services from design development, prototyping, component part 3D printing and laser cutting to full assembly, staging, testing, packaging and shipment. All-Niter, known to some as Niter Laser, has always kept up with technological advances. Starting its laser cutting department about a decade ago and testing the 3d printing market over 5 years ago, All-Niter has always provided the most cutting-edge services available. With a 3d printing farm of close to 50 machines, All-Niter is now capable of not just prototyping but high-volume manufacturing. Starting off as an architecture supply store providing materials to university level architecture students, All-Niter currently makes it a point to support its clients and contribute to the development of local businesses. As an evolving company they have created a community outreach program that provides opportunities for high school graduates to learn a business, gain experience and grow in an open and positive environment. All-Niter's virtuous and innovative philosophy gives them the edge on developing targeted logistical systems and customized production. This press release contains forward-looking statements. These statements are made under the \""safe harbor\"" provisions of the U.S. Private Securities Litigation Reform Act of 1995. Statements that are not historical facts, including statements about the Company's beliefs and expectations, are forward-looking statements. Forward-looking statements involve inherent risks and uncertainties, and a number of factors could cause actual results to differ materially from those contained in any forward-looking statement. In some cases, forward-looking statements can be identified by words or phrases such as \""may\"", \""will,\"" \""expect,\"" \""anticipate,\"" \""target,\"" \""aim,\"" \""estimate,\"" \""intend,\"" \""plan,\"" \""believe,\"" \""potential,\"" \""continue,\"" \""is/are likely to\"" or other similar expressions. Further information regarding these and other risks, uncertainties or factors is included in the Company's filings with the SEC. All information provided in this press release is as of the date of this press release, and the Company does not undertake any duty to update such information, except as required under applicable law. According to a press release announcing the rollout of the program, Alfi uses computer vision and its ads are tailored by age, gender, geography, demographics, brand behavior and interests'' and the tablet informs the advertisers that someone viewed their ad, the number of views, and each viewer's reaction to the ad. According to the release, hundreds'' of Ubers and Lyfts in Miami have installed the tablets. When passengers use ride-sharing services, they want to be safe and secure. They also have a reasonable expectation of privacy. We were therefore deeply disturbed to read that your drivers are installing digital tablets in the back seats of their cars to deliver targeted advertisements, the letter reads. These tablets are equipped with cameras that film passengers when they take an Uber or Lyft ride. The tablets then use artificial intelligence to assess their age, gender, race, location, and assumed interests based on their appearance, all of which they can use to show targeted advertisements to the Uber and Lyft passengers, the senators continue. According to news reports, the tablets then even provide information to advertisers about the passengers reactions to the ads, which seems even more intrusive. The senators end the letter with a list of questions for the companies to answer by July 12th, including: whether the firms have business relationships with Alfi, whether they have reviewed the program at all, if the adtech platform connects to the ride-hail apps, whether the program falls under Uber or Lyft's privacy policies, numbers on how many drivers have installed the tablets, and more information on the ads, content, data, and revenue generated by them. Lyft leaves third-party advertisements to be regulated by local laws, and drivers should keep in mind the impact they could have on riders, Lyft said in a statement to Motherboard. Uber does not have any relationship with Alfi. If drivers are participating in this program, they are doing so independently with Alfi, and would be subject to local regulations, Uber said in a statement. Biometric privacy is not something most U.S. consumers worry or even know about yet, but news of facial recognition systems being installed in Uber and Lyft cars prompted a rapid response from two U.S. senators. Miami is a test in the U.S. market for Uber, Lyft and Alfi. Some ride-hailing vehicles in London already carry the devices. Klobuchar, of Minnesota, and Blumenthal, of Connecticut, on June 29 requested information about, among other things, biometric data collection, storage and deletion. Two of the Senators questions focus on driver compensation. Alfi claims drivers agreeing to mount the tablets can earn as much as an extra $350 per month, and the senators want to know how those bonuses have total so far. The pair wants to know the average monthly compensation for May was. A freshly minted public company is one step closer to getting its facial recognition software in ride-hail fleets to record ad impressions. Founded in 2018, Alfi wants to be the intermediary between advertisers and passengers, collecting data from the faces of people watching content and ads on custom tablets. (It went public this spring. It has been hard if not impossible for advertisers to know with any degree of certainty if people are watching their messages. Small monitors showing local news and entertainment clips have played in the back of taxis for many years (it can feel like eons), and while everyone involved tells the ad people that riders pay at least some attention, the savvy know better. Alfis product is a functional tablet (it responds to typical commands), with camera, mounted on a seatback. Computer vision captures real-time demographic information and reactions. In theory, an advertiser could audit a representative sample of sessions to gauge a campaigns success. There seems to be no provision for the actions of angry, inebriated (or both) riders picked up when the clubs close. Noah Edwardsen, head of corporate communications at Uber, told DailyMail.com: 'Uber doesn't have a deal with Alfi. So if this is happening, it is something they are doing with drivers individually. While a Lyft spokes person told DailyMail.com:'Lyft leaves third-party advertisements to be regulated by local laws, and drivers should keep in mind the impact they could have on riders. Uber and Lyft vehicles will soon have digital tablets in back seats that display ads and track riders' faces to gauge reactions. However, Alfi has yet to reveal how it determines what visuals and messages are shown to which individuals. So instead of seeing ads for retirement homes or ads for wheelchairswhich are not really relevant to you as a 25 year old femaleinstead what you're seeing are lady's fashionable wear and designer sunglasses specifically curated for you. Alfia self-described 'AI enterprise SaaS platform company powering computer vision with machine learning models' announced last week it is providing drivers of the ride-sharing companies with 10,000 camera equipped devices. The tablets are equipped with Alfi's specialized algorithm that tracks 'small facial cues,' according to the company's website. Alfi's computer vision also changes and runs ads remotely 'to deliver the right content, to the right person, at the right time in a responsible and privacy compliant manner,' according to the firm. However, the first also notes that it 'respects user privacy; without tracking, storing cookies, or using identifiable personal information. Alfi is starting with drivers in Miami, Florida and plans to 'light up every major city in the USA,' Florentino Diaz, Project Manager of Alfi, said in a June 15 statement. The tablets are equipped with Alfi's specialized algorithm that tracks 'small facial cues,' according to the company's website. There is no other choice when it comes to accurate brand targeting and real data reporting,' Diaz added. We have a massive operation unfolding in Miami today with hundreds of Ubers and Lyfts installing Alfi tablets. It's an epic event and marks the start of our nationwide rollout. What's most rewarding, is we can help rideshare drivers increase their income and help create employment opportunities with community outreach programs for students. Last week, Uber and Lyft teamed up with a company that specializes in artificial intelligence (AI) to provide 10,000 drivers with digital tablets to track customers facial expressions and catalog riders information as they react to advertisements. The deal is part of a larger program in which Alfi gives drivers free tablets with built-in cameras, which will be used to recognize the demographics of the rider and display personalized content as well as advertisements. Drivers who participate in the program are promised a revenue share up to $350 per month if passengers engage with the content or advertisements. Alfi provides data rich reporting functionality that informs the advertisers that someone viewed their ad, the number of views, and each viewer's reaction to the ad. Increasingly, advertisers are demanding improved performance and capabilities from the ad technology they utilize. Alfi delivers for advertisers with analytics, accountability, transparency, proof of engagement and actual impressions, the companys press release stated. We have a massive operation unfolding in Miami today with hundreds of Ubers and Lyfts installing Alfi tablets, Florentino continued. Its an epic event and marks the start of our nationwide rollout. Whats most rewarding, is we can help rideshare drivers increase their income and help create employment opportunities with community outreach programs for students. Juan Carlos, a Miami-area Uber driver, said in a statement, This is a great opportunity for all of us rideshare drivers. Alfi not only gives us extra revenue, but it is a great tool for our customers. We can share places to eat, places to shop, and things to do around Miami. I absolutely love my Alfi. BC-Meme-Stock-Alfis-Facial-Recognition-Ad-Technology-Fans-Privacy-Concerns , Brody Ford Bloomberg) -- Alfi Inc., a small artificial intelligence software company, has ambitious plans to use facial recognition to target individualized ads to people as they walk through an airport, a shopping mall or stare at a screen in the back of an Uber. The idea could resonate with an ad industry thats grappling with the demise of tracking people across the internet through cookies. But the company, which has yet to sign up more than one advertiser,is also sparking some of the same concerns about privacy that has stymied even the more traditional tech giants. This disproportionate gap between its vision and its fundamentals has made Alfi a popular meme stock, subject to wide swings in its share price.Alfis software is designed to show ads to people based on their age, gender and ethnicity without specifically identifying the person. Using small facial cues,the company says it can then provide information to advertisers about a persons reaction to a product. An announcement in July that it plans to equip rideshare drivers with tablets to target ads to passengers caught the attention of two U.S. senators. Democrats Amy Klobuchar and Richard Blumenthalfired off a letter to the heads of the ride-hailing companies, claiming the program raises serious concerns about privacy for your passengers. Uber Technologies Inc. said in a statement to Bloomberg that it has no business relationship with Alfi the company contracts with the drivers directly and that drivers must comply with local laws and regulations. Lyft Inc. echoed the sentiment, and added that drivers may be deactivated if they collect passenger information in violation of local laws. Alfi, founded in 2018 and based in Miami Beach, Florida, insists it doesnt violate any privacy rules and is compliant with Europes General Data Protection Regulation, which imposes strict regulations on data collection, and Californias Consumer Privacy Act, which allows consumers to see all the data collected on them. The company says its system makes no attempt to identify viewers, doesnt save images, and targets ads in an ethical and privacy-compliant manner. No viewer is ever required or requested to enter any information about themselves on any Alfi-enabled device, according to the company. We only use a sensor to detect simple metrics like age and gender, said Paul Pereira, founder and chief executive officer of Alfi, in an interview. Just the fact that we can pick up a gender on a predictive model without violating privacy puts us 50% ahead of everything thats out there in the market today. Read More: Activists Urge Retailers to Halt Facial Recognition Use. To reiterate that its not collecting personal information, Alfi makes the distinction that it uses facial detection, which analyzes demographics and emotions, not facial recognition, which stores data to identify an individual. But data privacy advocates warn that whatever its called, the technology inherently invites abuse. Alfi goes out of its way to say that it doesnt gather images or recordings, but thats not to say it cant, and there are no laws that would require the company to disclose whether or not they did start gathering this information, said Caitlin Seeley George, campaign director at Fight for the Future, an activist group. Selling user data, another flash point for tech companies, is also part of Alfis long-term strategy. Cameras in the hardware monitor viewers reactions to the content in order to provide insights to advertisers. The company said subscriptions to engagement data including retina tracking, keyword recognition, voice intonation, and demographics will be sold to third parties without compromising the identity of the end user. Brazils Sao Paulo airport already features some Alfi reactive screens and the company is expanding its tablet program for ride-hailing vehicles, which was in testing mode with about 500 drivers in South Florida, to about 13 markets in the U.S. The company aims to install 150,000 facial recognition screens in the back of rideshare vehicles by the end of next year. More than 50,000 drivers have joined the wait-list to receive one of Alfis Lenovo Group Ltd. tablets, Pereira said. Theyre driven by the promise of commissions as high as $325 a month, though some drivers whove been using Alfi screens in the initial early rollout have had trouble receiving pay, and others are skeptical about how the model will work. Ive had it for almost two months and also havent received any pay, or any sort of news from Alfi on how we get paid, said a Miami-area rideshare driver who spoke on the condition of anonymity for fear of continued payment delays. Pereira said that while a driver payment system is fully operational, it hasnt been used yet, because many advertising deals and other revenue streams are still pending. In the past, some drivers were given pre-paid gift cards, he said. Indeed, Alfis monetization plans are still in the early stages. Alfi reported second-quarter revenue of less than $1,000 and a net loss of almost $5 million. A spokesperson said the out-of-home advertising market was severely impacted by Covid-19 but that Alfi has multiple customers coming onto the platform. The company expects to start reaping revenue from advertising in the third quarter. When it announced the national rollout of its rideshare partner program earlier this month, Alfis stock jumped 40%. The company, which became publicly traded in May, is no stranger to extreme share price moves as a meme stock, buffeted by retail traders who exchange news and rumors on internet message boards such as Reddits ALFISTOCK forum. Pereira said that retail traders are the majority of their base but that may not be such a bad thing. These retail traders possibly helped bring Alfi back from the brink of collapse. At the end of March 2021, Alfis total current assets sat at about $113,000, and its auditors found substantial doubt about its ability to continue. By the end of June, the company was flush with $20 million in cash thanks to an initial public offering and a share price that has doubled since its listing.",,Privacy,2023-07-03,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alfi-personalised-real-time-advertising,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The incident highlighted a lack of transparency in the AI system's decision-making process.,High impact due to potential misuse and misunderstanding of AI-generated content.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The system's lack of probabilistic output leads to over-reliance on its decisions.,High impact as it could result in following inaccurate or misleading AI-generated classifications.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The system's deterministic output shifts the burden of proof onto the suspect.,High impact due to potential misuse and harm from misleading AI-generated classifications.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Users are unable to understand the decision-making process of the AI system.,Medium impact as it leads to user frustration and lack of trust.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The system's influence on decision-making is not transparent.,Medium impact as it obscures accountability.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no measures for users to seek redress in case of harm.,High impact due to lack of recourse for affected individuals.,The dataset that was used to train the AI system did not include potentially marginalized groups (e.g. persons with disabilities).,The training dataset excluded marginalized groups.,Medium impact due to lack of inclusivity.,The development and testing phases of the AI system were not subject to guidance and checks to avoid bias.,Bias checks were not conducted during development and testing.,Medium impact due to potential biases in AI decisions.,"There is no reference to the inclusion of potentially marginalized groups in datasets, and a lack of guidance to avoid bias in algorithm development and testing.",The system fails to account for marginalized groups in its dataset.,Medium impact due to lack of inclusivity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The system lacks mechanisms to limit deployment to only suspected individuals.,Medium impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Individuals could not easily verify the authenticity of the information generated by the AI.,Medium impact due to difficulties faced by individuals in accessing and verifying data.,There are no mechanisms for the user to exercise control over the processing of personal data.,Users have no control over how their personal data is processed.,High impact as it affects user privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,The system does not ensure that personal data is processed lawfully.,High impact as it risks violating data protection laws.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The system lacks procedures to limit access to personal data.,High impact as it could lead to unauthorized access to sensitive data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The system does not provide mechanisms to exercise data subject rights.,High impact as it infringes on individual data rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks specific security measures for personal data.,High impact due to potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The system does not have procedures for conducting data protection impact assessments.,High impact as it risks non-compliance with data protection regulations.
fria-instance-gpt-27.ttl,Algorithm misses gambling addict red flags,Machine learning,"Luke Ashton, from Leicester, UK, was offered a free bet by gambling company Betfair and died after gambling over 100 times a day and building up debts of GBP 18,000. Ashton's lawyer said the company relied on a machine learning algorithm that daily analysed 277 elements of its customers' betting activities to detect problem gamblers who would then be telephoned by its player protection team.",A gambling addict who committed suicide in April 2021 after racking up large debts had been categorised as a 'low-risk' customer by a Betfair algorithm that had 'found nothing in his betting patterns that would trigger human intervention that might have restricted his gambling.,The system uses a machine learning algorithm to detect customer risk levels and track customer data. Betfair's algorithm analysed 277 elements of its customers' betting activities daily.,"The purpose of the machine learning algorithm was to detect problem gamblers and facilitate human intervention. However, it failed to flag Luke Ashton as high-risk despite significant red flags in his betting behavior.",2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/algorithm-misses-gambling-addict-red-flags,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,There is no indication of the extent to which the AI system influences the overall decision-making process.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,High,The AI system targets members of a specific social group.,The AI system targets members of a specific social group.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data stored, recorded, and produced are not easily accessible to concerned individuals.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,There are no mechanisms for the user to exercise control over the processing of personal data.,High,There are no measures to ensure the lawfulness of the processing of personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",High,There is no procedure to conduct a data protection impact assessment.,There is no procedure to conduct a data protection impact assessment.,High
fria-instance-gpt-28.ttl,Allocation algorithm wrongly places thousands of Italian teachers,Resource allocation algorithm,"An algorithm used by the Italian government to allocate the location of thousands of teachers in 2016 resulted in many of them having to relocate to inappropriate locations. The incident triggered controversy about the purpose, design, and the effectiveness of the algorithm, and led to multiple legal complaints, lawsuits, and to the discontinuation of the system.","Designed and developed by HP Enterprise and Finmeccanica to the tune of EUR 444,000, Italy's so-called 'Buona Scuola' algorithm was meant to assess and score every teacher based on criteria including their work experience and performance and their preferred destinations, and match them with the most appropriate vacancies. However, teachers and their families were relocated across the country in an apparently more or less random manner, triggering uproar. A subsequent assessment of the system found it to be fully automated, 'unmanageable', full of bugs, and impossible to properly evaluate due to its opaque nature.","The system uses a resource allocation algorithm to assign teacher positions based on multiple criteria, but failed due to technical flaws and lack of transparency.","The purpose of the algorithm was to efficiently allocate teachers to appropriate locations, but it failed, causing widespread disruption and legal challenges.",2023-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allocation-algorithm-wrongly-places-thousands-of-italian-teachers,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,There is no indication of the extent to which the AI system influences the overall decision-making process.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,High,The AI system targets members of a specific social group.,The AI system targets members of a specific social group.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data stored, recorded, and produced are not easily accessible to concerned individuals.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,There are no mechanisms for the user to exercise control over the processing of personal data.,High,There are no measures to ensure the lawfulness of the processing of personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",High,There is no procedure to conduct a data protection impact assessment.,There is no procedure to conduct a data protection impact assessment.,High
fria-instance-gpt-29.ttl,Allstate car insurance 'suckers list' overcharging,Price adjustment algorithm,"A joint investigation by The Mark Up and Consumer Reports has found that Allstate, the fourth largest insurer in the US, has been increasing premiums on a 'suckers list' of customers already paying the highest rates for their insurance, while keeping premiums more or less the same for 'thriftier' customers. To identify the 'suckers', Allstate devised a 'customer retention model' or 'advanced' price adjustment algorithm that was said to contain dozens of variables that would adjust insurance premiums in the direction of the company's new risk model. But, according to the investigators, the model was actually very simple: it identified existing big spenders and squeezed more money out of them than others. The algorithm also determined which customers were owed discounts. Though some customers were owed thousands of dollars, Allstate capped the discounts at a half percent irrespective of the amount owed. Senior customers were overrepresented within this cohort. Insurers are not obliged to inform customers if they are denied discounts, and the National Association of Insurance Commissioners told The Markup that it had never heard of an insurer voluntarily informing its customers that they had been denied a discount. Maryland rejected Allstate's proposal on the grounds that it was discriminatory. But it was approved by Arizona, Arkansas, Wisconsin, and a number of other states.","Seven years ago, Allstate Corporation told Maryland regulators it was time to update its auto insurance rates. The insurer said its new, sophisticated risk analysis showed it was charging nearly all of its 93,000 Maryland customers outdated premiums. Some of the old rates were off by miles. One 36-year-old man from Prince Georges County, Md., who Allstate said in public records should have been paying $3,750 every six months, was instead being charged twice that, more than $7,500. Other customers were paying hundreds or thousands of dollars less than they should have been, based on Allstate's new calculation of the risk that they would file a claim. Rather than apply the new rates all at once, Allstate asked the Maryland Insurance Administration for permission to run each policy through an advanced algorithm containing dozens of variables that would adjust it in the general direction of the new risk model. Allstate said the goal of this new customer retention model, which it was rolling out across the country, was to limit policy cancellations from sticker shock. After questions from regulators, the insurer submitted thousands of pages of documentation on the price changes including data showing how they would affect each individual customer, a rare public window into details of its auto insurance pricing that have otherwise been kept behind a wall of privacy, labeled a trade secret. When The Markup and Consumer Reports conducted a statistical analysis of the Maryland documents, we found that, despite the purported complexity of Allstate's price-adjustment algorithm, it was actually simple: It resulted in a 'suckers list' of Maryland customers who were big spenders and would squeeze more money out of them than others. Customers who were already paying the highest premiums, of about $1,900 or more every six months, and were due an increase would have borne price hikes of up to 20 percent. But drivers with cheaper policies, who deserved price jumps that were just as big, would be charged a maximum increase of only 5 percent. Customers in the 20 percent group were more likely to be middle-aged.",The system uses a price adjustment algorithm to assess customer risk but has led to biased and discriminatory practices.,"The purpose of the algorithm was to assess customer risk and adjust premiums accordingly, but it failed, causing discriminatory price adjustments based on age and income.",2023-01-30,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allstate-car-insurance-suckers-list-overcharging,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,There is no indication of the extent to which the AI system influences the overall decision-making process.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,High,The AI system targets members of a specific social group.,The AI system targets members of a specific social group.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data stored, recorded, and produced are not easily accessible to concerned individuals.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,There are no mechanisms for the user to exercise control over the processing of personal data.,High,There are no measures to ensure the lawfulness of the processing of personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",High,There is no procedure to conduct a data protection impact assessment.,There is no procedure to conduct a data protection impact assessment.,High
fria-instance-gpt-30.ttl,"Alonzo Sawyer facial recognition wrongful arrest, jailing",CCTV| Facial recognition,"54-year old Alonzo Sawyer was arrested and jailed for nine days for an assault and theft he did not commit thanks to poor analysis of CCTV footage using facial recognition by an intelligence analyst at the Maryland Transit Administration Police, a unit of Baltimore Police Department. The analysis failed to take into account the fact that Sawyer is older, taller than the suspect in the video, has facial hair and gaps between his teeth, and his right foot slews out when he walks, according to photographs shown to the police by his wife. Maryland Chiefs of Police Association president Russ Hamill said that what happened to Alonzo Sawyer was 'horrifying' when speaking in opposition to a bill seeking to regulate the use of facial recognition in Maryland.","In January 2023, it emerged that Georgia man Randall Reid had been wrongly arrested and jailed for a purse theft incident in Baton Rouge using facial recognition by Louisiana authorities, even though he had never visited the state. At a police station and in a meeting with her husband's former parole officer, the person who had confirmed the software's suggested match, Carronne drew attention to details in photos on her phone taken recently by her daughter. Her husband is taller than the suspect in the video, she explained, and has facial hair and gaps between his teeth. His right foot slews out when he walks, something she did not see in video footage of the attack. I said my husband is 54 years old. This guy looks like he could be our son, Carronne says. Alonzo was eventually released after nine days in jail, she says, during which time he missed his wife's Gladys Knight tribute show and his work as a barber, and could not complete a construction contract he had secured. I'm just grateful I was able to do all the labor and running around, because had I not he would still be sitting there for something he didn't do, Carronne says. The Sawyer's ordeal took place in spring 2022 but has not previously been reported. Around the time Alonzo was released, the victim in the bus incident identified another man as the suspect in the video, Deon Ballard, who is 7 inches shorter and more than 20 years younger than Sawyer, according to charging documents. Ballard's mother and a police officer who arrested him confirmed that identification, one document shows, and he is due to stand trial in April. Maryland Transit Administration Police did not respond to repeated requests for comment and deputy state's attorney for Baltimore County John Cox declined to confirm Ballard and Sawyer were arrested for the same crime. WIRED was unable to speak with Alonzo Sawyer, who is serving time in a Maryland jail on a charge unrelated to the bus incident.","The system uses facial recognition technology to analyze CCTV footage, but failed due to inaccuracies and biases in the technology.","The purpose of the facial recognition technology was to enhance security, but it failed, causing wrongful arrest and highlighting issues of accuracy, reliability, and bias.",2023-01-30,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alonzo-sawyer-facial-recognition-mistaken-arrest,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,High,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",High,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.","The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",High,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,High,There is no indication of the extent to which the AI system influences the overall decision-making process.,There is no indication of the extent to which the AI system influences the overall decision-making process.,High,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,High,The AI system targets members of a specific social group.,The AI system targets members of a specific social group.,High,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",High,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,High,There is no mechanism to limit the deployment of the AI system to suspected individuals.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,High,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","The data stored, recorded, and produced are not easily accessible to concerned individuals.",High,There are no mechanisms for the user to exercise control over the processing of personal data.,There are no mechanisms for the user to exercise control over the processing of personal data.,High,There are no measures to ensure the lawfulness of the processing of personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,High,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,High,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",High,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).","There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",High,There is no procedure to conduct a data protection impact assessment.,There is no procedure to conduct a data protection impact assessment.,High
fria-instance-gpt-31.ttl,"Amazon, Waterstones algorithms promote vaccine misinformation","Researchers at the University of Washington and Sky News conducted studies revealing the promotion of vaccine misinformation by Amazon, Waterstones, and Foyles algorithms.",Researchers at the University of Washington; Sky News.,,"The recommendation algorithms used by Amazon, Waterstones, and Foyles.",The study aimed to highlight how recommendation algorithms contribute to the spread of vaccine misinformation.,2023-07-07,https://www.aiaaic.org/aiaaic-repository/amazon-algorithms-promote-vaccine-misinformation,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"The algorithms do not provide transparency regarding how they prioritize and recommend content, which can lead to unfair treatment of certain viewpoints and the spread of misinformation.",Lack of transparency can undermine trust and the perceived fairness of the system.,The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect.,"The lack of probabilistic information leads to over-reliance on the recommendations, with users believing them to be more accurate than they might be.","This can lead to misinformation being taken as fact, impacting public health decisions.",The AI system produces an outcome that forces a reversal of burden of proof upon the suspect.,"The algorithms present information as absolute truths, making it difficult for users to contest or verify the information provided.",This can lead to an erosion of critical thinking and informed decision-making among users.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,"Users are not provided with explanations or the reasoning behind the recommendations, which hinders their ability to make informed choices.",This lack of explainability reduces user autonomy and trust in the system.,There is no indication of the extent to which the AI system influences the overall decision-making process.,"The influence of the algorithm on decision-making is not clear, making it difficult to assess its impact.","This opacity can lead to unchecked algorithmic influence, with potential negative consequences for user decision-making.",There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,There are no mechanisms in place for users to seek redress or corrections if they are harmed by the misinformation.,The lack of redress mechanisms can lead to prolonged harm and a lack of accountability for the platform.,The AI system targets members of a specific social group.,The algorithms may disproportionately target or exclude specific social groups based on biased data.,This can lead to unequal treatment and reinforce existing social biases.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The platform lacks robust mechanisms to identify and address bias or discrimination in its recommendations.,"Without such mechanisms, biased or discriminatory recommendations can persist unchecked.",The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,"The recommendation algorithms do not adequately account for the diversity of users, leading to potentially unrepresentative results.",This can result in certain groups being marginalized or underserved by the recommendations.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"The algorithms do not include measures to limit their application to individuals where appropriate, potentially leading to overreach.",This can result in unwarranted invasions of privacy and misuse of data.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.","Users do not have easy access to the data that is collected and used by the algorithms, hindering their ability to understand and control its use.",This lack of accessibility can lead to a loss of control over personal information and reduced transparency.,There are no mechanisms for the user to exercise control over the processing of personal data.,The system does not provide users with sufficient controls to manage how their personal data is processed.,This can lead to misuse of personal data and reduced user trust.,There are no measures to ensure the lawfulness of the processing of personal data.,The platform lacks adequate measures to ensure that personal data processing complies with legal standards.,This can result in unlawful processing activities and potential legal repercussions.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The algorithms do not have procedures to restrict access to personal data to only what is necessary.,This can lead to excessive data access and increased risk of data breaches.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).","The platform does not facilitate the exercise of data subject rights, such as access, rectification, and erasure of personal data.",This can result in non-compliance with data protection laws and reduced user rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The system lacks advanced security measures like encryption and anonymization to protect personal data.,This can lead to vulnerabilities and potential data breaches.,There is no procedure to conduct a data protection impact assessment.,The platform does not perform data protection impact assessments to identify and mitigate risks.,"Without impact assessments, potential risks to data protection are not proactively addressed."
fria-instance-gpt-32.ttl,Amazon Astro home robot,Robotics; Computer vision; Facial recognition,USA,,"Astro is powered by Amazon's Alexa smart home technology and incorporates advanced artificial intelligence, computer vision, sensor technology, and facial recognition. It includes features such as a periscope camera, digital eyes on a rotating screen, and the ability to patrol homes autonomously, recognize faces, and notify owners of unusual activities. Despite these advancements, internal documents reveal concerns about its reliability, person recognition capabilities, and potential safety hazards such as falling down stairs.","Astro aims to strengthen home security by monitoring for unusual activities, assisting with home monitoring, and interacting with household members through its personality-driven design. It is marketed as a helper for tasks such as checking on pets, delivering items within the home, and even assisting the elderly. However, there are significant concerns related to privacy, surveillance, accuracy/reliability, dual/multi-use, and the appropriateness or need for such a device in homes.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-astro-home-robot,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-33.ttl,Amazon UK automated pricing glitch,Pricing automation,UK,,"The glitch was caused by Repricer Express, a software that automatically adjusts prices of third-party products on Amazon Marketplace to ensure they are the cheapest available. The error led to thousands of items being listed at 1p, causing significant financial losses to many small businesses. Repricer Express's software is designed to continuously reprice items based on competitive listings, but it malfunctioned, drastically reducing prices across the platform.","The primary purpose of the Repricer Express software is to maintain competitive pricing for sellers on Amazon Marketplace. However, the glitch highlighted significant risks associated with automated pricing systems, including the potential for large-scale financial loss and disruption to business operations. The incident underscores the need for robust fail-safes and oversight mechanisms in pricing automation to prevent similar occurrences.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-automated-pricing-glitch,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-34.ttl,Amazon AWS Panorama automated workplace surveillance,"CCTV, Computer vision",USA,,"Amazon AWS Panorama is a new service that enables the automated analysis of workplace security camera footage using computer vision technology. It can automate inspection tasks, detect manufacturing defects, track the movement of barcodes and labels, and monitor workplace safety and security. The system can be used to detect when staff are not wearing face masks or socially distancing, as well as monitor compliance with other workplace rules. It involves retrofitting a box to existing security cameras, allowing analysis to occur on-device without the need for data to leave the customer's facility.","The primary purpose of AWS Panorama is to improve industrial operations and workplace safety by automating the analysis of security camera footage. It can be used to evaluate manufacturing quality, identify bottlenecks in industrial processes, and monitor workplace safety and security. However, it raises concerns about workplace privacy and surveillance, as it can monitor people in addition to products. The technology is marketed as a solution to help companies comply with COVID-19 safety measures, but it can also be used to track employee performance and adherence to workplace rules, potentially leading to privacy and ethical issues.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-aws-panorama-workplace-surveillance,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-35.ttl,Amazon botches delivery drone commercial launch,Drone,USA,,"Amazon's Prime Air drone delivery service uses the MK27-2 drone, designed to deliver packages quickly and safely. The technology includes a sophisticated sense-and-avoid system that helps the drone navigate and avoid obstacles. However, the system has faced significant technical challenges, including a failed flight package software and issues with replacement drones. Additionally, Amazon's drones have had safety incidents, such as a crash in Oregon that led to a fire. The technology is still undergoing extensive testing and regulatory scrutiny, particularly from the FAA.","Prime Air aims to revolutionize product delivery by using drones to deliver packages within 30 minutes. The service is designed to enhance logistics efficiency, reduce delivery times, and decrease carbon emissions. However, the launch has been marred by technical failures, regulatory hurdles, and safety concerns. The program's success is crucial for Amazon's goal of scaling drone deliveries across large communities. Feedback from initial test locations like Lockeford, California, is intended to help refine the service for broader deployment.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-botches-delivery-drone-commercial-launch,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-36.ttl,Amazon chemical food preservative suicides,Recommendation algorithm,USA; India,,"Amazon's recommendation algorithm has come under scrutiny for promoting a food preservative containing a chemical compound that has been used in suicides. The algorithm reportedly suggested related items that people planning suicide typically purchase. The preservative is legally sold in the US and many other countries, but platforms like Etsy and eBay have banned its sale. Amazon has resisted limiting or stopping its sale despite warnings from families of suicide victims and pressure from lawmakers.","The primary purpose of Amazon's recommendation algorithm is to suggest products to customers based on their browsing and purchase history. However, this has led to unintended consequences where the algorithm recommended items that could be used in conjunction with a chemical preservative for suicide. The ease of purchasing this preservative and related items has raised safety concerns. Lawmakers and families have urged Amazon to address this issue, emphasizing the need for responsible product recommendations to prevent misuse and protect vulnerable individuals.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-chemical-food-preservative-suicides,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-37.ttl,Amazon Driveri delivery driver safety monitoring,CCTV | Computer vision,USA,,"Amazon is installing 'innovative' AI-enabled video cameras in Amazon-branded delivery vans. The Netradyne-supplied cameras access drivers' location, movement, and biometric data to detect risky driver behavior. Verbal warnings are issued when drivers appear distracted, ignore signposts, or drive too fast. Drivers who refuse to sign forms allowing Amazon to collect, store, and use their facial and other biometric data lose their jobs. Complaints have been raised about the accuracy, fairness, security, and privacy of the system. There are also concerns about the impact on employment, job satisfaction, and the ability to lodge complaints and appeals.","The primary purpose of Amazon's AI-enabled video cameras is to improve the safety of delivery drivers by monitoring and alerting them to risky behaviors. However, the system has raised significant concerns among drivers and digital rights advocates. Drivers argue that the system is inaccurate, unfair, and intrusive, impacting their job security and privacy. The system's implementation has led to increased pressure on drivers and difficulties in lodging meaningful complaints and appeals. The cameras record continuously and can trigger uploads to a secure portal accessible by Amazon and delivery service providers (DSPs). The footage can be used for safety enforcement and incident investigations.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-delivery-driver-safety-cameras,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defence of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,There are no mechanisms to flag and correct issues of bias and discrimination.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-38.ttl,"Amazon delivery drone malfunctions, sparks 25-acre fire",Drone,USA,,"An Amazon Prime Air delivery drone on a test flight crashed into a field in eastern Oregon in June 2021, setting 25 acres of wheat on fire. The drone's motor shut off during a flight transition, with two safety features failing. This caused the drone to flip upside down and tumble until it hit the ground, resulting in an intense lithium battery fire.","The primary purpose of Amazon's Prime Air delivery drones is to deliver products. The incident raised significant concerns about the accuracy, reliability, robustness, safety, and environmental impact of the drone program. Amazon has faced challenges with multiple crashes, high turnover, and federal scrutiny. Despite receiving a Part 135 Air Carrier certification from the FAA in August 2020, the program has yet to achieve widespread commercial deployment.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-delivery-drone-crashes-sparks-22-acre-fire,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,There are no mechanisms to flag and correct issues of bias and discrimination.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-39.ttl,Amazon Echo Dot Kids remembers kids' conversations,Speech recognition| Natural language understanding (NLU),USA,,"A coalition of nineteen privacy groups has filed a legal complaint with the US Federal Trade Commission (FTC) alleging that Amazon was holding onto a child’s personal information for too long and violating the US Children’s Online Privacy Protection Act (COPPA). The coalition, led by Campaign for a Commercial-Free Childhood (CCFC), Center for Digital Democracy (CDC), and Georgetown University’s Institute for Public Representation, said Amazon's Echo Dot Kids smart speaker records and collects 'vast amounts of sensitive, personal information from children under 13' without adequate parental consent. Amazon's Echo Dot Kids Edition was found to retain personal details and conversations of children despite attempts to delete them using the FreeTime feature on Amazon Alexa mobile app.","The primary purpose of Amazon Echo Dot Kids is to provide information and services to children. However, the incident raised significant concerns about privacy, as the device collects and retains sensitive personal information from children under 13 without proper parental consent. Amazon markets the device as a tool for education and entertainment, but privacy groups argue that it is a means to gather data from children.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-echo-dot-kids-remembers-kids-conversations,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,There are no mechanisms to flag and correct issues of bias and discrimination.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-40.ttl,Amazon employees use Ring to spy on customers,CCTV| Computer vision,USA,,"Employees at Amazon Ring and a Ukrainian contractor were able to access and download customer videos and use them however they liked. The discovery prompted accusations of privacy abuse and incurred a USD 5.8 million fine. According to the US Federal Trade Commission (FTC), Amazon's Ring doorbell unit violated a section of the FTC Act that prohibits unfair or deceptive business practices, with some of its people viewing thousands of videos of female users in their bedrooms and bathrooms until Ring restricted employee access to customer videos in September 2017. As part of the proposed settlement, Ring is required to delete any customer videos and data collected from an individual's face that it obtained prior to 2018, and delete any work products it derived from those videos. Amazon responded to the settlement by saying that Ring had already addressed the privacy and security issues before the FTC began its inquiry.","The primary purpose of Amazon Ring is to strengthen security through CCTV and computer vision technologies. However, the incident highlighted severe privacy and security issues as employees accessed and misused sensitive customer videos.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-employees-use-ring-to-spy-on-customers,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,There are no mechanisms to flag and correct issues of bias and discrimination.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-41.ttl,Amazon Flex algorithm fires delivery drivers,Automated management system| Image recognition,USA,,"A report from Bloomberg alleges that millions of independent contract drivers working for Amazon's Flex delivery service are being managed and fired by an algorithm with little human intervention. Some dismissals are considered unfair, with riders given only 10 days to appeal their terminations. If their appeal fails, they must then pay USD 200 for it to go to arbitration. To make matters even more challenging, riders are unable to have access to the algorithm and its decision-making processes. In addition, notes Ars Technica, Flex drivers are complaining in forums about having their accounts terminated because their selfies failed to 'meet the requirements for the Amazon Flex program.' Executives knew this was going to fail, according to a former engineer who designed the system. Flex hirings, performance reports, and firings are all handled by software, with minimal intervention by humans. Drivers sign up and upload required documents via a smartphone app, through which they also sign up for shifts, coordinate deliveries, and report problems. It's also how drivers monitor their ratings, which fall into four broad buckets: Fantastic, Great, Fair, or At Risk. Flex drivers are assessed on a range of variables, including on-time performance, details like whether the package is sufficiently hidden from the street, and a driver's ability to fulfill customer requests. Amazon refutes the idea that drivers have been unfairly treated. Another driver Bloomberg spoke with had trouble delivering packages to apartment complexes before dawn due to locked gates and unresponsive package recipients, leading to a lower rating and eventual termination. Flex drivers' forums are filled with complaints about accounts being terminated due to failed selfie verification by image recognition algorithms, especially in low-light conditions.","The primary purpose of Amazon Flex is to increase efficiency through an automated management system and image recognition technology. However, the incident has raised significant concerns regarding fairness, employment pay, and termination processes.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-flex-algorithm-delivery-driver-firings,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,The AI system indirectly targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-42.ttl,Amazon Flex delivery drivers forced to take unsafe routes,Routing algorithm,USA; EU; UK; Australia,,"Amazon delivery drivers are being forced to take dangerous routes and run across four-lane highways at night with multiple packages and boxes, Vice News reports. Per Vice, Amazon's routing algorithms sometimes group deliveries on both sides of a street into a single stop, forcing drivers to risk their personal safety in order to satisfy the demands of the company's cost and speed-focused Flex routing algorithm. Other drivers bring another person with them to park and look after their vehicles when they make deliveries. Approximately 85,000 contracted delivery drivers across the US and Europe purportedly use Amazon's Flex algorithm. Mike, an Amazon delivery driver in central Florida, is accustomed to jogging back and forth across three-lane highways to deliver packages. He has sprinted across busy commercial streets during rush hour and crossed rural highways on foot at sundown. 'I find the most dangerous to be smaller two-lane highways with almost no room to pull off the road,' Mike told Motherboard in May. (Mike spoke on condition of pseudonymity because he fears he could lose his job for speaking to the press.) 'The speed limits on these roads will often be 50-60 mph and we're having to pull halfway off the road and then [walk across]... oftentimes at night.' Motherboard spoke to Amazon delivery drivers who work in Florida, Illinois, Michigan, South Carolina, Tennessee, Indiana, and California who described sprinting across the street or the highway to follow the Flex app's directions. This app determines delivery routes for both Amazon's contracted delivery drivers, who drive Amazon-branded vans, and members of its independent contractor workforce, known as Amazon Flex drivers, who drive their own cars. When a driver has to make deliveries to several addresses that are clustered together, the Flex app combines them into a single stop, rather than making a stop at each address. Drivers call these 'group stops,' while Amazon research scientists and engineers tasked with optimizing routes that incorporate hundreds of stops per shift refer to this routing mechanism as 'stop consolidation.' Amazon's routing algorithm combines multiple nearby addresses with deliveries into a single stop. In the example above, A, B, and C are stops. While stops A and C are deliveries to a single house, stop B requires a driver park and then deliver to seven houses on foot, walking or sprinting back and forth across the street to do so. This process saves time but can be highly dangerous for drivers crossing highways and busy commercial streets.","The primary purpose of Amazon Flex is to manage package delivery efficiently through a routing algorithm. However, the incident raises significant concerns about safety and fairness.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-flex-delivery-driver-routing-safety,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system directly targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,The AI system indirectly targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-43.ttl,Amazon Go fails to inform NYC customers about facial recognition,Facial recognition; Computer vision; Deep learning,USA,,"A pair of lawsuits allege that Amazon failed to inform customers about its use of facial and body biometrics scanning at its cashierless Go retail stores in New York for over a year. According to a class-action lawsuit filed by Rodriguez Perez, Amazon had not informed him that his body and palm were scanned. Another suit, filed in February 2023 by Richard McCall, claims his palm was scanned. Both are alleged to be in violation of New York City's 2021 Biometric Identifier Information Law. The law requires all New York City businesses to post a sign informing customers or visitors that their biometrics are being recorded. Amazon denied the claims, telling Gizmodo, 'We do not use facial recognition technology in any of our stores, and claims made otherwise are false.' 'Only shoppers who choose to enroll in Amazon One and choose to be identified by hovering their palm over the Amazon One device have their palm-biometric data securely collected, and these individuals are provided the appropriate privacy disclosures during the enrollment process,' it said. First launched in 2018, Amazon Go is supposed to showcase the company's automated Just Walk Out system, which it says uses computer vision, deep learning algorithms, and sensor fusion to track consumers' 'virtual carts' to notate when they put an item in their cart or take it off the tab if they remove it. In 2023, Amazon announced it would close eight Amazon Go stores in Seattle, New York City, and San Francisco. If shoppers need a receipt, they can visit a kiosk inside the store and enter their email address. The receipt will then be emailed to them for that shopping trip and for any subsequent ones for which they will use the same credit card.","The primary purpose of Amazon Go's technology is to verify identity and manage store operations without cashiers. However, this incident raises significant concerns about privacy.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-go-fails-to-inform-nyc-customers-about-facial-recognition,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,The AI system directly targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,The AI system indirectly targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-44.ttl,Amazon India own brand search engine rigging,Search engine algorithm,"Amazon, Diego Piacentini, Russell Grandinetti, Amit Nanda, U.S. Congress, FTC, Reuters","Reuters has obtained thousands of pages of internal Amazon documents that show Amazon India systematically copied other companies' products and promoted them by manipulating search results on Amazon.in. This involved techniques such as 'search seeding' and 'search sparkles' to ensure Amazon's private label products, like AmazonBasics and Solimo, appeared among the top search results.","Search seeding, search sparkles, and internal data analysis of third-party products to develop and promote Amazon's own products.",Rank content/search results,2022-03-31,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-india-search-rigging,Manipulating search results to favor Amazon's products disrupts fair competition.,"Amazon India manipulated search results to favor its own private-label products, giving them an unfair advantage over competitors.",High impact due to distortion of competitive market dynamics.,Using internal data to replicate third-party products compromises market fairness.,"Amazon employees used internal data to copy successful third-party products, undermining fair competition.",High impact due to unfair use of proprietary data.,,,,,,,,,,,,,Copying products infringes on third-party intellectual property rights.,Amazon's strategy of copying third-party products infringes on intellectual property rights.,High impact due to intellectual property infringement.,,,,,,,Non-transparent manipulation of search results affects trust and fairness.,Amazon's practices of search seeding and sparkles were not transparent to third-party sellers or customers.,High impact due to lack of transparency in search result manipulation.,,,,Amazon's conflict of interest as both a marketplace operator and seller harms fair marketplace principles.,Amazon's dual role as a marketplace operator and seller of its own brands gives it undue advantage over other sellers.,High impact due to conflict of interest in marketplace operations.,Search result manipulation creates an unfair competitive environment.,The manipulation of search results to favor Amazon's private brands disadvantages other sellers.,High impact due to anti-competitive practices.,There are no procedures to limit the access to personal data to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data to the extent and amount necessary for those purposes.,High impact due to lack of control over personal data.,There is no clear mechanism for users to refuse police requests for footage without fear of repercussions.,There is no clear mechanism for users to refuse police requests for footage without fear of repercussions.,High impact due to lack of control over personal data sharing.,Amazon Ring systems lack specific security measures to ensure data protection in the event of unauthorized access.,Amazon Ring systems lack specific security measures to ensure data protection in the event of unauthorized access.,High impact due to potential security vulnerabilities.,There is no procedure to conduct a data protection impact assessment.,Amazon Ring systems lack a procedure to conduct a data protection impact assessment.,High impact due to lack of impact assessment procedures.
fria-instance-gpt-45.ttl,Amazon Mentor delivery driver scoring,Performance scoring algorithm,USA,,"A report by CNBC finds Amazon drivers are being monitored by Mentor, an app that promises to improve driver safety by generating a 'FICO' score each day that measures their driving performance. However, drivers say the app is often inaccurate, can lead to unfair disciplinary action, including loss of bonuses and perks. Mentor can also be invasive, tracking drivers' location after they clock out from work. Amazon also recently started using AI cameras in delivery vans to monitor driver behaviour and flag safety issues. Amazon requires contracted delivery drivers to download and continuously run a smartphone app, called 'Mentor,' that monitors their driving behavior while they're on the job. The app, which Amazon bills as a tool to improve driver safety, generates a score each day that measures employees' driving performance. Amazon spokesperson Deborah Bass told CNBC in a statement: 'Safety is Amazon's top priority. Whether it's state-of-the-art telemetrics and advanced safety technology in last-mile vans, driver-safety training programs, or continuous improvements within our mapping and routing technology, we have invested tens of millions of dollars in safety mechanisms across our network, and regularly communicate safety best practices to drivers.' But Bass did not respond to any of the specific allegations DSP drivers made to CNBC about the Mentor app detailed in this story, as well as questions about how the app uses certain behaviors to score drivers. The scores generated by the Mentor app are used in more ways than just evaluating an individual's job performance, drivers say. Amazon also looks at the scores, in part, when ranking a delivery partner's status, according to the drivers, who asked to remain anonymous out of fear of retaliation from Amazon. The ranking system for DSPs ranges from 'Poor' to 'Good' to 'Fantastic' to the top tier, referred to as 'Fantastic+.' A surplus of poor Mentor scores among a delivery partner's workforce can drag down the DSP's ranking, which can potentially jeopardize their access to benefits provided by Amazon, such as optimal delivery routes, the drivers said. The app also features a dashboard for drivers to 'see how they stack up against the rest of their team.' Mentor's score-based system raises concerns that the app intensifies the pressure of the job, pitting drivers and competing DSPs against each other to an unhealthy degree.","The primary purpose of the Mentor app is to assess delivery driver performance and enhance safety. However, issues regarding the app's accuracy, fairness, and privacy have been raised by drivers.",2024-07-07,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-mentor-dsp-delivery-driver-scoring,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,The evaluation content for FRIA-reportEvaluation11.,The impact level content for FRIA-reportImpactLevel11.,"The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.",The evaluation content for FRIA-reportEvaluation12.,The impact level content for FRIA-reportImpactLevel12.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.",The evaluation content for FRIA-reportEvaluation13.,The impact level content for FRIA-reportImpactLevel13.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,The evaluation content for FRIA-reportEvaluation14.,The impact level content for FRIA-reportImpactLevel14.,There is no indication of the extent to which the AI system influences the overall decision-making process.,The evaluation content for FRIA-reportEvaluation15.,The impact level content for FRIA-reportImpactLevel15.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,The evaluation content for FRIA-reportEvaluation16.,The impact level content for FRIA-reportImpactLevel16.,"The AI system impacts fundamental rights beyond those protected under non-discrimination laws, such as indirect impact on human dignity, access to health care, education, housing, etc.",The evaluation content for FRIA-reportEvaluation21.,The impact level content for FRIA-reportImpactLevel21.,The AI system indirectly targets members of a specific social group.,The evaluation content for FRIA-reportEvaluation22.,The impact level content for FRIA-reportImpactLevel22.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,The evaluation content for FRIA-reportEvaluation23.,The impact level content for FRIA-reportImpactLevel23.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,The evaluation content for FRIA-reportEvaluation31.,The impact level content for FRIA-reportImpactLevel31.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",The evaluation content for FRIA-reportEvaluation32.,The impact level content for FRIA-reportImpactLevel32.,There are no mechanisms for the user to exercise control over the processing of personal data.,The evaluation content for FRIA-reportEvaluation41.,The impact level content for FRIA-reportImpactLevel41.,There are no measures to ensure the lawfulness of the processing of personal data.,The evaluation content for FRIA-reportEvaluation42.,The impact level content for FRIA-reportImpactLevel42.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,The evaluation content for FRIA-reportEvaluation43.,The impact level content for FRIA-reportImpactLevel43.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",The evaluation content for FRIA-reportEvaluation44.,The impact level content for FRIA-reportImpactLevel44.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",The evaluation content for FRIA-reportEvaluation45.,The impact level content for FRIA-reportImpactLevel45.,There is no procedure to conduct a data protection impact assessment.,The evaluation content for FRIA-reportEvaluation46.,The impact level content for FRIA-reportImpactLevel46.
fria-instance-gpt-46.ttl,Amazon One Palmprint Biometric Opacity,Palm print scanning,"TechCrunch, Albert Fox Cahn, Surveillance Technology Oversight Project, U.S. Federal Trade Commission, advocacy groups","Amazon offers USD 10 for palmprints when signing up for Amazon One. Used in Amazon checkout-free stores to pay by holding palm over a scanner. Concerns raised over potential commercial use of palm data, despite encryption promises. Palm data stored indefinitely unless deleted by user.",Palm scanning hardware captures surface-area details and subcutaneous features to create a palm signature stored in the cloud.,Verify identity; Authorize transactions,2023-07-11,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-one-palmprint-biometrics,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Amazon One does not explicitly inform users that the transaction confirmation is generated algorithmically.,"Critical impact due to lack of transparency, affecting a large number of users.","The AI system does not provide percentages or other indication on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user that there is no possibility of error and therefore that the outcome is undoubtedly incriminating.","Amazon One does not provide information on the degree of likelihood that the outcome is correct/incorrect, prejudicing the user to believe there is no possibility of error.",High impact due to potential for significant user misinformation.,"The AI system produces an outcome that forces a reversal of burden of proof upon the suspect, by presenting itself as an absolute truth, practically depriving the defense of any chance to counter it.","Amazon One creates an outcome that forces a reversal of burden of proof upon the user, presenting itself as an absolute truth.",Very high impact due to undermining fundamental rights.,There is no explanation of reasons and criteria behind a certain output of the AI system that the user can understand.,Amazon One does not provide an explanation of reasons and criteria behind certain outputs that users can understand.,High impact due to lack of user understanding and potential for misuse.,There is no indication of the extent to which the AI system influences the overall decision-making process.,Amazon One does not indicate the extent to which the AI system influences the overall decision-making process.,Medium impact due to uncertainty in decision-making influence.,There is no set of measures that allow for redress in case of the occurrence of any harm or adverse impact.,Amazon One lacks measures that allow for redress in case of any harm or adverse impact.,High impact due to lack of redress mechanisms.,The AI system targets members of a specific social group.,Amazon One targets members of a specific social group.,Very high impact due to potential for significant discrimination.,"There are no mechanisms to flag and correct issues related to bias, discrimination, or poor performance.","Amazon One lacks mechanisms to flag and correct issues related to bias, discrimination, or poor performance.",High impact due to lack of corrective mechanisms.,The AI system does not consider the diversity and representativeness for specific population or problematic use cases.,Amazon One does not consider diversity and representativeness for specific population or problematic use cases.,High impact due to lack of consideration for diversity.,There is no mechanism to limit the deployment of the AI system to suspected individuals.,Amazon One lacks a mechanism to limit deployment to suspected individuals.,High impact due to potential overreach.,"The data stored, recorded, and produced are not easily accessible to concerned individuals.",Amazon One data is not easily accessible to concerned individuals.,High impact due to lack of data accessibility.,There are no mechanisms for the user to exercise control over the processing of personal data.,Amazon One lacks mechanisms for user control over personal data processing.,High impact due to lack of user control over personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,Amazon One lacks measures to ensure lawful processing of personal data.,High impact due to potential unlawful data processing.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,Amazon One lacks procedures to limit access to personal data to the necessary extent.,High impact due to overexposure of personal data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",Amazon One lacks mechanisms to comply with data subject's rights.,High impact due to non-compliance with data subject's rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",Amazon One lacks specific measures to enhance security of personal data processing.,High impact due to lack of security measures.,There is no procedure to conduct a data protection impact assessment.,Amazon One lacks a procedure to conduct a data protection impact assessment.,High impact due to lack of impact assessment procedures.
fria-instance-gpt-47.ttl,Amazon Ring Always Home Cam,Drone; Computer vision,"Amazon, TechCrunch, Evan Greer, Fight for the Future, Big Brother Watch, Ben Wood, Jake Moore","Amazon launched the Ring Always Home Cam, a drone with a security camera that moves around homes on pre-programmed routes, streaming video. Despite privacy-first claims, concerns exist about privacy erosion and security risks.","Autonomous drone with a camera, capable of moving around pre-programmed routes in homes, streaming video to a smartphone or tablet. Includes obstacle avoidance technology and a neural processing unit for scenario identification.",Strengthen home security,2021-09-28,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-always-home-cam,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,Amazon Ring Always Home Cam does not explicitly inform users that surveillance and security alerts are generated algorithmically.,"High impact due to lack of transparency, potentially affecting user trust and understanding.",,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,Amazon Ring Always Home Cam lacks mechanisms to prevent bias or discrimination in its surveillance activities.,High impact due to potential for discriminatory surveillance practices.,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,"Amazon Ring Always Home Cam lacks mechanisms to limit deployment to specific, consenting individuals.",High impact due to potential overreach and lack of consent.,,,,There are no mechanisms for the user to exercise control over the processing of personal data.,Amazon Ring Always Home Cam lacks mechanisms for user control over personal data and its processing.,High impact due to lack of control over personal data.,There are no measures to ensure the lawfulness of the processing of personal data.,Amazon Ring Always Home Cam lacks measures to ensure lawful processing of personal data.,High impact due to potential unlawful data processing.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,Amazon Ring Always Home Cam lacks procedures to limit access to personal data to the necessary extent.,High impact due to overexposure of personal data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",Amazon Ring Always Home Cam lacks mechanisms to comply with data subject's rights.,High impact due to non-compliance with data subject's rights.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",Amazon Ring Always Home Cam lacks specific measures to enhance security of personal data processing.,High impact due to lack of security measures.,There is no procedure to conduct a data protection impact assessment.,Amazon Ring Always Home Cam lacks a procedure to conduct a data protection impact assessment.,High impact due to lack of impact assessment procedures.
fria-instance-gpt-48.ttl,Police request Amazon Ring BLM protest footage,CCTV,"Amazon, LAPD, EFF, Motherboard, BBC",The LAPD requested footage from Amazon Ring users for the 2020 Black Lives Matter protests. The EFF criticized this for potential privacy violations and lack of oversight.,"Ring cameras, including doorbells and security cameras, which record video footage. Police requested footage through emails, sometimes without specifying a crime.","Strengthen security, safety",2020-06-01,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-blm-protest-surveillance,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"Police requests for footage did not specify a crime, raising concerns about the presumption of innocence and lack of specific incident details.",High impact due to potential misuse of surveillance without clear justification.,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"The police requests could disproportionately target protestors, potentially leading to discriminatory surveillance practices.",High impact due to potential for discriminatory surveillance.,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,Requests for footage during protests could chill freedom of expression and deter individuals from participating in protests.,Very high impact due to chilling effects on freedom of expression.,,,,There are no mechanisms for the user to exercise control over the processing of personal data.,"Police requests for footage could invade the privacy of individuals and families, capturing data unrelated to any crime.",High impact due to potential invasion of privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,Requests for footage without clear oversight could lead to unlawful processing of personal data.,High impact due to potential unlawful data processing.,There are no procedures to limit the access to personal data and to the extent and amount necessary for those purposes.,Requests for broad surveillance footage could result in over-collection of personal data.,High impact due to overexposure of personal data.,"There is no mechanism allowing to comply with the exercise of data subject’s rights (access, rectification and erasure of data relating to a specific individual).",There is no clear mechanism for users to refuse police requests for footage without fear of repercussions.,High impact due to lack of control over personal data sharing.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymisation and aggregation).",Amazon Ring systems lack specific security measures to ensure data protection in the event of unauthorized access.,High impact due to potential security vulnerabilities.,There is no procedure to conduct a data protection impact assessment.,Amazon Ring systems lack a procedure to conduct a data protection impact assessment.,High impact due to lack of impact assessment procedures.
fria-instance-gpt-49.ttl,Amazon shares Ring data with police,CCTV | Computer vision,"Amazon, Senator Ed Markey, EFF, Motherboard, BBC",Amazon Ring shared private recordings with the US police eleven times in 2022 without notifying Ring owners. This practice raises concerns about police reliance on private surveillance and user awareness of data usage.,"Ring cameras, including doorbells and security cameras, record video and audio footage. Police accessed footage under emergency circumstances without a warrant or owner consent.",Strengthen security,2022-07-13,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-police-data-sharing,The AI system does not communicate that a decision/advice or outcome is the result of an algorithmic decision.,"Police accessed footage without notifying owners, raising concerns about the presumption of innocence and lack of transparency in the process.",High impact due to potential misuse of surveillance without clear justification.,,,,,,,,,,,,,,,,The AI system targets members of a specific social group.,"The police requests could disproportionately target certain groups, leading to discriminatory surveillance practices.",High impact due to potential for discriminatory surveillance.,,,,,,,There is no mechanism to limit the deployment of the AI system to suspected individuals.,Accessing footage without consent could deter individuals from exercising their freedom of expression due to fear of surveillance.,Very high impact due to chilling effects on freedom of expression.,,,,There are no mechanisms for the user to exercise control over the processing of personal data.,"Police accessed footage without owner consent, potentially invading the privacy of individuals and families.",High impact due to potential invasion of privacy.,There are no measures to ensure the lawfulness of the processing of personal data.,Requests for footage without clear oversight could lead to unlawful processing of personal data.,High impact due to potential unlawful data processing.,There are no procedures to limit the access to personal data to the extent and amount necessary for those purposes.,Requests for broad surveillance footage could result in over-collection of personal data.,High impact due to overexposure of personal data.,"There is no mechanism allowing compliance with the exercise of data subject’s rights (access, rectification, and erasure of data relating to a specific individual).",There is no clear mechanism for users to refuse police requests for footage without fear of repercussions.,High impact due to lack of control over personal data sharing.,"There are no specific measures in place to enhance the security of the processing of personal data (via encryption, anonymization, and aggregation).",Amazon Ring systems lack specific security measures to ensure data protection in the event of unauthorized access.,High impact due to potential security vulnerabilities.,There is no procedure to conduct a data protection impact assessment.,Amazon Ring systems lack a procedure to conduct a data protection impact assessment.,High impact due to lack of impact assessment procedures.
fria-instance-gpt-50.ttl,Amazon Ring video doorbell 'invades' neighbour privacy,CCTV | Computer vision,"Amazon, Jon Woodard, Dr Mary Fairhurst, Judge Melissa Clarke, ProPrivacy, Information Commissioner's Office","Amazon-manufactured CCTV cameras and a Ring doorbell installed by Jon Woodard in Oxfordshire, UK, 'unjustifiably invaded' the privacy of his neighbour Dr Mary Fairhurst, capturing images and audio of her property and personal conversations. The court ruled that these devices contributed to harassment and violated data protection laws.","Ring doorbells and CCTV cameras with audio and video recording capabilities. Prior to 2020, the audio recording feature could not be turned off.",Strengthen security,2020-10-14,https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-video-doorbell-neighbour-privacy-invasion,"Personal data may be captured from people who are not even aware that the device is there, or that it records and processes audio and personal data.","The Ring doorbell and CCTV cameras recorded images and audio of the neighbour's property and personal conversations, violating her privacy.",High impact due to significant invasion of privacy.,"The devices collected audio data that could capture conversations, which was found to be more problematic and detrimental than video data.","The devices collected audio data that could capture conversations, which was found to be more problematic and detrimental than video data.",High impact due to potential for capturing sensitive personal conversations.,,,,,,,,,,,,,"The devices contributed to harassment of the neighbour, impacting her equality and non-discrimination rights.","The court ruled that the devices contributed to the harassment of the neighbour, impacting her equality and non-discrimination rights.",High impact due to harassment and unequal treatment.,,,,,,,The surveillance devices deterred the neighbour from expressing concerns due to fear of being recorded.,The use of surveillance devices deterred the neighbour from expressing concerns due to fear of being recorded.,High impact due to chilling effect on freedom of expression.,,,,"The devices were installed under the pretext of security, but were used to harass the neighbour, affecting her right to a fair trial.","The devices were installed under the pretext of security, but were used to harass the neighbour, affecting her right to a fair trial.",High impact due to misuse of surveillance for harassment.,"The defendant misled the claimant about the operation of the cameras, impacting the fairness of the trial.","The defendant misled the claimant about the operation of the cameras, impacting the fairness of the trial.",High impact due to lack of transparency in the operation of surveillance devices.,There are no procedures to limit the access to personal data to the extent and amount necessary for those purposes.,There are no procedures to limit the access to personal data to the extent and amount necessary for those purposes.,High impact due to lack of control over personal data.,There is no clear mechanism for users to refuse police requests for footage without fear of repercussions.,There is no clear mechanism for users to refuse police requests for footage without fear of repercussions.,High impact due to lack of control over personal data sharing.,Amazon Ring systems lack specific security measures to ensure data protection in the event of unauthorized access.,Amazon Ring systems lack specific security measures to ensure data protection in the event of unauthorized access.,High impact due to potential security vulnerabilities.,There is no procedure to conduct a data protection impact assessment.,Amazon Ring systems lack a procedure to conduct a data protection impact assessment.,High impact due to lack of impact assessment procedures.
